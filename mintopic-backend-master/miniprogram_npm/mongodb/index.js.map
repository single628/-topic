{"version":3,"sources":["index.js","cursor/abstract_cursor.js","utils.js","promise_provider.js","error.js","write_concern.js","sdam/common.js","read_concern.js","read_preference.js","cmap/wire_protocol/constants.js","../package.json","sessions.js","transactions.js","cmap/wire_protocol/shared.js","sdam/topology_description.js","sdam/server_description.js","operations/execute_operation.js","operations/operation.js","sdam/server_selection.js","operations/run_command.js","operations/command.js","explain.js","cmap/connection.js","cmap/message_stream.js","cmap/commands.js","cmap/wire_protocol/compression.js","cmap/stream_description.js","cmap/command_monitoring_events.js","mongo_types.js","cmap/metrics.js","operations/get_more.js","cursor/aggregation_cursor.js","operations/aggregate.js","cursor/find_cursor.js","operations/count.js","operations/find.js","sort.js","operations/indexes.js","operations/common_functions.js","operations/list_collections.js","constants.js","admin.js","operations/add_user.js","operations/remove_user.js","operations/validate_collection.js","operations/list_databases.js","mongo_client.js","db.js","collection.js","bulk/unordered.js","bulk/common.js","operations/insert.js","operations/bulk_write.js","operations/update.js","operations/delete.js","bulk/ordered.js","change_stream.js","operations/count_documents.js","operations/distinct.js","operations/drop.js","operations/estimated_document_count.js","operations/find_and_modify.js","operations/is_capped.js","operations/map_reduce.js","operations/options_operation.js","operations/rename.js","operations/stats.js","logger.js","operations/collections.js","operations/create_collection.js","operations/profiling_level.js","operations/set_profiling_level.js","operations/connect.js","sdam/topology.js","sdam/server.js","cmap/connection_pool.js","cmap/connect.js","cmap/auth/defaultAuthProviders.js","cmap/auth/mongocr.js","cmap/auth/auth_provider.js","cmap/auth/x509.js","cmap/auth/plain.js","cmap/auth/gssapi.js","cmap/auth/scram.js","cmap/auth/mongodb_aws.js","cmap/auth/mongo_credentials.js","cmap/errors.js","cmap/connection_pool_events.js","sdam/monitor.js","sdam/events.js","sdam/srv_polling.js","gridfs/index.js","gridfs/download.js","gridfs/upload.js"],"names":[],"mappings":";;;;;;;AAAA;AACA;AACA;ACFA,ADGA;ACFA,ADGA;ACFA,ADGA;ACFA,ADGA,AENA;ADIA,ADGA,AENA;ADIA,ADGA,AENA;ADIA,ADGA,AGTA,ADGA;ADIA,ADGA,AGTA,ADGA;ADIA,ADGA,AGTA,ADGA;ADIA,AGTA,AJYA,AGTA,ADGA;ADIA,AGTA,AJYA,AGTA,ADGA;ADIA,AGTA,AJYA,AGTA,ADGA;ADIA,AGTA,AJYA,AGTA,ADGA,AGTA;AJaA,AGTA,AJYA,AGTA,ADGA,AGTA;AJaA,AGTA,AJYA,AGTA,ADGA,AGTA;AJaA,AGTA,AJYA,AGTA,AGTA,AJYA,AGTA;AJaA,AGTA,AJYA,AGTA,AGTA,AJYA,AGTA;AJaA,AGTA,AJYA,AGTA,AGTA,AJYA,AGTA;AJaA,AGTA,AJYA,AGTA,AIZA,ADGA,AJYA,AGTA;AJaA,AGTA,AJYA,AGTA,AIZA,ADGA,AJYA,AGTA;AJaA,AGTA,AJYA,AGTA,AIZA,ADGA,AJYA,AGTA;AJaA,AGTA,AJYA,AGTA,AIZA,ACHA,AFMA,AJYA,AGTA;AJaA,AGTA,AJYA,AGTA,AIZA,ACHA,AFMA,AJYA,AGTA;AJaA,AGTA,AJYA,AGTA,AIZA,ACHA,AFMA,AJYA,AGTA;AIXA,ARwBA,AGTA,AJYA,AGTA,AIZA,ACHA,AFMA,AJYA,AGTA;AIXA,ARwBA,AGTA,AJYA,AGTA,AIZA,ACHA,AFMA,AJYA,AGTA;AIXA,ARwBA,AGTA,AJYA,AGTA,AIZA,ACHA,AFMA,AJYA,AGTA;AKdA,ADGA,ARwBA,AGTA,AJYA,AGTA,AIZA,ACHA,AFMA,AJYA,AGTA;AKdA,ADGA,ARwBA,AGTA,AJYA,AGTA,AIZA,ACHA,AFMA,AJYA,AGTA;AKdA,ADGA,ARwBA,AGTA,AJYA,AGTA,AIZA,ACHA,AFMA,AJYA,AGTA;AKdA,ADGA,ARwBA,AGTA,AJYA,AGTA,AIZA,ACHA,AFMA,AKfA,AT2BA,AGTA;AKdA,ADGA,ARwBA,AGTA,AJYA,AGTA,AIZA,ACHA,AFMA,AKfA,AT2BA,AGTA;AKdA,ADGA,ARwBA,AGTA,AJYA,AGTA,AIZA,ACHA,AFMA,AKfA,AT2BA,AGTA;AKdA,ADGA,ARwBA,AGTA,AJYA,AGTA,AIZA,ACHA,AFMA,AKfA,ACHA,AV8BA,AGTA;AKdA,ADGA,ARwBA,AGTA,AJYA,AGTA,AIZA,ACHA,AFMA,AKfA,ACHA,AV8BA,AGTA;AKdA,ADGA,ARwBA,AGTA,AJYA,AGTA,AIZA,ACHA,AFMA,AKfA,ACHA,AV8BA,AGTA;AKdA,ADGA,AIZA,AZoCA,AGTA,AJYA,AGTA,AIZA,ACHA,AFMA,AKfA,ACHA,AV8BA,AGTA;AKdA,ADGA,AIZA,AZoCA,AGTA,AJYA,AGTA,AIZA,ACHA,AFMA,AKfA,ACHA,AV8BA,AGTA;AKdA,ADGA,AIZA,AZoCA,AGTA,AJYA,AGTA,AIZA,ACHA,AFMA,AKfA,ACHA,AV8BA,AGTA;AKdA,ADGA,AIZA,AZoCA,AGTA,AJYA,AGTA,AIZA,ACHA,AFMA,AQxBA,AHSA,ACHA,AV8BA,AGTA;AKdA,ADGA,AIZA,AZoCA,AGTA,AJYA,AGTA,AIZA,ACHA,AFMA,AQxBA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AJYA,AGTA,AIZA,ACHA,AFMA,AQxBA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AJYA,AOrBA,ACHA,AFMA,AS3BA,ADGA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AJYA,AOrBA,ACHA,AFMA,AS3BA,ADGA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AJYA,AOrBA,ACHA,AFMA,AS3BA,ADGA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AJYA,AgBhDA,AT2BA,ACHA,AFMA,AS3BA,ADGA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AJYA,AgBhDA,AT2BA,ACHA,AFMA,AS3BA,ADGA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AJYA,AgBhDA,AT2BA,ACHA,AFMA,AS3BA,ADGA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AJYA,AgBhDA,ACHA,AV8BA,ACHA,AFMA,AS3BA,ADGA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AJYA,AgBhDA,ACHA,AV8BA,ACHA,AFMA,AS3BA,ADGA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AJYA,AgBhDA,ACHA,AV8BA,ACHA,AFMA,AS3BA,ADGA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AJYA,AgBhDA,ACHA,AV8BA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AJYA,AgBhDA,ACHA,AV8BA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AJYA,AgBhDA,ACHA,AV8BA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AJYA,AgBhDA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AJYA,AgBhDA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AJYA,AgBhDA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AJYA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AJYA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AJYA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AiBnDA,ArB+DA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AiBnDA,ArB+DA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AGTA,AZoCA,AGTA,AiBnDA,ArB+DA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AYpCA,AT2BA,AZoCA,AGTA,AiBnDA,ArB+DA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AYpCA,AT2BA,AZoCA,AGTA,AiBnDA,ArB+DA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AYpCA,AT2BA,AZoCA,AGTA,AiBnDA,ArB+DA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AYpCA,ACHA,AV8BA,AZoCA,AGTA,AiBnDA,ArB+DA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AYpCA,ACHA,AV8BA,AZoCA,AGTA,AiBnDA,ArB+DA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AYpCA,ACHA,AV8BA,AZoCA,AGTA,AiBnDA,ArB+DA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,Ac1CA,AFMA,ACHA,AV8BA,AZoCA,AGTA,AiBnDA,ArB+DA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,Ac1CA,AFMA,ACHA,AV8BA,AZoCA,AGTA,AiBnDA,ArB+DA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,Ac1CA,AFMA,ACHA,AV8BA,AZoCA,AGTA,AiBnDA,ArB+DA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,Ac1CA,AFMA,ACHA,AENA,AZoCA,AZoCA,AGTA,AiBnDA,ArB+DA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,Ac1CA,AFMA,ACHA,AENA,AZoCA,AZoCA,AGTA,AiBnDA,ArB+DA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,Ac1CA,AFMA,ACHA,AENA,AZoCA,AZoCA,AGTA,AiBnDA,ArB+DA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,Ac1CA,AFMA,ACHA,AGTA,ADGA,AZoCA,AZoCA,AGTA,AiBnDA,ArB+DA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AFMA,AS3BA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,Ac1CA,AFMA,ACHA,AGTA,ADGA,AZoCA,AZoCA,AGTA,AiBnDA,ArB+DA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,Ac1CA,AFMA,ACHA,AGTA,ADGA,AZoCA,AZoCA,AGTA,AiBnDA,ArB+DA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AiBnDA,AHSA,AFMA,ACHA,AGTA,ADGA,AZoCA,AZoCA,AGTA,AiBnDA,ArB+DA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AiBnDA,AHSA,AFMA,ACHA,AGTA,ADGA,AZoCA,AZoCA,AGTA,AiBnDA,ArB+DA,AoB5DA,AJYA,ACHA,AENA,AZoCA,ACHA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AiBnDA,AHSA,AFMA,ACHA,AGTA,ADGA,AZoCA,AZoCA,AGTA,AiBnDA,ArB+DA,AoB5DA,AJYA,ACHA,AV8BA,ACHA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AiBnDA,AHSA,AFMA,ACHA,AGTA,ADGA,AZoCA,AZoCA,AGTA,AiBnDA,ArB+DA,A4BpFA,ARwBA,AJYA,ACHA,AV8BA,ACHA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AiBnDA,AHSA,AFMA,ACHA,AGTA,ADGA,AZoCA,AZoCA,AGTA,AiBnDA,ArB+DA,A4BpFA,ARwBA,AJYA,ACHA,AV8BA,ACHA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA,AGTA;AKdA,AiBnDA,AHSA,AFMA,ACHA,AGTA,ADGA,AZoCA,AZoCA,AGTA,AiBnDA,ArB+DA,A4BpFA,ARwBA,AJYA,ACHA,AV8BA,ACHA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AZoCA,AZoCA,AGTA,AiBnDA,ArB+DA,A4BpFA,ARwBA,AJYA,ACHA,AV8BA,ACHA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AZoCA,AZoCA,AGTA,AiBnDA,ArB+DA,A4BpFA,ARwBA,AJYA,ACHA,AV8BA,ACHA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AZoCA,AZoCA,AGTA,AiBnDA,ArB+DA,A4BpFA,ARwBA,AJYA,ACHA,AV8BA,ACHA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,AGTA,AiBnDA,ArB+DA,A4BpFA,ARwBA,AJYA,Ac1CA,AbuCA,AV8BA,ACHA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,AGTA,AiBnDA,ArB+DA,A4BpFA,ARwBA,AJYA,Ac1CA,AbuCA,AV8BA,ACHA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,AGTA,AiBnDA,ArB+DA,A4BpFA,ARwBA,AJYA,Ac1CA,AbuCA,AV8BA,ACHA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,A3BiFA,AiBnDA,ArB+DA,A4BpFA,ARwBA,AJYA,Ac1CA,AbuCA,AV8BA,ACHA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,A3BiFA,AiBnDA,ArB+DA,A4BpFA,ARwBA,AJYA,Ac1CA,AbuCA,AT2BA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,A3BiFA,AiBnDA,ArB+DA,A4BpFA,ARwBA,AJYA,Ac1CA,AbuCA,AT2BA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,A3BiFA,AiBnDA,ArB+DA,A4BpFA,AIZA,AZoCA,AJYA,Ac1CA,AbuCA,AT2BA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,A3BiFA,AiBnDA,ArB+DA,A4BpFA,AIZA,AZoCA,AJYA,Ac1CA,AbuCA,AT2BA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,A3BiFA,AJYA,A4BpFA,AIZA,AZoCA,AJYA,Ac1CA,AbuCA,AT2BA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,AJYA,Ac1CA,AbuCA,AT2BA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,AJYA,Ac1CA,AbuCA,AT2BA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,AJYA,Ac1CA,AbuCA,AT2BA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,Ac1CA,AlBsDA,Ac1CA,AbuCA,AT2BA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,Ac1CA,AlBsDA,Ac1CA,AbuCA,AT2BA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,Ac1CA,AlBsDA,Ac1CA,AbuCA,AT2BA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,Ac1CA,AlBsDA,AmBzDA,ALeA,AbuCA,AT2BA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,Ac1CA,AlBsDA,AmBzDA,ALeA,AbuCA,AT2BA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,Ac1CA,AlBsDA,AmBzDA,ALeA,AbuCA,AT2BA,AOrBA,AGTA,AJYA,AHSA,ACHA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,Ac1CA,AlBsDA,AmBzDA,ALeA,AbuCA,AT2BA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,Ac1CA,AlBsDA,AmBzDA,ALeA,AbuCA,AT2BA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,Ac1CA,AlBsDA,AmBzDA,ALeA,AbuCA,AT2BA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,Ac1CA,AlBsDA,AmBzDA,ALeA,AOrBA,ApB4DA,AT2BA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,Ac1CA,AlBsDA,AmBzDA,ALeA,AOrBA,ApB4DA,AT2BA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,Ac1CA,AlBsDA,AmBzDA,ALeA,AOrBA,ApB4DA,AT2BA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,ALeA,AOrBA,ApB4DA,AT2BA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,ALeA,AOrBA,ApB4DA,AT2BA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,ALeA,AOrBA,ApB4DA,AT2BA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,ALeA,AOrBA,AENA,AtBkEA,AT2BA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,AtBkEA,AT2BA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,AtBkEA,AT2BA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,Ae7CA,AvCqHA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,A/B6FA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,Ae7CA,AvCqHA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,A/B6FA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,AiBnDA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,Ae7CA,AvCqHA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,A/B6FA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,Ae7CA,AvCqHA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,A/B6FA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,Ae7CA,AvCqHA,A8B1FA,AENA,A7BuFA,AJYA,A4BpFA,AIZA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,A/B6FA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AHSA,ADGA,Ae7CA,AvCqHA,A8B1FA,AENA,A7BuFA,AJYA,AgChGA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,A/B6FA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AJYA,Ae7CA,AvCqHA,A8B1FA,AENA,A7BuFA,AJYA,A0C9HA,AV8BA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,A/B6FA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AJYA,Ae7CA,AvCqHA,A8B1FA,AENA,A7BuFA,AJYA,A0C9HA,AV8BA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,A/B6FA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AJYA,Ae7CA,AvCqHA,A8B1FA,AENA,A7BuFA,AJYA,A0C9HA,AV8BA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,A/B6FA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AJYA,Ae7CA,AvCqHA,A8B1FA,AENA,A7BuFA,AJYA,A0C9HA,AV8BA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,AIZA,AnCyGA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AJYA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A0C9HA,AV8BA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,AIZA,AnCyGA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AJYA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A0C9HA,AV8BA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,AIZA,AnCyGA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AJYA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A0C9HA,AV8BA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,AIZA,ACHA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AJYA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A0C9HA,AV8BA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,AIZA,ACHA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AJYA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A0C9HA,AV8BA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,AIZA,ACHA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AJYA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A0C9HA,AV8BA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,AMlBA,AFMA,ACHA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AJYA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A0C9HA,AV8BA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,AMlBA,AFMA,ACHA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AJYA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A0C9HA,AV8BA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,AMlBA,AFMA,ACHA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AJYA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A8C1IA,AJYA,AV8BA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,AMlBA,AFMA,ACHA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AJYA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A8C1IA,AJYA,AV8BA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,AMlBA,AFMA,ACHA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AJYA,AxBwEA,A8B1FA,AENA,A7BuFA,AJYA,A8C1IA,AJYA,AV8BA,AZoCA,AkBtDA,AJYA,AlBsDA,AmBzDA,AENA,AENA,AMlBA,AFMA,ACHA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AJYA,AxBwEA,A8B1FA,AENA,Ac1CA,A3CiIA,AJYA,A8C1IA,AJYA,AV8BA,AZoCA,AkBtDA,AtBkEA,AmBzDA,AENA,AENA,AMlBA,AFMA,ACHA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AJYA,AxBwEA,A8B1FA,AENA,Ac1CA,A3CiIA,AJYA,A8C1IA,AJYA,AV8BA,AZoCA,AkBtDA,AtBkEA,AmBzDA,AENA,AENA,AMlBA,AFMA,ACHA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AJYA,AxBwEA,A8B1FA,AENA,Ac1CA,A3CiIA,AJYA,A8C1IA,AJYA,AV8BA,AZoCA,AkBtDA,AtBkEA,AmBzDA,AENA,AENA,AMlBA,AFMA,ACHA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AJYA,AuBrEA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,AJYA,A8C1IA,AJYA,AV8BA,AZoCA,AkBtDA,AtBkEA,AmBzDA,AENA,AENA,AMlBA,AFMA,ACHA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AJYA,AuBrEA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,AJYA,A8C1IA,AJYA,AV8BA,AZoCA,AkBtDA,AtBkEA,AmBzDA,AENA,AENA,AMlBA,AFMA,ACHA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,Ad0CA,AHSA,AFMA,ACHA,AMlBA,AJYA,AuBrEA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,AJYA,A8C1IA,AJYA,AV8BA,AZoCA,AkBtDA,AtBkEA,AmBzDA,AENA,AENA,AMlBA,AFMA,ACHA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AQxBA,AtBkEA,AHSA,AFMA,ACHA,AMlBA,AJYA,AuBrEA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,AJYA,A8C1IA,AJYA,AV8BA,AZoCA,AkBtDA,AtBkEA,AmBzDA,AENA,AENA,AMlBA,AFMA,ACHA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AQxBA,AtBkEA,AHSA,AFMA,ACHA,AMlBA,AJYA,AuBrEA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,AJYA,A8C1IA,AJYA,AV8BA,AZoCA,AkBtDA,AtBkEA,AmBzDA,AENA,AENA,AMlBA,AFMA,ACHA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AQxBA,AtBkEA,AHSA,AFMA,ACHA,AENA,AuBrEA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AZoCA,AkBtDA,AtBkEA,AmBzDA,AENA,AENA,AMlBA,AFMA,ACHA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,ADGA,AtBkEA,AHSA,AFMA,ACHA,AENA,AuBrEA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AZoCA,AkBtDA,AtBkEA,AmBzDA,AENA,AENA,AMlBA,ADGA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,ADGA,AtBkEA,AHSA,AFMA,ACHA,AENA,AuBrEA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AZoCA,AkBtDA,AtBkEA,AmBzDA,AENA,AENA,AMlBA,ADGA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,ADGA,AtBkEA,AHSA,AFMA,ACHA,AENA,AuBrEA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AZoCA,AkBtDA,AtBkEA,AmBzDA,AENA,AENA,AMlBA,ADGA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,ADGA,AtBkEA,AHSA,AFMA,ACHA,AENA,AuBrEA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AZoCA,AkBtDA,AtBkEA,AmBzDA,AENA,Ac1CA,AZoCA,AMlBA,ADGA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,ADGA,AtBkEA,AHSA,AFMA,ACHA,AENA,AuBrEA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AZoCA,AkBtDA,AtBkEA,AmBzDA,AENA,Ac1CA,AZoCA,AMlBA,ADGA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,ADGA,AtBkEA,AHSA,AFMA,ACHA,AENA,AuBrEA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AZoCA,AkBtDA,AtBkEA,AmBzDA,AENA,Ac1CA,AZoCA,AMlBA,ADGA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,ADGA,AtBkEA,AHSA,AFMA,ACHA,AENA,AuBrEA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,AhCgGA,AkBtDA,AtBkEA,AmBzDA,AENA,Ac1CA,AZoCA,AMlBA,ADGA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,ADGA,AtBkEA,AHSA,AFMA,ACHA,AENA,AuBrEA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,AhCgGA,AkBtDA,AtBkEA,AmBzDA,AENA,Ac1CA,AZoCA,AMlBA,ADGA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,ADGA,AtBkEA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,AhCgGA,AkBtDA,AtBkEA,AmBzDA,AENA,Ac1CA,AZoCA,AMlBA,ADGA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,ADGA,AtBkEA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,Ad0CA,AtBkEA,AmBzDA,AENA,Ac1CA,AZoCA,AMlBA,AQxBA,AT2BA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,ADGA,AtBkEA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,Ad0CA,AtBkEA,AmBzDA,AENA,Ac1CA,AZoCA,AMlBA,AQxBA,AT2BA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,ADGA,AtBkEA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,Ad0CA,AtBkEA,AmBzDA,AENA,Ac1CA,AZoCA,AMlBA,AQxBA,AT2BA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,ADGA,AtBkEA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,Ad0CA,AgBhDA,AtCkHA,AmBzDA,AENA,Ac1CA,AZoCA,AMlBA,AQxBA,AT2BA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,ADGA,AtBkEA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,Ad0CA,AgBhDA,AtCkHA,AmBzDA,AENA,Ac1CA,AZoCA,AMlBA,AQxBA,AT2BA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,ADGA,AtBkEA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,Ad0CA,AgBhDA,AtCkHA,AmBzDA,AENA,Ac1CA,AZoCA,AMlBA,AQxBA,AT2BA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AtBkEA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,Ad0CA,AgBhDA,AtCkHA,AmBzDA,AENA,Ac1CA,AZoCA,Ac1CA,AT2BA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AtBkEA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,Ad0CA,AgBhDA,AtCkHA,AmBzDA,AENA,Ac1CA,AZoCA,Ac1CA,AT2BA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AtBkEA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,Ad0CA,AgBhDA,AtCkHA,AmBzDA,AENA,Ac1CA,AZoCA,Ac1CA,AT2BA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,Ad0CA,AgBhDA,AtCkHA,AmBzDA,AENA,Ac1CA,AZoCA,Ac1CA,AT2BA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,Ad0CA,AgBhDA,AtCkHA,AmBzDA,AENA,Ac1CA,AZoCA,Ac1CA,AT2BA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,Ad0CA,AgBhDA,AtCkHA,AmBzDA,AENA,Ac1CA,AZoCA,Ac1CA,AT2BA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,Ad0CA,AmBzDA,AHSA,AtCkHA,AmBzDA,AENA,Ac1CA,AZoCA,Ac1CA,AT2BA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,Ad0CA,AmBzDA,AHSA,AtCkHA,AmBzDA,AENA,Ac1CA,AZoCA,Ac1CA,AT2BA,ApC4GA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,Ad0CA,AmBzDA,AHSA,AtCkHA,AmBzDA,AENA,Ac1CA,AZoCA,Ac1CA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AxBwEA,AV8BA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,Ad0CA,AmBzDA,AHSA,AIZA,A1C8HA,AmBzDA,AENA,Ac1CA,AZoCA,Ac1CA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,Ad0CA,AmBzDA,AHSA,AIZA,A1C8HA,AmBzDA,AENA,Ac1CA,AZoCA,Ac1CA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,Ad0CA,AmBzDA,AHSA,AIZA,A1C8HA,AmBzDA,AENA,Ac1CA,AZoCA,Ac1CA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,Ad0CA,AmBzDA,AHSA,AIZA,ACHA,A3CiIA,AmBzDA,AENA,Ac1CA,AZoCA,Ac1CA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,AKfA,AHSA,AIZA,ACHA,A3CiIA,AmBzDA,AENA,Ac1CA,AZoCA,Ac1CA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,AKfA,AHSA,AIZA,ACHA,A3CiIA,AmBzDA,AENA,Ac1CA,AZoCA,Ac1CA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AV8BA,AoB5DA,AKfA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,AENA,Ac1CA,AZoCA,Ac1CA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AU9BA,AKfA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,AENA,Ac1CA,AZoCA,Ac1CA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AU9BA,AKfA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,AENA,Ac1CA,AZoCA,Ac1CA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AU9BA,AKfA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AZoCA,Ac1CA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AU9BA,AKfA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AZoCA,Ac1CA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AU9BA,AKfA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AZoCA,Ac1CA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AQvBA,A+B7FA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AU9BA,AKfA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,Ac1CA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AU9BA,AKfA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,Ac1CA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AU9BA,AKfA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,Ac1CA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AU9BA,AKfA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,AV8BA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AJYA,AU9BA,AKfA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,AV8BA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AMlBA,AKfA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,AV8BA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AMlBA,AKfA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,AXiCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AMlBA,AKfA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,AXiCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AMlBA,AKfA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,AXiCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AMlBA,AKfA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,ACHA,AZoCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AMlBA,AKfA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,ACHA,AZoCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AMlBA,AKfA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,ACHA,AZoCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AMlBA,AKfA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,ACHA,ACHA,AbuCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AWjCA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,ACHA,ACHA,AbuCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A0C9HA,AWjCA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,ACHA,ACHA,AbuCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AWjCA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,ACHA,ACHA,AbuCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AWjCA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,ACHA,ACHA,AbuCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AWjCA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,ACHA,ACHA,AbuCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,AXiCA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,ACHA,ACHA,AbuCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AyB3EA,AlCsGA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,AXiCA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,ACHA,ACHA,AbuCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,AXiCA,AHSA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,ACHA,ACHA,AbuCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,AXiCA,AYpCA,Af6CA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,ACHA,ACHA,AbuCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,ACHA,Af6CA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,ACHA,ACHA,AbuCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,ACHA,Af6CA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,ACHA,ACHA,AbuCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,ACHA,AyB3EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,ACHA,Af6CA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,AMlBA,ALeA,ACHA,AbuCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,A0B9EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,ACHA,Af6CA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,AMlBA,ALeA,ACHA,AbuCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,A0B9EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,ACHA,Af6CA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,AMlBA,ALeA,ACHA,AbuCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,A0B9EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,ACHA,Af6CA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,AMlBA,ALeA,AMlBA,ALeA,AbuCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,A0B9EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,ACHA,Af6CA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,AMlBA,ALeA,AMlBA,ALeA,AbuCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,A0B9EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,ACHA,Af6CA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AWjCA,AvBqEA,AwBxEA,ACHA,AMlBA,ALeA,AMlBA,ALeA,AbuCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,A0B9EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,AIZA,AHSA,Af6CA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AZoCA,AwBxEA,ACHA,AMlBA,ALeA,AMlBA,ALeA,AbuCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,A0B9EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,AIZA,AHSA,Af6CA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AZoCA,AwBxEA,ACHA,AMlBA,ALeA,AMlBA,ALeA,AbuCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,A0B9EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,AIZA,AHSA,Af6CA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AZoCA,AwBxEA,ACHA,AMlBA,ALeA,AMlBA,ALeA,AbuCA,A7CuIA,AOrBA,AGTA,AJYA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,A0B9EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,AIZA,AHSA,Af6CA,AIZA,ACHA,ACHA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AZoCA,AwBxEA,ACHA,AMlBA,ALeA,AMlBA,ALeA,AbuCA,A7CuIA,AOrBA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,A0B9EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,AIZA,AHSA,Af6CA,AIZA,AENA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AZoCA,AwBxEA,ACHA,AMlBA,ALeA,AMlBA,ALeA,AbuCA,A7CuIA,AOrBA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,A0B9EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,AIZA,AHSA,Af6CA,AIZA,AENA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AZoCA,AwBxEA,AOrBA,ALeA,AMlBA,ALeA,AbuCA,AtCkHA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,A0B9EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,AIZA,AHSA,Af6CA,AIZA,AENA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AZoCA,AwBxEA,AOrBA,ALeA,AMlBA,ALeA,AbuCA,AqB/DA,A3DiLA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,A0B9EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,AIZA,AHSA,Af6CA,AIZA,AENA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AZoCA,AwBxEA,AOrBA,ALeA,AMlBA,ALeA,AbuCA,AqB/DA,A3DiLA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,A0B9EA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,AIZA,AHSA,Af6CA,AIZA,AENA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AZoCA,AwBxEA,AOrBA,ALeA,AMlBA,ALeA,AbuCA,AqB/DA,A3DiLA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,AIZA,AHSA,Af6CA,AIZA,AENA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AZoCA,AwBxEA,AOrBA,ALeA,AMlBA,ALeA,AbuCA,AqB/DA,A3DiLA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,AIZA,AHSA,Af6CA,AIZA,AENA,A5CoIA,AmBzDA,A0B9EA,AxBwEA,Ac1CA,AZoCA,AwBxEA,AOrBA,ALeA,AMlBA,ALeA,AbuCA,AqB/DA,A3DiLA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AFMA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,AIZA,AHSA,Af6CA,AIZA,AENA,AzB2EA,A0B9EA,AxBwEA,Ac1CA,AYpCA,AOrBA,ALeA,AMlBA,ALeA,AbuCA,AqB/DA,A3DiLA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,AIZA,AHSA,Af6CA,AIZA,AENA,AzB2EA,A0B9EA,AxBwEA,Ac1CA,AYpCA,AOrBA,ALeA,AMlBA,ALeA,AbuCA,AqB/DA,A3DiLA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,AIZA,AHSA,Af6CA,AIZA,AENA,AzB2EA,A0B9EA,AxBwEA,Ac1CA,AYpCA,AOrBA,ALeA,AMlBA,ALeA,AbuCA,AqB/DA,A3DiLA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,AKfA,ANkBA,AOrBA,A7BuFA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,AIZA,AHSA,Af6CA,AIZA,AENA,AzB2EA,A0B9EA,AxBwEA,Ac1CA,AYpCA,AOrBA,ALeA,AMlBA,ALeA,AbuCA,AqB/DA,A3DiLA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,ADGA,AOrBA,AqB/DA,AlDsJA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,AIZA,AHSA,Af6CA,AIZA,AENA,AzB2EA,A0B9EA,AxBwEA,Ac1CA,AYpCA,AOrBA,ALeA,AMlBA,ALeA,AbuCA,AqB/DA,A3DiLA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,ADGA,AOrBA,AqB/DA,AlDsJA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,AsBlEA,AIZA,AHSA,Af6CA,AIZA,AvBqEA,A0B9EA,AxBwEA,Ac1CA,AYpCA,AOrBA,ALeA,AMlBA,ALeA,AbuCA,AqB/DA,A3DiLA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,ADGA,AOrBA,AqB/DA,AlDsJA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AIZA,AvBqEA,A0B9EA,AxBwEA,Ac1CA,AYpCA,AOrBA,ALeA,AMlBA,ALeA,AbuCA,AqB/DA,A3DiLA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,ADGA,AOrBA,AqB/DA,ACHA,AnDyJA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AIZA,AvBqEA,A0B9EA,AxBwEA,Ac1CA,AYpCA,AOrBA,ALeA,AMlBA,ALeA,AbuCA,AqB/DA,A3DiLA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,ADGA,AOrBA,AqB/DA,ACHA,AnDyJA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AIZA,AvBqEA,A0B9EA,AxBwEA,Ac1CA,AYpCA,AOrBA,ALeA,AMlBA,ALeA,AbuCA,AqB/DA,A3DiLA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,ADGA,AOrBA,AqB/DA,ACHA,AnDyJA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AIZA,AvBqEA,A0B9EA,AxBwEA,Ac1CA,AYpCA,AOrBA,ALeA,AMlBA,ALeA,AbuCA,AqB/DA,A3DiLA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,ADGA,AOrBA,AuBrEA,AFMA,ACHA,AnDyJA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AIZA,AvBqEA,A0B9EA,AxBwEA,Ac1CA,AYpCA,AOrBA,ALeA,AMlBA,ALeA,AbuCA,AqB/DA,A3DiLA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,AMlBA,AuBrEA,AFMA,ACHA,AnDyJA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AIZA,AvBqEA,A0B9EA,AxBwEA,Ac1CA,AYpCA,AOrBA,ALeA,AMlBA,ALeA,AbuCA,AqB/DA,A3DiLA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,AMlBA,AuBrEA,AFMA,ACHA,AnDyJA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AIZA,AvBqEA,A0B9EA,AxBwEA,Ac1CA,AYpCA,AOrBA,ALeA,AMlBA,ALeA,AbuCA,AqB/DA,A3DiLA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,AMlBA,AuBrEA,AFMA,ACHA,AENA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AIZA,AvBqEA,A0B9EA,AxBwEA,Ac1CA,AYpCA,AOrBA,ALeA,AMlBA,ALeA,AbuCA,AqB/DA,A3DiLA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,AMlBA,AuBrEA,AFMA,ACHA,AENA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,Ac1CA,AYpCA,AOrBA,ACHA,ALeA,AbuCA,AqB/DA,A3DiLA,AGTA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,AMlBA,AuBrEA,AFMA,ACHA,AENA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,Ac1CA,AYpCA,AOrBA,ACHA,ALeA,AbuCA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,AMlBA,AuBrEA,AFMA,ACHA,AGTA,ADGA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,Ac1CA,AYpCA,AQxBA,ALeA,AbuCA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,AMlBA,AuBrEA,AFMA,ACHA,AGTA,ADGA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,Ac1CA,AYpCA,AQxBA,ALeA,AbuCA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,AMlBA,AuBrEA,AFMA,ACHA,AGTA,ADGA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,Ac1CA,AYpCA,AQxBA,ALeA,AbuCA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AuCpHA,AS3BA,AMlBA,AuBrEA,AFMA,AKfA,AJYA,AGTA,ADGA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AQxBA,AlBsDA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AFMA,AKfA,AJYA,AGTA,ADGA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AQxBA,AlBsDA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AFMA,AKfA,AJYA,AGTA,ADGA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AQxBA,AlBsDA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AFMA,AKfA,AJYA,AGTA,AENA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AQxBA,AlBsDA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AFMA,AKfA,AJYA,AGTA,AENA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AQxBA,AlBsDA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AFMA,AKfA,AJYA,AGTA,AENA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AQxBA,AlBsDA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AFMA,AKfA,AJYA,AMlBA,AHSA,AENA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AQxBA,AlBsDA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AFMA,AKfA,AJYA,AMlBA,AHSA,AENA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AQxBA,AlBsDA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AFMA,AKfA,AJYA,AMlBA,AHSA,AENA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AQxBA,AlBsDA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AFMA,AKfA,AGTA,APqBA,AMlBA,AHSA,AENA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AQxBA,AlBsDA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AFMA,AKfA,AGTA,APqBA,AMlBA,AHSA,AENA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AQxBA,AlBsDA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AFMA,AKfA,AGTA,APqBA,AMlBA,AHSA,AENA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AQxBA,AlBsDA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AFMA,AKfA,AGTA,APqBA,AMlBA,AHSA,AENA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,AWjCA,AtCkHA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AQxBA,AlBsDA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AFMA,AKfA,AGTA,APqBA,AMlBA,AHSA,AENA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,AWjCA,AtCkHA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AQxBA,AlBsDA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AFMA,AKfA,AGTA,APqBA,AMlBA,AHSA,AENA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,AWjCA,AtCkHA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AQxBA,AlBsDA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AGTA,AGTA,APqBA,AMlBA,AHSA,AENA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AQxBA,AlBsDA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AGTA,AGTA,APqBA,AMlBA,AHSA,AENA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AQxBA,AlBsDA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AGTA,AGTA,APqBA,AMlBA,AHSA,AENA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AQxBA,AlBsDA,AqB/DA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AGTA,AGTA,APqBA,AMlBA,AHSA,AENA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,A8B1FA,AENA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AmCzGA,Ad0CA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AGTA,AGTA,APqBA,AMlBA,AHSA,AENA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AmCzGA,Ad0CA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AGTA,AGTA,APqBA,AMlBA,AHSA,AENA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AHSA,Af6CA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AmCzGA,Ad0CA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AGTA,AGTA,APqBA,AMlBA,AHSA,AENA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AGTA,AGTA,APqBA,AMlBA,AHSA,AENA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AGTA,AGTA,APqBA,AMlBA,AHSA,AENA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AxDwKA,AuDrKA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AGTA,AGTA,APqBA,AMlBA,ADGA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AxDwKA,AwExNA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AGTA,AGTA,APqBA,AMlBA,ADGA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AxDwKA,AwExNA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,AuBrEA,AGTA,AGTA,APqBA,AMlBA,ADGA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,A+D7LA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AxDwKA,AwExNA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,APqBA,AMlBA,ADGA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AuFrQA,AxBwEA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AxDwKA,AwExNA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,APqBA,AMlBA,ADGA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AuFrQA,AxBwEA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AxDwKA,AwExNA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,APqBA,AMlBA,ADGA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AuFrQA,AxBwEA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AxDwKA,AwExNA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,APqBA,AMlBA,ADGA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AxBwEA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AxDwKA,AwExNA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,APqBA,AMlBA,ADGA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AxBwEA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AxDwKA,AwExNA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AHSA,ArD+JA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AxBwEA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AxDwKA,AwExNA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AxDwKA,AwExNA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AxDwKA,AwExNA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AxDwKA,AwExNA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,ADGA,AtCkHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,AlBsDA,AnByDA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,ArC+GA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,ArC+GA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,ArC+GA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,ArC+GA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,ArC+GA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,ArC+GA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,ArC+GA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,ArC+GA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,ArC+GA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,ArC+GA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,ArC+GA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,ArC+GA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,ArC+GA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,A0B9EA,ArC+GA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AXiCA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AXiCA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AXiCA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AXiCA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AXiCA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AXiCA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AXiCA,A0B9EA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,Ae7CA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,Ae7CA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,Ae7CA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,Ae7CA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,Ae7CA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,Ae7CA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,Ae7CA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,Ae7CA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,Ae7CA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,Ae7CA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,Ae7CA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,Ae7CA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,Ae7CA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,Ae7CA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,Ae7CA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,Ae7CA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,Ae7CA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,Ae7CA,AxBwEA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,AxDwKA,AHSA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,A0B9EA,AV8BA,AoC5GA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,A0B9EA,A0B9EA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AGTA,ADGA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AENA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AENA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AENA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AENA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AENA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AENA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AENA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AENA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AENA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AENA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AENA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,AgBhDA,AjBmDA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AENA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AoD5JA,ADGA,Ad0CA,ADGA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AENA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AmDzJA,Ad0CA,ADGA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A0B9EA,AENA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AmDzJA,Ad0CA,ADGA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AmDzJA,Ad0CA,ADGA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AmDzJA,Ad0CA,ADGA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AmDzJA,Ad0CA,ADGA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AmDzJA,Ad0CA,ADGA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AmDzJA,Ad0CA,ADGA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,ArB+DA,AT2BA,AmDzJA,Ad0CA,ADGA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,A9B0FA,AmDzJA,Ad0CA,ADGA,A3DiLA,AHSA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,A9B0FA,AmDzJA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,A9B0FA,AmDzJA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,A9B0FA,AmDzJA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,A9B0FA,AmDzJA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,A9B0FA,AmDzJA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,A9B0FA,AmDzJA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,AYpCA,AvCqHA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,A9B0FA,AmDzJA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,AqB/DA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,AqB/DA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,AqB/DA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,AqB/DA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,AqB/DA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,AqB/DA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,AqB/DA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,AqB/DA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,AqB/DA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,A1B8EA,AqB/DA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ADGA,AENA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A4BpFA,ADGA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,Ac1CA,A3CiIA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,A7BuFA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,A7BuFA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,A7BuFA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,A7BuFA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,A7BuFA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,A7BuFA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,A7BuFA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,A7BuFA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,A7BuFA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,A7BuFA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,A7BuFA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,A7BuFA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,A7BuFA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,A7BuFA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,A7BuFA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,A7BuFA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AgChGA,A7BuFA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,A2BjFA,A3DiLA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AoD5JA,AtDkKA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,ALeA,Ad0CA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,AnByDA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,AnByDA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,AnByDA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,AnByDA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,AnByDA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,AnByDA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,AnByDA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AwFxQA,ACHA,AnByDA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,AnByDA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,AnByDA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,AnByDA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,AnByDA,ADGA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AFMA,AqD/JA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,A3BiFA,A/C6IA,AGTA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,A3BiFA,A5CoIA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AyF3QA,ApB4DA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AMlBA,AhCgGA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A1B8EA,AmDzJA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AyB3EA,AvEqNA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,A9C0IA,AqE/MA,A9D0LA,AT2BA;AgD/IA,AuBrEA,A9D0LA,AT2BA;AgD/IA,AuBrEA,A9D0LA,AT2BA;AgD/IA,AuBrEA,A9D0LA,AT2BA;AgD/IA,AuBrEA,A9D0LA,AT2BA;AgD/IA,AuBrEA,A9D0LA,AT2BA;AgD/IA,AuBrEA,A9D0LA,AT2BA;AgD/IA,AuBrEA,A9D0LA,AT2BA;AgD/IA,AuBrEA,A9D0LA,AT2BA;AgD/IA,AuBrEA,A9D0LA,AT2BA;AgD/IA,AuBrEA,A9D0LA,AT2BA;AgD/IA,AuBrEA,A9D0LA,AT2BA;AgD/IA,AuBrEA,A9D0LA,AT2BA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AuBrEA,AvEqNA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AgD/IA,AhDgJA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","file":"index.js","sourcesContent":["\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.Logger = exports.Collection = exports.Db = exports.MongoClient = exports.Admin = exports.Promise = exports.MongoBulkWriteError = exports.MongoTopologyClosedError = exports.MongoServerClosedError = exports.MongoKerberosError = exports.MongoTransactionError = exports.MongoExpiredSessionError = exports.MongoNotConnectedError = exports.MongoCursorInUseError = exports.MongoCursorExhaustedError = exports.MongoBatchReExecutionError = exports.MongoDecompressionError = exports.MongoGridFSChunkError = exports.MongoGridFSStreamError = exports.MongoChangeStreamError = exports.MongoRuntimeError = exports.MongoWriteConcernError = exports.MongoParseError = exports.MongoServerSelectionError = exports.MongoSystemError = exports.MongoNetworkTimeoutError = exports.MongoNetworkError = exports.MongoMissingDependencyError = exports.MongoMissingCredentialsError = exports.MongoInvalidArgumentError = exports.MongoCompatibilityError = exports.MongoAPIError = exports.MongoDriverError = exports.MongoServerError = exports.MongoError = exports.ObjectID = exports.Map = exports.BSONSymbol = exports.BSONRegExp = exports.Decimal128 = exports.Timestamp = exports.ObjectId = exports.MaxKey = exports.MinKey = exports.Long = exports.Int32 = exports.Double = exports.DBRef = exports.Code = exports.Binary = void 0;\nexports.SrvPollingEvent = exports.TopologyOpeningEvent = exports.TopologyDescriptionChangedEvent = exports.TopologyClosedEvent = exports.ServerOpeningEvent = exports.ServerDescriptionChangedEvent = exports.ServerClosedEvent = exports.ServerHeartbeatFailedEvent = exports.ServerHeartbeatSucceededEvent = exports.ServerHeartbeatStartedEvent = exports.ConnectionReadyEvent = exports.ConnectionPoolMonitoringEvent = exports.ConnectionPoolCreatedEvent = exports.ConnectionPoolClosedEvent = exports.ConnectionPoolClearedEvent = exports.ConnectionCreatedEvent = exports.ConnectionClosedEvent = exports.ConnectionCheckedOutEvent = exports.ConnectionCheckedInEvent = exports.ConnectionCheckOutStartedEvent = exports.ConnectionCheckOutFailedEvent = exports.CommandFailedEvent = exports.CommandSucceededEvent = exports.CommandStartedEvent = exports.ReadPreference = exports.ReadConcern = exports.WriteConcern = exports.BSONType = exports.ServerApiVersion = exports.ReadPreferenceMode = exports.ReadConcernLevel = exports.ExplainVerbosity = exports.ReturnDocument = exports.Compressor = exports.CURSOR_FLAGS = exports.AuthMechanism = exports.BatchType = exports.AutoEncryptionLoggerLevel = exports.LoggerLevel = exports.TopologyType = exports.ServerType = exports.ProfilingLevel = exports.CancellationToken = exports.GridFSBucket = exports.ListCollectionsCursor = exports.ListIndexesCursor = exports.FindCursor = exports.AggregationCursor = exports.AbstractCursor = void 0;\nconst abstract_cursor_1 = require(\"./cursor/abstract_cursor\");\nObject.defineProperty(exports, \"AbstractCursor\", { enumerable: true, get: function () { return abstract_cursor_1.AbstractCursor; } });\nconst aggregation_cursor_1 = require(\"./cursor/aggregation_cursor\");\nObject.defineProperty(exports, \"AggregationCursor\", { enumerable: true, get: function () { return aggregation_cursor_1.AggregationCursor; } });\nconst find_cursor_1 = require(\"./cursor/find_cursor\");\nObject.defineProperty(exports, \"FindCursor\", { enumerable: true, get: function () { return find_cursor_1.FindCursor; } });\nconst indexes_1 = require(\"./operations/indexes\");\nObject.defineProperty(exports, \"ListIndexesCursor\", { enumerable: true, get: function () { return indexes_1.ListIndexesCursor; } });\nconst list_collections_1 = require(\"./operations/list_collections\");\nObject.defineProperty(exports, \"ListCollectionsCursor\", { enumerable: true, get: function () { return list_collections_1.ListCollectionsCursor; } });\nconst promise_provider_1 = require(\"./promise_provider\");\nObject.defineProperty(exports, \"Promise\", { enumerable: true, get: function () { return promise_provider_1.PromiseProvider; } });\nconst admin_1 = require(\"./admin\");\nObject.defineProperty(exports, \"Admin\", { enumerable: true, get: function () { return admin_1.Admin; } });\nconst mongo_client_1 = require(\"./mongo_client\");\nObject.defineProperty(exports, \"MongoClient\", { enumerable: true, get: function () { return mongo_client_1.MongoClient; } });\nconst db_1 = require(\"./db\");\nObject.defineProperty(exports, \"Db\", { enumerable: true, get: function () { return db_1.Db; } });\nconst collection_1 = require(\"./collection\");\nObject.defineProperty(exports, \"Collection\", { enumerable: true, get: function () { return collection_1.Collection; } });\nconst logger_1 = require(\"./logger\");\nObject.defineProperty(exports, \"Logger\", { enumerable: true, get: function () { return logger_1.Logger; } });\nconst gridfs_1 = require(\"./gridfs\");\nObject.defineProperty(exports, \"GridFSBucket\", { enumerable: true, get: function () { return gridfs_1.GridFSBucket; } });\nconst mongo_types_1 = require(\"./mongo_types\");\nObject.defineProperty(exports, \"CancellationToken\", { enumerable: true, get: function () { return mongo_types_1.CancellationToken; } });\nvar bson_1 = require(\"./bson\");\nObject.defineProperty(exports, \"Binary\", { enumerable: true, get: function () { return bson_1.Binary; } });\nObject.defineProperty(exports, \"Code\", { enumerable: true, get: function () { return bson_1.Code; } });\nObject.defineProperty(exports, \"DBRef\", { enumerable: true, get: function () { return bson_1.DBRef; } });\nObject.defineProperty(exports, \"Double\", { enumerable: true, get: function () { return bson_1.Double; } });\nObject.defineProperty(exports, \"Int32\", { enumerable: true, get: function () { return bson_1.Int32; } });\nObject.defineProperty(exports, \"Long\", { enumerable: true, get: function () { return bson_1.Long; } });\nObject.defineProperty(exports, \"MinKey\", { enumerable: true, get: function () { return bson_1.MinKey; } });\nObject.defineProperty(exports, \"MaxKey\", { enumerable: true, get: function () { return bson_1.MaxKey; } });\nObject.defineProperty(exports, \"ObjectId\", { enumerable: true, get: function () { return bson_1.ObjectId; } });\nObject.defineProperty(exports, \"Timestamp\", { enumerable: true, get: function () { return bson_1.Timestamp; } });\nObject.defineProperty(exports, \"Decimal128\", { enumerable: true, get: function () { return bson_1.Decimal128; } });\nObject.defineProperty(exports, \"BSONRegExp\", { enumerable: true, get: function () { return bson_1.BSONRegExp; } });\nObject.defineProperty(exports, \"BSONSymbol\", { enumerable: true, get: function () { return bson_1.BSONSymbol; } });\nObject.defineProperty(exports, \"Map\", { enumerable: true, get: function () { return bson_1.Map; } });\nconst bson_2 = require(\"bson\");\n/**\n * @public\n * @deprecated Please use `ObjectId`\n */\nexports.ObjectID = bson_2.ObjectId;\nvar error_1 = require(\"./error\");\nObject.defineProperty(exports, \"MongoError\", { enumerable: true, get: function () { return error_1.MongoError; } });\nObject.defineProperty(exports, \"MongoServerError\", { enumerable: true, get: function () { return error_1.MongoServerError; } });\nObject.defineProperty(exports, \"MongoDriverError\", { enumerable: true, get: function () { return error_1.MongoDriverError; } });\nObject.defineProperty(exports, \"MongoAPIError\", { enumerable: true, get: function () { return error_1.MongoAPIError; } });\nObject.defineProperty(exports, \"MongoCompatibilityError\", { enumerable: true, get: function () { return error_1.MongoCompatibilityError; } });\nObject.defineProperty(exports, \"MongoInvalidArgumentError\", { enumerable: true, get: function () { return error_1.MongoInvalidArgumentError; } });\nObject.defineProperty(exports, \"MongoMissingCredentialsError\", { enumerable: true, get: function () { return error_1.MongoMissingCredentialsError; } });\nObject.defineProperty(exports, \"MongoMissingDependencyError\", { enumerable: true, get: function () { return error_1.MongoMissingDependencyError; } });\nObject.defineProperty(exports, \"MongoNetworkError\", { enumerable: true, get: function () { return error_1.MongoNetworkError; } });\nObject.defineProperty(exports, \"MongoNetworkTimeoutError\", { enumerable: true, get: function () { return error_1.MongoNetworkTimeoutError; } });\nObject.defineProperty(exports, \"MongoSystemError\", { enumerable: true, get: function () { return error_1.MongoSystemError; } });\nObject.defineProperty(exports, \"MongoServerSelectionError\", { enumerable: true, get: function () { return error_1.MongoServerSelectionError; } });\nObject.defineProperty(exports, \"MongoParseError\", { enumerable: true, get: function () { return error_1.MongoParseError; } });\nObject.defineProperty(exports, \"MongoWriteConcernError\", { enumerable: true, get: function () { return error_1.MongoWriteConcernError; } });\nObject.defineProperty(exports, \"MongoRuntimeError\", { enumerable: true, get: function () { return error_1.MongoRuntimeError; } });\nObject.defineProperty(exports, \"MongoChangeStreamError\", { enumerable: true, get: function () { return error_1.MongoChangeStreamError; } });\nObject.defineProperty(exports, \"MongoGridFSStreamError\", { enumerable: true, get: function () { return error_1.MongoGridFSStreamError; } });\nObject.defineProperty(exports, \"MongoGridFSChunkError\", { enumerable: true, get: function () { return error_1.MongoGridFSChunkError; } });\nObject.defineProperty(exports, \"MongoDecompressionError\", { enumerable: true, get: function () { return error_1.MongoDecompressionError; } });\nObject.defineProperty(exports, \"MongoBatchReExecutionError\", { enumerable: true, get: function () { return error_1.MongoBatchReExecutionError; } });\nObject.defineProperty(exports, \"MongoCursorExhaustedError\", { enumerable: true, get: function () { return error_1.MongoCursorExhaustedError; } });\nObject.defineProperty(exports, \"MongoCursorInUseError\", { enumerable: true, get: function () { return error_1.MongoCursorInUseError; } });\nObject.defineProperty(exports, \"MongoNotConnectedError\", { enumerable: true, get: function () { return error_1.MongoNotConnectedError; } });\nObject.defineProperty(exports, \"MongoExpiredSessionError\", { enumerable: true, get: function () { return error_1.MongoExpiredSessionError; } });\nObject.defineProperty(exports, \"MongoTransactionError\", { enumerable: true, get: function () { return error_1.MongoTransactionError; } });\nObject.defineProperty(exports, \"MongoKerberosError\", { enumerable: true, get: function () { return error_1.MongoKerberosError; } });\nObject.defineProperty(exports, \"MongoServerClosedError\", { enumerable: true, get: function () { return error_1.MongoServerClosedError; } });\nObject.defineProperty(exports, \"MongoTopologyClosedError\", { enumerable: true, get: function () { return error_1.MongoTopologyClosedError; } });\nvar common_1 = require(\"./bulk/common\");\nObject.defineProperty(exports, \"MongoBulkWriteError\", { enumerable: true, get: function () { return common_1.MongoBulkWriteError; } });\n// enums\nvar set_profiling_level_1 = require(\"./operations/set_profiling_level\");\nObject.defineProperty(exports, \"ProfilingLevel\", { enumerable: true, get: function () { return set_profiling_level_1.ProfilingLevel; } });\nvar common_2 = require(\"./sdam/common\");\nObject.defineProperty(exports, \"ServerType\", { enumerable: true, get: function () { return common_2.ServerType; } });\nObject.defineProperty(exports, \"TopologyType\", { enumerable: true, get: function () { return common_2.TopologyType; } });\nvar logger_2 = require(\"./logger\");\nObject.defineProperty(exports, \"LoggerLevel\", { enumerable: true, get: function () { return logger_2.LoggerLevel; } });\nvar deps_1 = require(\"./deps\");\nObject.defineProperty(exports, \"AutoEncryptionLoggerLevel\", { enumerable: true, get: function () { return deps_1.AutoEncryptionLoggerLevel; } });\nvar common_3 = require(\"./bulk/common\");\nObject.defineProperty(exports, \"BatchType\", { enumerable: true, get: function () { return common_3.BatchType; } });\nvar defaultAuthProviders_1 = require(\"./cmap/auth/defaultAuthProviders\");\nObject.defineProperty(exports, \"AuthMechanism\", { enumerable: true, get: function () { return defaultAuthProviders_1.AuthMechanism; } });\nvar abstract_cursor_2 = require(\"./cursor/abstract_cursor\");\nObject.defineProperty(exports, \"CURSOR_FLAGS\", { enumerable: true, get: function () { return abstract_cursor_2.CURSOR_FLAGS; } });\nvar compression_1 = require(\"./cmap/wire_protocol/compression\");\nObject.defineProperty(exports, \"Compressor\", { enumerable: true, get: function () { return compression_1.Compressor; } });\nvar find_and_modify_1 = require(\"./operations/find_and_modify\");\nObject.defineProperty(exports, \"ReturnDocument\", { enumerable: true, get: function () { return find_and_modify_1.ReturnDocument; } });\nvar explain_1 = require(\"./explain\");\nObject.defineProperty(exports, \"ExplainVerbosity\", { enumerable: true, get: function () { return explain_1.ExplainVerbosity; } });\nvar read_concern_1 = require(\"./read_concern\");\nObject.defineProperty(exports, \"ReadConcernLevel\", { enumerable: true, get: function () { return read_concern_1.ReadConcernLevel; } });\nvar read_preference_1 = require(\"./read_preference\");\nObject.defineProperty(exports, \"ReadPreferenceMode\", { enumerable: true, get: function () { return read_preference_1.ReadPreferenceMode; } });\nvar mongo_client_2 = require(\"./mongo_client\");\nObject.defineProperty(exports, \"ServerApiVersion\", { enumerable: true, get: function () { return mongo_client_2.ServerApiVersion; } });\nvar mongo_types_2 = require(\"./mongo_types\");\nObject.defineProperty(exports, \"BSONType\", { enumerable: true, get: function () { return mongo_types_2.BSONType; } });\n// Helper classes\nvar write_concern_1 = require(\"./write_concern\");\nObject.defineProperty(exports, \"WriteConcern\", { enumerable: true, get: function () { return write_concern_1.WriteConcern; } });\nvar read_concern_2 = require(\"./read_concern\");\nObject.defineProperty(exports, \"ReadConcern\", { enumerable: true, get: function () { return read_concern_2.ReadConcern; } });\nvar read_preference_2 = require(\"./read_preference\");\nObject.defineProperty(exports, \"ReadPreference\", { enumerable: true, get: function () { return read_preference_2.ReadPreference; } });\n// events\nvar command_monitoring_events_1 = require(\"./cmap/command_monitoring_events\");\nObject.defineProperty(exports, \"CommandStartedEvent\", { enumerable: true, get: function () { return command_monitoring_events_1.CommandStartedEvent; } });\nObject.defineProperty(exports, \"CommandSucceededEvent\", { enumerable: true, get: function () { return command_monitoring_events_1.CommandSucceededEvent; } });\nObject.defineProperty(exports, \"CommandFailedEvent\", { enumerable: true, get: function () { return command_monitoring_events_1.CommandFailedEvent; } });\nvar connection_pool_events_1 = require(\"./cmap/connection_pool_events\");\nObject.defineProperty(exports, \"ConnectionCheckOutFailedEvent\", { enumerable: true, get: function () { return connection_pool_events_1.ConnectionCheckOutFailedEvent; } });\nObject.defineProperty(exports, \"ConnectionCheckOutStartedEvent\", { enumerable: true, get: function () { return connection_pool_events_1.ConnectionCheckOutStartedEvent; } });\nObject.defineProperty(exports, \"ConnectionCheckedInEvent\", { enumerable: true, get: function () { return connection_pool_events_1.ConnectionCheckedInEvent; } });\nObject.defineProperty(exports, \"ConnectionCheckedOutEvent\", { enumerable: true, get: function () { return connection_pool_events_1.ConnectionCheckedOutEvent; } });\nObject.defineProperty(exports, \"ConnectionClosedEvent\", { enumerable: true, get: function () { return connection_pool_events_1.ConnectionClosedEvent; } });\nObject.defineProperty(exports, \"ConnectionCreatedEvent\", { enumerable: true, get: function () { return connection_pool_events_1.ConnectionCreatedEvent; } });\nObject.defineProperty(exports, \"ConnectionPoolClearedEvent\", { enumerable: true, get: function () { return connection_pool_events_1.ConnectionPoolClearedEvent; } });\nObject.defineProperty(exports, \"ConnectionPoolClosedEvent\", { enumerable: true, get: function () { return connection_pool_events_1.ConnectionPoolClosedEvent; } });\nObject.defineProperty(exports, \"ConnectionPoolCreatedEvent\", { enumerable: true, get: function () { return connection_pool_events_1.ConnectionPoolCreatedEvent; } });\nObject.defineProperty(exports, \"ConnectionPoolMonitoringEvent\", { enumerable: true, get: function () { return connection_pool_events_1.ConnectionPoolMonitoringEvent; } });\nObject.defineProperty(exports, \"ConnectionReadyEvent\", { enumerable: true, get: function () { return connection_pool_events_1.ConnectionReadyEvent; } });\nvar events_1 = require(\"./sdam/events\");\nObject.defineProperty(exports, \"ServerHeartbeatStartedEvent\", { enumerable: true, get: function () { return events_1.ServerHeartbeatStartedEvent; } });\nObject.defineProperty(exports, \"ServerHeartbeatSucceededEvent\", { enumerable: true, get: function () { return events_1.ServerHeartbeatSucceededEvent; } });\nObject.defineProperty(exports, \"ServerHeartbeatFailedEvent\", { enumerable: true, get: function () { return events_1.ServerHeartbeatFailedEvent; } });\nObject.defineProperty(exports, \"ServerClosedEvent\", { enumerable: true, get: function () { return events_1.ServerClosedEvent; } });\nObject.defineProperty(exports, \"ServerDescriptionChangedEvent\", { enumerable: true, get: function () { return events_1.ServerDescriptionChangedEvent; } });\nObject.defineProperty(exports, \"ServerOpeningEvent\", { enumerable: true, get: function () { return events_1.ServerOpeningEvent; } });\nObject.defineProperty(exports, \"TopologyClosedEvent\", { enumerable: true, get: function () { return events_1.TopologyClosedEvent; } });\nObject.defineProperty(exports, \"TopologyDescriptionChangedEvent\", { enumerable: true, get: function () { return events_1.TopologyDescriptionChangedEvent; } });\nObject.defineProperty(exports, \"TopologyOpeningEvent\", { enumerable: true, get: function () { return events_1.TopologyOpeningEvent; } });\nvar srv_polling_1 = require(\"./sdam/srv_polling\");\nObject.defineProperty(exports, \"SrvPollingEvent\", { enumerable: true, get: function () { return srv_polling_1.SrvPollingEvent; } });\n//# sourceMappingURL=index.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.assertUninitialized = exports.AbstractCursor = exports.CURSOR_FLAGS = void 0;\nconst utils_1 = require(\"../utils\");\nconst bson_1 = require(\"../bson\");\nconst sessions_1 = require(\"../sessions\");\nconst error_1 = require(\"../error\");\nconst read_preference_1 = require(\"../read_preference\");\nconst stream_1 = require(\"stream\");\nconst execute_operation_1 = require(\"../operations/execute_operation\");\nconst get_more_1 = require(\"../operations/get_more\");\nconst read_concern_1 = require(\"../read_concern\");\nconst mongo_types_1 = require(\"../mongo_types\");\n/** @internal */\nconst kId = Symbol('id');\n/** @internal */\nconst kDocuments = Symbol('documents');\n/** @internal */\nconst kServer = Symbol('server');\n/** @internal */\nconst kNamespace = Symbol('namespace');\n/** @internal */\nconst kTopology = Symbol('topology');\n/** @internal */\nconst kSession = Symbol('session');\n/** @internal */\nconst kOptions = Symbol('options');\n/** @internal */\nconst kTransform = Symbol('transform');\n/** @internal */\nconst kInitialized = Symbol('initialized');\n/** @internal */\nconst kClosed = Symbol('closed');\n/** @internal */\nconst kKilled = Symbol('killed');\n/** @public */\nexports.CURSOR_FLAGS = [\n    'tailable',\n    'oplogReplay',\n    'noCursorTimeout',\n    'awaitData',\n    'exhaust',\n    'partial'\n];\n/** @public */\nclass AbstractCursor extends mongo_types_1.TypedEventEmitter {\n    /** @internal */\n    constructor(topology, namespace, options = {}) {\n        super();\n        this[kTopology] = topology;\n        this[kNamespace] = namespace;\n        this[kDocuments] = []; // TODO: https://github.com/microsoft/TypeScript/issues/36230\n        this[kInitialized] = false;\n        this[kClosed] = false;\n        this[kKilled] = false;\n        this[kOptions] = {\n            readPreference: options.readPreference && options.readPreference instanceof read_preference_1.ReadPreference\n                ? options.readPreference\n                : read_preference_1.ReadPreference.primary,\n            ...(0, bson_1.pluckBSONSerializeOptions)(options)\n        };\n        const readConcern = read_concern_1.ReadConcern.fromOptions(options);\n        if (readConcern) {\n            this[kOptions].readConcern = readConcern;\n        }\n        if (typeof options.batchSize === 'number') {\n            this[kOptions].batchSize = options.batchSize;\n        }\n        if (options.comment != null) {\n            this[kOptions].comment = options.comment;\n        }\n        if (typeof options.maxTimeMS === 'number') {\n            this[kOptions].maxTimeMS = options.maxTimeMS;\n        }\n        if (options.session instanceof sessions_1.ClientSession) {\n            this[kSession] = options.session;\n        }\n    }\n    get id() {\n        return this[kId];\n    }\n    /** @internal */\n    get topology() {\n        return this[kTopology];\n    }\n    /** @internal */\n    get server() {\n        return this[kServer];\n    }\n    get namespace() {\n        return this[kNamespace];\n    }\n    get readPreference() {\n        return this[kOptions].readPreference;\n    }\n    get readConcern() {\n        return this[kOptions].readConcern;\n    }\n    /** @internal */\n    get session() {\n        return this[kSession];\n    }\n    set session(clientSession) {\n        this[kSession] = clientSession;\n    }\n    /** @internal */\n    get cursorOptions() {\n        return this[kOptions];\n    }\n    get closed() {\n        return this[kClosed];\n    }\n    get killed() {\n        return this[kKilled];\n    }\n    get loadBalanced() {\n        return this[kTopology].loadBalanced;\n    }\n    /** Returns current buffered documents length */\n    bufferedCount() {\n        return this[kDocuments].length;\n    }\n    /** Returns current buffered documents */\n    readBufferedDocuments(number) {\n        return this[kDocuments].splice(0, number !== null && number !== void 0 ? number : this[kDocuments].length);\n    }\n    [Symbol.asyncIterator]() {\n        return {\n            next: () => this.next().then(value => value != null ? { value, done: false } : { value: undefined, done: true })\n        };\n    }\n    stream(options) {\n        if (options === null || options === void 0 ? void 0 : options.transform) {\n            const transform = options.transform;\n            const readable = makeCursorStream(this);\n            return readable.pipe(new stream_1.Transform({\n                objectMode: true,\n                highWaterMark: 1,\n                transform(chunk, _, callback) {\n                    try {\n                        const transformed = transform(chunk);\n                        callback(undefined, transformed);\n                    }\n                    catch (err) {\n                        callback(err);\n                    }\n                }\n            }));\n        }\n        return makeCursorStream(this);\n    }\n    hasNext(callback) {\n        return (0, utils_1.maybePromise)(callback, done => {\n            if (this[kId] === bson_1.Long.ZERO) {\n                return done(undefined, false);\n            }\n            if (this[kDocuments].length) {\n                return done(undefined, true);\n            }\n            next(this, true, (err, doc) => {\n                if (err)\n                    return done(err);\n                if (doc) {\n                    this[kDocuments].unshift(doc);\n                    done(undefined, true);\n                    return;\n                }\n                done(undefined, false);\n            });\n        });\n    }\n    next(callback) {\n        return (0, utils_1.maybePromise)(callback, done => {\n            if (this[kId] === bson_1.Long.ZERO) {\n                return done(new error_1.MongoCursorExhaustedError());\n            }\n            next(this, true, done);\n        });\n    }\n    tryNext(callback) {\n        return (0, utils_1.maybePromise)(callback, done => {\n            if (this[kId] === bson_1.Long.ZERO) {\n                return done(new error_1.MongoCursorExhaustedError());\n            }\n            next(this, false, done);\n        });\n    }\n    forEach(iterator, callback) {\n        if (typeof iterator !== 'function') {\n            throw new error_1.MongoInvalidArgumentError('Argument \"iterator\" must be a function');\n        }\n        return (0, utils_1.maybePromise)(callback, done => {\n            const transform = this[kTransform];\n            const fetchDocs = () => {\n                next(this, true, (err, doc) => {\n                    if (err || doc == null)\n                        return done(err);\n                    let result;\n                    // NOTE: no need to transform because `next` will do this automatically\n                    try {\n                        result = iterator(doc); // TODO(NODE-3283): Improve transform typing\n                    }\n                    catch (error) {\n                        return done(error);\n                    }\n                    if (result === false)\n                        return done();\n                    // these do need to be transformed since they are copying the rest of the batch\n                    const internalDocs = this[kDocuments].splice(0, this[kDocuments].length);\n                    for (let i = 0; i < internalDocs.length; ++i) {\n                        try {\n                            result = iterator((transform ? transform(internalDocs[i]) : internalDocs[i]) // TODO(NODE-3283): Improve transform typing\n                            );\n                        }\n                        catch (error) {\n                            return done(error);\n                        }\n                        if (result === false)\n                            return done();\n                    }\n                    fetchDocs();\n                });\n            };\n            fetchDocs();\n        });\n    }\n    close(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = options !== null && options !== void 0 ? options : {};\n        const needsToEmitClosed = !this[kClosed];\n        this[kClosed] = true;\n        return (0, utils_1.maybePromise)(callback, done => cleanupCursor(this, { needsToEmitClosed }, done));\n    }\n    toArray(callback) {\n        return (0, utils_1.maybePromise)(callback, done => {\n            const docs = [];\n            const transform = this[kTransform];\n            const fetchDocs = () => {\n                // NOTE: if we add a `nextBatch` then we should use it here\n                next(this, true, (err, doc) => {\n                    if (err)\n                        return done(err);\n                    if (doc == null)\n                        return done(undefined, docs);\n                    // NOTE: no need to transform because `next` will do this automatically\n                    docs.push(doc);\n                    // these do need to be transformed since they are copying the rest of the batch\n                    const internalDocs = (transform\n                        ? this[kDocuments].splice(0, this[kDocuments].length).map(transform)\n                        : this[kDocuments].splice(0, this[kDocuments].length)); // TODO(NODE-3283): Improve transform typing\n                    if (internalDocs) {\n                        docs.push(...internalDocs);\n                    }\n                    fetchDocs();\n                });\n            };\n            fetchDocs();\n        });\n    }\n    /**\n     * Add a cursor flag to the cursor\n     *\n     * @param flag - The flag to set, must be one of following ['tailable', 'oplogReplay', 'noCursorTimeout', 'awaitData', 'partial' -.\n     * @param value - The flag boolean value.\n     */\n    addCursorFlag(flag, value) {\n        assertUninitialized(this);\n        if (!exports.CURSOR_FLAGS.includes(flag)) {\n            throw new error_1.MongoInvalidArgumentError(`Flag ${flag} is not one of ${exports.CURSOR_FLAGS}`);\n        }\n        if (typeof value !== 'boolean') {\n            throw new error_1.MongoInvalidArgumentError(`Flag ${flag} must be a boolean value`);\n        }\n        this[kOptions][flag] = value;\n        return this;\n    }\n    /**\n     * Map all documents using the provided function\n     * If there is a transform set on the cursor, that will be called first and the result passed to\n     * this function's transform.\n     *\n     * @remarks\n     * **Note for Typescript Users:** adding a transform changes the return type of the iteration of this cursor,\n     * it **does not** return a new instance of a cursor. This means when calling map,\n     * you should always assign the result to a new variable in order to get a correctly typed cursor variable.\n     * Take note of the following example:\n     *\n     * @example\n     * ```typescript\n     * const cursor: FindCursor<Document> = coll.find();\n     * const mappedCursor: FindCursor<number> = cursor.map(doc => Object.keys(doc).length);\n     * const keyCounts: number[] = await mappedCursor.toArray(); // cursor.toArray() still returns Document[]\n     * ```\n     * @param transform - The mapping transformation method.\n     */\n    map(transform) {\n        assertUninitialized(this);\n        const oldTransform = this[kTransform]; // TODO(NODE-3283): Improve transform typing\n        if (oldTransform) {\n            this[kTransform] = doc => {\n                return transform(oldTransform(doc));\n            };\n        }\n        else {\n            this[kTransform] = transform;\n        }\n        return this;\n    }\n    /**\n     * Set the ReadPreference for the cursor.\n     *\n     * @param readPreference - The new read preference for the cursor.\n     */\n    withReadPreference(readPreference) {\n        assertUninitialized(this);\n        if (readPreference instanceof read_preference_1.ReadPreference) {\n            this[kOptions].readPreference = readPreference;\n        }\n        else if (typeof readPreference === 'string') {\n            this[kOptions].readPreference = read_preference_1.ReadPreference.fromString(readPreference);\n        }\n        else {\n            throw new error_1.MongoInvalidArgumentError(`Invalid read preference: ${readPreference}`);\n        }\n        return this;\n    }\n    /**\n     * Set the ReadPreference for the cursor.\n     *\n     * @param readPreference - The new read preference for the cursor.\n     */\n    withReadConcern(readConcern) {\n        assertUninitialized(this);\n        const resolvedReadConcern = read_concern_1.ReadConcern.fromOptions({ readConcern });\n        if (resolvedReadConcern) {\n            this[kOptions].readConcern = resolvedReadConcern;\n        }\n        return this;\n    }\n    /**\n     * Set a maxTimeMS on the cursor query, allowing for hard timeout limits on queries (Only supported on MongoDB 2.6 or higher)\n     *\n     * @param value - Number of milliseconds to wait before aborting the query.\n     */\n    maxTimeMS(value) {\n        assertUninitialized(this);\n        if (typeof value !== 'number') {\n            throw new error_1.MongoInvalidArgumentError('Argument for maxTimeMS must be a number');\n        }\n        this[kOptions].maxTimeMS = value;\n        return this;\n    }\n    /**\n     * Set the batch size for the cursor.\n     *\n     * @param value - The number of documents to return per batch. See {@link https://docs.mongodb.com/manual/reference/command/find/|find command documentation}.\n     */\n    batchSize(value) {\n        assertUninitialized(this);\n        if (this[kOptions].tailable) {\n            throw new error_1.MongoTailableCursorError('Tailable cursor does not support batchSize');\n        }\n        if (typeof value !== 'number') {\n            throw new error_1.MongoInvalidArgumentError('Operation \"batchSize\" requires an integer');\n        }\n        this[kOptions].batchSize = value;\n        return this;\n    }\n    /**\n     * Rewind this cursor to its uninitialized state. Any options that are present on the cursor will\n     * remain in effect. Iterating this cursor will cause new queries to be sent to the server, even\n     * if the resultant data has already been retrieved by this cursor.\n     */\n    rewind() {\n        if (!this[kInitialized]) {\n            return;\n        }\n        this[kId] = undefined;\n        this[kDocuments] = [];\n        this[kClosed] = false;\n        this[kKilled] = false;\n        this[kInitialized] = false;\n        const session = this[kSession];\n        if (session) {\n            // We only want to end this session if we created it, and it hasn't ended yet\n            if (session.explicit === false && !session.hasEnded) {\n                session.endSession();\n            }\n            this[kSession] = undefined;\n        }\n    }\n    /** @internal */\n    _getMore(batchSize, callback) {\n        const cursorId = this[kId];\n        const cursorNs = this[kNamespace];\n        const server = this[kServer];\n        if (cursorId == null) {\n            callback(new error_1.MongoRuntimeError('Unable to iterate cursor with no id'));\n            return;\n        }\n        if (server == null) {\n            callback(new error_1.MongoRuntimeError('Unable to iterate cursor without selected server'));\n            return;\n        }\n        const getMoreOperation = new get_more_1.GetMoreOperation(cursorNs, cursorId, server, {\n            ...this[kOptions],\n            session: this[kSession],\n            batchSize\n        });\n        (0, execute_operation_1.executeOperation)(this.topology, getMoreOperation, callback);\n    }\n}\nexports.AbstractCursor = AbstractCursor;\n/** @event */\nAbstractCursor.CLOSE = 'close';\nfunction nextDocument(cursor) {\n    if (cursor[kDocuments] == null || !cursor[kDocuments].length) {\n        return null;\n    }\n    const doc = cursor[kDocuments].shift();\n    if (doc) {\n        const transform = cursor[kTransform];\n        if (transform) {\n            return transform(doc);\n        }\n        return doc;\n    }\n    return null;\n}\nfunction next(cursor, blocking, callback) {\n    const cursorId = cursor[kId];\n    if (cursor.closed) {\n        return callback(undefined, null);\n    }\n    if (cursor[kDocuments] && cursor[kDocuments].length) {\n        callback(undefined, nextDocument(cursor));\n        return;\n    }\n    if (cursorId == null) {\n        // All cursors must operate within a session, one must be made implicitly if not explicitly provided\n        if (cursor[kSession] == null && cursor[kTopology].hasSessionSupport()) {\n            cursor[kSession] = cursor[kTopology].startSession({ owner: cursor, explicit: false });\n        }\n        cursor._initialize(cursor[kSession], (err, state) => {\n            if (state) {\n                const response = state.response;\n                cursor[kServer] = state.server;\n                cursor[kSession] = state.session;\n                if (response.cursor) {\n                    cursor[kId] =\n                        typeof response.cursor.id === 'number'\n                            ? bson_1.Long.fromNumber(response.cursor.id)\n                            : response.cursor.id;\n                    if (response.cursor.ns) {\n                        cursor[kNamespace] = (0, utils_1.ns)(response.cursor.ns);\n                    }\n                    cursor[kDocuments] = response.cursor.firstBatch;\n                }\n                else {\n                    // NOTE: This is for support of older servers (<3.2) which do not use commands\n                    cursor[kId] =\n                        typeof response.cursorId === 'number'\n                            ? bson_1.Long.fromNumber(response.cursorId)\n                            : response.cursorId;\n                    cursor[kDocuments] = response.documents;\n                }\n                // When server responses return without a cursor document, we close this cursor\n                // and return the raw server response. This is often the case for explain commands\n                // for example\n                if (cursor[kId] == null) {\n                    cursor[kId] = bson_1.Long.ZERO;\n                    // TODO(NODE-3286): ExecutionResult needs to accept a generic parameter\n                    cursor[kDocuments] = [state.response];\n                }\n            }\n            // the cursor is now initialized, even if an error occurred or it is dead\n            cursor[kInitialized] = true;\n            if (err || cursorIsDead(cursor)) {\n                return cleanupCursor(cursor, { error: err }, () => callback(err, nextDocument(cursor)));\n            }\n            next(cursor, blocking, callback);\n        });\n        return;\n    }\n    if (cursorIsDead(cursor)) {\n        return cleanupCursor(cursor, undefined, () => callback(undefined, null));\n    }\n    // otherwise need to call getMore\n    const batchSize = cursor[kOptions].batchSize || 1000;\n    cursor._getMore(batchSize, (err, response) => {\n        if (response) {\n            const cursorId = typeof response.cursor.id === 'number'\n                ? bson_1.Long.fromNumber(response.cursor.id)\n                : response.cursor.id;\n            cursor[kDocuments] = response.cursor.nextBatch;\n            cursor[kId] = cursorId;\n        }\n        if (err || cursorIsDead(cursor)) {\n            return cleanupCursor(cursor, { error: err }, () => callback(err, nextDocument(cursor)));\n        }\n        if (cursor[kDocuments].length === 0 && blocking === false) {\n            return callback(undefined, null);\n        }\n        next(cursor, blocking, callback);\n    });\n}\nfunction cursorIsDead(cursor) {\n    const cursorId = cursor[kId];\n    return !!cursorId && cursorId.isZero();\n}\nfunction cleanupCursor(cursor, options, callback) {\n    var _a;\n    const cursorId = cursor[kId];\n    const cursorNs = cursor[kNamespace];\n    const server = cursor[kServer];\n    const session = cursor[kSession];\n    const error = options === null || options === void 0 ? void 0 : options.error;\n    const needsToEmitClosed = (_a = options === null || options === void 0 ? void 0 : options.needsToEmitClosed) !== null && _a !== void 0 ? _a : cursor[kDocuments].length === 0;\n    if (error) {\n        if (cursor.loadBalanced && error instanceof error_1.MongoNetworkError) {\n            return completeCleanup();\n        }\n    }\n    if (cursorId == null || server == null || cursorId.isZero() || cursorNs == null) {\n        if (needsToEmitClosed) {\n            cursor[kClosed] = true;\n            cursor[kId] = bson_1.Long.ZERO;\n            cursor.emit(AbstractCursor.CLOSE);\n        }\n        if (session) {\n            if (session.owner === cursor) {\n                return session.endSession({ error }, callback);\n            }\n            if (!session.inTransaction()) {\n                (0, sessions_1.maybeClearPinnedConnection)(session, { error });\n            }\n        }\n        return callback();\n    }\n    function completeCleanup() {\n        if (session) {\n            if (session.owner === cursor) {\n                return session.endSession({ error }, () => {\n                    cursor.emit(AbstractCursor.CLOSE);\n                    callback();\n                });\n            }\n            if (!session.inTransaction()) {\n                (0, sessions_1.maybeClearPinnedConnection)(session, { error });\n            }\n        }\n        cursor.emit(AbstractCursor.CLOSE);\n        return callback();\n    }\n    cursor[kKilled] = true;\n    server.killCursors(cursorNs, [cursorId], { ...(0, bson_1.pluckBSONSerializeOptions)(cursor[kOptions]), session }, () => completeCleanup());\n}\n/** @internal */\nfunction assertUninitialized(cursor) {\n    if (cursor[kInitialized]) {\n        throw new error_1.MongoCursorInUseError();\n    }\n}\nexports.assertUninitialized = assertUninitialized;\nfunction makeCursorStream(cursor) {\n    const readable = new stream_1.Readable({\n        objectMode: true,\n        autoDestroy: false,\n        highWaterMark: 1\n    });\n    let initialized = false;\n    let reading = false;\n    let needToClose = true; // NOTE: we must close the cursor if we never read from it, use `_construct` in future node versions\n    readable._read = function () {\n        if (initialized === false) {\n            needToClose = false;\n            initialized = true;\n        }\n        if (!reading) {\n            reading = true;\n            readNext();\n        }\n    };\n    readable._destroy = function (error, cb) {\n        if (needToClose) {\n            cursor.close(err => process.nextTick(cb, err || error));\n        }\n        else {\n            cb(error);\n        }\n    };\n    function readNext() {\n        needToClose = false;\n        next(cursor, true, (err, result) => {\n            needToClose = err ? !cursor.closed : result != null;\n            if (err) {\n                // NOTE: This is questionable, but we have a test backing the behavior. It seems the\n                //       desired behavior is that a stream ends cleanly when a user explicitly closes\n                //       a client during iteration. Alternatively, we could do the \"right\" thing and\n                //       propagate the error message by removing this special case.\n                if (err.message.match(/server is closed/)) {\n                    cursor.close();\n                    return readable.push(null);\n                }\n                // NOTE: This is also perhaps questionable. The rationale here is that these errors tend\n                //       to be \"operation interrupted\", where a cursor has been closed but there is an\n                //       active getMore in-flight. This used to check if the cursor was killed but once\n                //       that changed to happen in cleanup legitimate errors would not destroy the\n                //       stream. There are change streams test specifically test these cases.\n                if (err.message.match(/interrupted/)) {\n                    return readable.push(null);\n                }\n                return readable.destroy(err);\n            }\n            if (result == null) {\n                readable.push(null);\n            }\n            else if (readable.destroyed) {\n                cursor.close();\n            }\n            else {\n                if (readable.push(result)) {\n                    return readNext();\n                }\n                reading = false;\n            }\n        });\n    }\n    return readable;\n}\n//# sourceMappingURL=abstract_cursor.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.shuffle = exports.parsePackageVersion = exports.supportsRetryableWrites = exports.enumToString = exports.emitWarningOnce = exports.emitWarning = exports.MONGODB_WARNING_CODE = exports.DEFAULT_PK_FACTORY = exports.HostAddress = exports.BufferPool = exports.deepCopy = exports.isRecord = exports.setDifference = exports.isSuperset = exports.resolveOptions = exports.hasAtomicOperators = exports.makeInterruptibleAsyncInterval = exports.calculateDurationInMs = exports.now = exports.makeClientMetadata = exports.makeStateMachine = exports.errorStrictEqual = exports.arrayStrictEqual = exports.eachAsyncSeries = exports.eachAsync = exports.collationNotSupported = exports.maxWireVersion = exports.uuidV4 = exports.databaseNamespace = exports.maybePromise = exports.makeCounter = exports.MongoDBNamespace = exports.ns = exports.deprecateOptions = exports.defaultMsgHandler = exports.getTopology = exports.decorateWithExplain = exports.decorateWithReadConcern = exports.decorateWithCollation = exports.isPromiseLike = exports.applyWriteConcern = exports.applyRetryableWrites = exports.executeLegacyOperation = exports.filterOptions = exports.mergeOptions = exports.isObject = exports.parseIndexOptions = exports.normalizeHintField = exports.checkCollectionName = exports.MAX_JS_INT = void 0;\nconst os = require(\"os\");\nconst crypto = require(\"crypto\");\nconst promise_provider_1 = require(\"./promise_provider\");\nconst error_1 = require(\"./error\");\nconst write_concern_1 = require(\"./write_concern\");\nconst common_1 = require(\"./sdam/common\");\nconst read_concern_1 = require(\"./read_concern\");\nconst bson_1 = require(\"./bson\");\nconst read_preference_1 = require(\"./read_preference\");\nconst url_1 = require(\"url\");\nconst constants_1 = require(\"./cmap/wire_protocol/constants\");\nexports.MAX_JS_INT = Number.MAX_SAFE_INTEGER + 1;\n/**\n * Throws if collectionName is not a valid mongodb collection namespace.\n * @internal\n */\nfunction checkCollectionName(collectionName) {\n    if ('string' !== typeof collectionName) {\n        throw new error_1.MongoInvalidArgumentError('Collection name must be a String');\n    }\n    if (!collectionName || collectionName.indexOf('..') !== -1) {\n        throw new error_1.MongoInvalidArgumentError('Collection names cannot be empty');\n    }\n    if (collectionName.indexOf('$') !== -1 &&\n        collectionName.match(/((^\\$cmd)|(oplog\\.\\$main))/) == null) {\n        // TODO(NODE-3483): Use MongoNamespace static method\n        throw new error_1.MongoInvalidArgumentError(\"Collection names must not contain '$'\");\n    }\n    if (collectionName.match(/^\\.|\\.$/) != null) {\n        // TODO(NODE-3483): Use MongoNamespace static method\n        throw new error_1.MongoInvalidArgumentError(\"Collection names must not start or end with '.'\");\n    }\n    // Validate that we are not passing 0x00 in the collection name\n    if (collectionName.indexOf('\\x00') !== -1) {\n        // TODO(NODE-3483): Use MongoNamespace static method\n        throw new error_1.MongoInvalidArgumentError('Collection names cannot contain a null character');\n    }\n}\nexports.checkCollectionName = checkCollectionName;\n/**\n * Ensure Hint field is in a shape we expect:\n * - object of index names mapping to 1 or -1\n * - just an index name\n * @internal\n */\nfunction normalizeHintField(hint) {\n    let finalHint = undefined;\n    if (typeof hint === 'string') {\n        finalHint = hint;\n    }\n    else if (Array.isArray(hint)) {\n        finalHint = {};\n        hint.forEach(param => {\n            finalHint[param] = 1;\n        });\n    }\n    else if (hint != null && typeof hint === 'object') {\n        finalHint = {};\n        for (const name in hint) {\n            finalHint[name] = hint[name];\n        }\n    }\n    return finalHint;\n}\nexports.normalizeHintField = normalizeHintField;\n/**\n * Create an index specifier based on\n * @internal\n */\nfunction parseIndexOptions(indexSpec) {\n    const fieldHash = {};\n    const indexes = [];\n    let keys;\n    // Get all the fields accordingly\n    if ('string' === typeof indexSpec) {\n        // 'type'\n        indexes.push(indexSpec + '_' + 1);\n        fieldHash[indexSpec] = 1;\n    }\n    else if (Array.isArray(indexSpec)) {\n        indexSpec.forEach((f) => {\n            if ('string' === typeof f) {\n                // [{location:'2d'}, 'type']\n                indexes.push(f + '_' + 1);\n                fieldHash[f] = 1;\n            }\n            else if (Array.isArray(f)) {\n                // [['location', '2d'],['type', 1]]\n                indexes.push(f[0] + '_' + (f[1] || 1));\n                fieldHash[f[0]] = f[1] || 1;\n            }\n            else if (isObject(f)) {\n                // [{location:'2d'}, {type:1}]\n                keys = Object.keys(f);\n                keys.forEach(k => {\n                    indexes.push(k + '_' + f[k]);\n                    fieldHash[k] = f[k];\n                });\n            }\n            else {\n                // undefined (ignore)\n            }\n        });\n    }\n    else if (isObject(indexSpec)) {\n        // {location:'2d', type:1}\n        keys = Object.keys(indexSpec);\n        Object.entries(indexSpec).forEach(([key, value]) => {\n            indexes.push(key + '_' + value);\n            fieldHash[key] = value;\n        });\n    }\n    return {\n        name: indexes.join('_'),\n        keys: keys,\n        fieldHash: fieldHash\n    };\n}\nexports.parseIndexOptions = parseIndexOptions;\n/**\n * Checks if arg is an Object:\n * - **NOTE**: the check is based on the `[Symbol.toStringTag]() === 'Object'`\n * @internal\n */\n// eslint-disable-next-line @typescript-eslint/ban-types\nfunction isObject(arg) {\n    return '[object Object]' === Object.prototype.toString.call(arg);\n}\nexports.isObject = isObject;\n/** @internal */\nfunction mergeOptions(target, source) {\n    return { ...target, ...source };\n}\nexports.mergeOptions = mergeOptions;\n/** @internal */\nfunction filterOptions(options, names) {\n    const filterOptions = {};\n    for (const name in options) {\n        if (names.includes(name)) {\n            filterOptions[name] = options[name];\n        }\n    }\n    // Filtered options\n    return filterOptions;\n}\nexports.filterOptions = filterOptions;\n/**\n * Executes the given operation with provided arguments.\n *\n * @remarks\n * This method reduces large amounts of duplication in the entire codebase by providing\n * a single point for determining whether callbacks or promises should be used. Additionally\n * it allows for a single point of entry to provide features such as implicit sessions, which\n * are required by the Driver Sessions specification in the event that a ClientSession is\n * not provided\n *\n * @internal\n *\n * @param topology - The topology to execute this operation on\n * @param operation - The operation to execute\n * @param args - Arguments to apply the provided operation\n * @param options - Options that modify the behavior of the method\n */\nfunction executeLegacyOperation(topology, operation, args, options) {\n    const Promise = promise_provider_1.PromiseProvider.get();\n    if (!Array.isArray(args)) {\n        // TODO(NODE-3483)\n        throw new error_1.MongoRuntimeError('This method requires an array of arguments to apply');\n    }\n    options = options !== null && options !== void 0 ? options : {};\n    let callback = args[args.length - 1];\n    // The driver sessions spec mandates that we implicitly create sessions for operations\n    // that are not explicitly provided with a session.\n    let session;\n    let opOptions;\n    let owner;\n    if (!options.skipSessions && topology.hasSessionSupport()) {\n        opOptions = args[args.length - 2];\n        if (opOptions == null || opOptions.session == null) {\n            owner = Symbol();\n            session = topology.startSession({ owner });\n            const optionsIndex = args.length - 2;\n            args[optionsIndex] = Object.assign({}, args[optionsIndex], { session: session });\n        }\n        else if (opOptions.session && opOptions.session.hasEnded) {\n            throw new error_1.MongoExpiredSessionError();\n        }\n    }\n    function makeExecuteCallback(resolve, reject) {\n        return function (err, result) {\n            if (session && session.owner === owner && !(options === null || options === void 0 ? void 0 : options.returnsCursor)) {\n                session.endSession(() => {\n                    delete opOptions.session;\n                    if (err)\n                        return reject(err);\n                    resolve(result);\n                });\n            }\n            else {\n                if (err)\n                    return reject(err);\n                resolve(result);\n            }\n        };\n    }\n    // Execute using callback\n    if (typeof callback === 'function') {\n        callback = args.pop();\n        const handler = makeExecuteCallback(result => callback(undefined, result), err => callback(err, null));\n        args.push(handler);\n        try {\n            return operation(...args);\n        }\n        catch (e) {\n            handler(e);\n            throw e;\n        }\n    }\n    // Return a Promise\n    if (args[args.length - 1] != null) {\n        // TODO(NODE-3483)\n        throw new error_1.MongoRuntimeError('Final argument to `executeLegacyOperation` must be a callback');\n    }\n    return new Promise((resolve, reject) => {\n        const handler = makeExecuteCallback(resolve, reject);\n        args[args.length - 1] = handler;\n        try {\n            return operation(...args);\n        }\n        catch (e) {\n            handler(e);\n        }\n    });\n}\nexports.executeLegacyOperation = executeLegacyOperation;\n/**\n * Applies retryWrites: true to a command if retryWrites is set on the command's database.\n * @internal\n *\n * @param target - The target command to which we will apply retryWrites.\n * @param db - The database from which we can inherit a retryWrites value.\n */\nfunction applyRetryableWrites(target, db) {\n    var _a;\n    if (db && ((_a = db.s.options) === null || _a === void 0 ? void 0 : _a.retryWrites)) {\n        target.retryWrites = true;\n    }\n    return target;\n}\nexports.applyRetryableWrites = applyRetryableWrites;\n/**\n * Applies a write concern to a command based on well defined inheritance rules, optionally\n * detecting support for the write concern in the first place.\n * @internal\n *\n * @param target - the target command we will be applying the write concern to\n * @param sources - sources where we can inherit default write concerns from\n * @param options - optional settings passed into a command for write concern overrides\n */\nfunction applyWriteConcern(target, sources, options) {\n    options = options !== null && options !== void 0 ? options : {};\n    const db = sources.db;\n    const coll = sources.collection;\n    if (options.session && options.session.inTransaction()) {\n        // writeConcern is not allowed within a multi-statement transaction\n        if (target.writeConcern) {\n            delete target.writeConcern;\n        }\n        return target;\n    }\n    const writeConcern = write_concern_1.WriteConcern.fromOptions(options);\n    if (writeConcern) {\n        return Object.assign(target, { writeConcern });\n    }\n    if (coll && coll.writeConcern) {\n        return Object.assign(target, { writeConcern: Object.assign({}, coll.writeConcern) });\n    }\n    if (db && db.writeConcern) {\n        return Object.assign(target, { writeConcern: Object.assign({}, db.writeConcern) });\n    }\n    return target;\n}\nexports.applyWriteConcern = applyWriteConcern;\n/**\n * Checks if a given value is a Promise\n *\n * @typeParam T - The result type of maybePromise\n * @param maybePromise - An object that could be a promise\n * @returns true if the provided value is a Promise\n */\nfunction isPromiseLike(maybePromise) {\n    return !!maybePromise && typeof maybePromise.then === 'function';\n}\nexports.isPromiseLike = isPromiseLike;\n/**\n * Applies collation to a given command.\n * @internal\n *\n * @param command - the command on which to apply collation\n * @param target - target of command\n * @param options - options containing collation settings\n */\nfunction decorateWithCollation(command, target, options) {\n    const capabilities = getTopology(target).capabilities;\n    if (options.collation && typeof options.collation === 'object') {\n        if (capabilities && capabilities.commandsTakeCollation) {\n            command.collation = options.collation;\n        }\n        else {\n            throw new error_1.MongoCompatibilityError(`Current topology does not support collation`);\n        }\n    }\n}\nexports.decorateWithCollation = decorateWithCollation;\n/**\n * Applies a read concern to a given command.\n * @internal\n *\n * @param command - the command on which to apply the read concern\n * @param coll - the parent collection of the operation calling this method\n */\nfunction decorateWithReadConcern(command, coll, options) {\n    if (options && options.session && options.session.inTransaction()) {\n        return;\n    }\n    const readConcern = Object.assign({}, command.readConcern || {});\n    if (coll.s.readConcern) {\n        Object.assign(readConcern, coll.s.readConcern);\n    }\n    if (Object.keys(readConcern).length > 0) {\n        Object.assign(command, { readConcern: readConcern });\n    }\n}\nexports.decorateWithReadConcern = decorateWithReadConcern;\n/**\n * Applies an explain to a given command.\n * @internal\n *\n * @param command - the command on which to apply the explain\n * @param options - the options containing the explain verbosity\n */\nfunction decorateWithExplain(command, explain) {\n    if (command.explain) {\n        return command;\n    }\n    return { explain: command, verbosity: explain.verbosity };\n}\nexports.decorateWithExplain = decorateWithExplain;\n/**\n * A helper function to get the topology from a given provider. Throws\n * if the topology cannot be found.\n * @internal\n */\nfunction getTopology(provider) {\n    if (`topology` in provider && provider.topology) {\n        return provider.topology;\n    }\n    else if ('client' in provider.s && provider.s.client.topology) {\n        return provider.s.client.topology;\n    }\n    else if ('db' in provider.s && provider.s.db.s.client.topology) {\n        return provider.s.db.s.client.topology;\n    }\n    throw new error_1.MongoNotConnectedError('MongoClient must be connected to perform this operation');\n}\nexports.getTopology = getTopology;\n/**\n * Default message handler for generating deprecation warnings.\n * @internal\n *\n * @param name - function name\n * @param option - option name\n * @returns warning message\n */\nfunction defaultMsgHandler(name, option) {\n    return `${name} option [${option}] is deprecated and will be removed in a later version.`;\n}\nexports.defaultMsgHandler = defaultMsgHandler;\n/**\n * Deprecates a given function's options.\n * @internal\n *\n * @param this - the bound class if this is a method\n * @param config - configuration for deprecation\n * @param fn - the target function of deprecation\n * @returns modified function that warns once per deprecated option, and executes original function\n */\nfunction deprecateOptions(config, fn) {\n    if (process.noDeprecation === true) {\n        return fn;\n    }\n    const msgHandler = config.msgHandler ? config.msgHandler : defaultMsgHandler;\n    const optionsWarned = new Set();\n    function deprecated(...args) {\n        const options = args[config.optionsIndex];\n        // ensure options is a valid, non-empty object, otherwise short-circuit\n        if (!isObject(options) || Object.keys(options).length === 0) {\n            return fn.bind(this)(...args); // call the function, no change\n        }\n        // interrupt the function call with a warning\n        for (const deprecatedOption of config.deprecatedOptions) {\n            if (deprecatedOption in options && !optionsWarned.has(deprecatedOption)) {\n                optionsWarned.add(deprecatedOption);\n                const msg = msgHandler(config.name, deprecatedOption);\n                emitWarning(msg);\n                if (this && 'getLogger' in this) {\n                    const logger = this.getLogger();\n                    if (logger) {\n                        logger.warn(msg);\n                    }\n                }\n            }\n        }\n        return fn.bind(this)(...args);\n    }\n    // These lines copied from https://github.com/nodejs/node/blob/25e5ae41688676a5fd29b2e2e7602168eee4ceb5/lib/internal/util.js#L73-L80\n    // The wrapper will keep the same prototype as fn to maintain prototype chain\n    Object.setPrototypeOf(deprecated, fn);\n    if (fn.prototype) {\n        // Setting this (rather than using Object.setPrototype, as above) ensures\n        // that calling the unwrapped constructor gives an instanceof the wrapped\n        // constructor.\n        deprecated.prototype = fn.prototype;\n    }\n    return deprecated;\n}\nexports.deprecateOptions = deprecateOptions;\n/** @internal */\nfunction ns(ns) {\n    return MongoDBNamespace.fromString(ns);\n}\nexports.ns = ns;\n/** @public */\nclass MongoDBNamespace {\n    /**\n     * Create a namespace object\n     *\n     * @param db - database name\n     * @param collection - collection name\n     */\n    constructor(db, collection) {\n        this.db = db;\n        this.collection = collection;\n    }\n    toString() {\n        return this.collection ? `${this.db}.${this.collection}` : this.db;\n    }\n    withCollection(collection) {\n        return new MongoDBNamespace(this.db, collection);\n    }\n    static fromString(namespace) {\n        if (!namespace) {\n            // TODO(NODE-3483): Replace with MongoNamespaceError\n            throw new error_1.MongoRuntimeError(`Cannot parse namespace from \"${namespace}\"`);\n        }\n        const [db, ...collection] = namespace.split('.');\n        return new MongoDBNamespace(db, collection.join('.'));\n    }\n}\nexports.MongoDBNamespace = MongoDBNamespace;\n/** @internal */\nfunction* makeCounter(seed = 0) {\n    let count = seed;\n    while (true) {\n        const newCount = count;\n        count += 1;\n        yield newCount;\n    }\n}\nexports.makeCounter = makeCounter;\n/**\n * Helper function for either accepting a callback, or returning a promise\n * @internal\n *\n * @param callback - The last function argument in exposed method, controls if a Promise is returned\n * @param wrapper - A function that wraps the callback\n * @returns Returns void if a callback is supplied, else returns a Promise.\n */\nfunction maybePromise(callback, wrapper) {\n    const Promise = promise_provider_1.PromiseProvider.get();\n    let result;\n    if (typeof callback !== 'function') {\n        result = new Promise((resolve, reject) => {\n            callback = (err, res) => {\n                if (err)\n                    return reject(err);\n                resolve(res);\n            };\n        });\n    }\n    wrapper((err, res) => {\n        if (err != null) {\n            try {\n                // eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n                callback(err);\n            }\n            catch (error) {\n                process.nextTick(() => {\n                    throw error;\n                });\n            }\n            return;\n        }\n        // eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n        callback(err, res);\n    });\n    return result;\n}\nexports.maybePromise = maybePromise;\n/** @internal */\nfunction databaseNamespace(ns) {\n    return ns.split('.')[0];\n}\nexports.databaseNamespace = databaseNamespace;\n/**\n * Synchronously Generate a UUIDv4\n * @internal\n */\nfunction uuidV4() {\n    const result = crypto.randomBytes(16);\n    result[6] = (result[6] & 0x0f) | 0x40;\n    result[8] = (result[8] & 0x3f) | 0x80;\n    return result;\n}\nexports.uuidV4 = uuidV4;\n/**\n * A helper function for determining `maxWireVersion` between legacy and new topology instances\n * @internal\n */\nfunction maxWireVersion(topologyOrServer) {\n    if (topologyOrServer) {\n        if (topologyOrServer.loadBalanced) {\n            // Since we do not have a monitor, we assume the load balanced server is always\n            // pointed at the latest mongodb version. There is a risk that for on-prem\n            // deployments that don't upgrade immediately that this could alert to the\n            // application that a feature is avaiable that is actually not.\n            return constants_1.MAX_SUPPORTED_WIRE_VERSION;\n        }\n        if (topologyOrServer.ismaster) {\n            return topologyOrServer.ismaster.maxWireVersion;\n        }\n        if ('lastIsMaster' in topologyOrServer && typeof topologyOrServer.lastIsMaster === 'function') {\n            const lastIsMaster = topologyOrServer.lastIsMaster();\n            if (lastIsMaster) {\n                return lastIsMaster.maxWireVersion;\n            }\n        }\n        if (topologyOrServer.description &&\n            'maxWireVersion' in topologyOrServer.description &&\n            topologyOrServer.description.maxWireVersion != null) {\n            return topologyOrServer.description.maxWireVersion;\n        }\n    }\n    return 0;\n}\nexports.maxWireVersion = maxWireVersion;\n/**\n * Checks that collation is supported by server.\n * @internal\n *\n * @param server - to check against\n * @param cmd - object where collation may be specified\n */\nfunction collationNotSupported(server, cmd) {\n    return cmd && cmd.collation && maxWireVersion(server) < 5;\n}\nexports.collationNotSupported = collationNotSupported;\n/**\n * Applies the function `eachFn` to each item in `arr`, in parallel.\n * @internal\n *\n * @param arr - An array of items to asynchronously iterate over\n * @param eachFn - A function to call on each item of the array. The callback signature is `(item, callback)`, where the callback indicates iteration is complete.\n * @param callback - The callback called after every item has been iterated\n */\nfunction eachAsync(arr, eachFn, callback) {\n    arr = arr || [];\n    let idx = 0;\n    let awaiting = 0;\n    for (idx = 0; idx < arr.length; ++idx) {\n        awaiting++;\n        eachFn(arr[idx], eachCallback);\n    }\n    if (awaiting === 0) {\n        callback();\n        return;\n    }\n    function eachCallback(err) {\n        awaiting--;\n        if (err) {\n            callback(err);\n            return;\n        }\n        if (idx === arr.length && awaiting <= 0) {\n            callback();\n        }\n    }\n}\nexports.eachAsync = eachAsync;\n/** @internal */\nfunction eachAsyncSeries(arr, eachFn, callback) {\n    arr = arr || [];\n    let idx = 0;\n    let awaiting = arr.length;\n    if (awaiting === 0) {\n        callback();\n        return;\n    }\n    function eachCallback(err) {\n        idx++;\n        awaiting--;\n        if (err) {\n            callback(err);\n            return;\n        }\n        if (idx === arr.length && awaiting <= 0) {\n            callback();\n            return;\n        }\n        eachFn(arr[idx], eachCallback);\n    }\n    eachFn(arr[idx], eachCallback);\n}\nexports.eachAsyncSeries = eachAsyncSeries;\n/** @internal */\nfunction arrayStrictEqual(arr, arr2) {\n    if (!Array.isArray(arr) || !Array.isArray(arr2)) {\n        return false;\n    }\n    return arr.length === arr2.length && arr.every((elt, idx) => elt === arr2[idx]);\n}\nexports.arrayStrictEqual = arrayStrictEqual;\n/** @internal */\nfunction errorStrictEqual(lhs, rhs) {\n    if (lhs === rhs) {\n        return true;\n    }\n    if (!lhs || !rhs) {\n        return lhs === rhs;\n    }\n    if ((lhs == null && rhs != null) || (lhs != null && rhs == null)) {\n        return false;\n    }\n    if (lhs.constructor.name !== rhs.constructor.name) {\n        return false;\n    }\n    if (lhs.message !== rhs.message) {\n        return false;\n    }\n    return true;\n}\nexports.errorStrictEqual = errorStrictEqual;\n/** @internal */\nfunction makeStateMachine(stateTable) {\n    return function stateTransition(target, newState) {\n        const legalStates = stateTable[target.s.state];\n        if (legalStates && legalStates.indexOf(newState) < 0) {\n            throw new error_1.MongoRuntimeError(`illegal state transition from [${target.s.state}] => [${newState}], allowed: [${legalStates}]`);\n        }\n        target.emit('stateChanged', target.s.state, newState);\n        target.s.state = newState;\n    };\n}\nexports.makeStateMachine = makeStateMachine;\n// eslint-disable-next-line @typescript-eslint/no-var-requires\nconst NODE_DRIVER_VERSION = require('../package.json').version;\nfunction makeClientMetadata(options) {\n    options = options !== null && options !== void 0 ? options : {};\n    const metadata = {\n        driver: {\n            name: 'nodejs',\n            version: NODE_DRIVER_VERSION\n        },\n        os: {\n            type: os.type(),\n            name: process.platform,\n            architecture: process.arch,\n            version: os.release()\n        },\n        platform: `Node.js ${process.version}, ${os.endianness()} (unified)`\n    };\n    // support optionally provided wrapping driver info\n    if (options.driverInfo) {\n        if (options.driverInfo.name) {\n            metadata.driver.name = `${metadata.driver.name}|${options.driverInfo.name}`;\n        }\n        if (options.driverInfo.version) {\n            metadata.version = `${metadata.driver.version}|${options.driverInfo.version}`;\n        }\n        if (options.driverInfo.platform) {\n            metadata.platform = `${metadata.platform}|${options.driverInfo.platform}`;\n        }\n    }\n    if (options.appName) {\n        // MongoDB requires the appName not exceed a byte length of 128\n        const buffer = Buffer.from(options.appName);\n        metadata.application = {\n            name: buffer.byteLength > 128 ? buffer.slice(0, 128).toString('utf8') : options.appName\n        };\n    }\n    return metadata;\n}\nexports.makeClientMetadata = makeClientMetadata;\n/** @internal */\nfunction now() {\n    const hrtime = process.hrtime();\n    return Math.floor(hrtime[0] * 1000 + hrtime[1] / 1000000);\n}\nexports.now = now;\n/** @internal */\nfunction calculateDurationInMs(started) {\n    if (typeof started !== 'number') {\n        throw new error_1.MongoInvalidArgumentError('Numeric value required to calculate duration');\n    }\n    const elapsed = now() - started;\n    return elapsed < 0 ? 0 : elapsed;\n}\nexports.calculateDurationInMs = calculateDurationInMs;\n/**\n * Creates an interval timer which is able to be woken up sooner than\n * the interval. The timer will also debounce multiple calls to wake\n * ensuring that the function is only ever called once within a minimum\n * interval window.\n * @internal\n *\n * @param fn - An async function to run on an interval, must accept a `callback` as its only parameter\n */\nfunction makeInterruptibleAsyncInterval(fn, options) {\n    let timerId;\n    let lastCallTime;\n    let cannotBeExpedited = false;\n    let stopped = false;\n    options = options !== null && options !== void 0 ? options : {};\n    const interval = options.interval || 1000;\n    const minInterval = options.minInterval || 500;\n    const immediate = typeof options.immediate === 'boolean' ? options.immediate : false;\n    const clock = typeof options.clock === 'function' ? options.clock : now;\n    function wake() {\n        const currentTime = clock();\n        const nextScheduledCallTime = lastCallTime + interval;\n        const timeUntilNextCall = nextScheduledCallTime - currentTime;\n        // For the streaming protocol: there is nothing obviously stopping this\n        // interval from being woken up again while we are waiting \"infinitely\"\n        // for `fn` to be called again`. Since the function effectively\n        // never completes, the `timeUntilNextCall` will continue to grow\n        // negatively unbounded, so it will never trigger a reschedule here.\n        // This is possible in virtualized environments like AWS Lambda where our\n        // clock is unreliable. In these cases the timer is \"running\" but never\n        // actually completes, so we want to execute immediately and then attempt\n        // to reschedule.\n        if (timeUntilNextCall < 0) {\n            executeAndReschedule();\n            return;\n        }\n        // debounce multiple calls to wake within the `minInterval`\n        if (cannotBeExpedited) {\n            return;\n        }\n        // reschedule a call as soon as possible, ensuring the call never happens\n        // faster than the `minInterval`\n        if (timeUntilNextCall > minInterval) {\n            reschedule(minInterval);\n            cannotBeExpedited = true;\n        }\n    }\n    function stop() {\n        stopped = true;\n        if (timerId) {\n            clearTimeout(timerId);\n            timerId = undefined;\n        }\n        lastCallTime = 0;\n        cannotBeExpedited = false;\n    }\n    function reschedule(ms) {\n        if (stopped)\n            return;\n        if (timerId) {\n            clearTimeout(timerId);\n        }\n        timerId = setTimeout(executeAndReschedule, ms || interval);\n    }\n    function executeAndReschedule() {\n        cannotBeExpedited = false;\n        lastCallTime = clock();\n        fn(err => {\n            if (err)\n                throw err;\n            reschedule(interval);\n        });\n    }\n    if (immediate) {\n        executeAndReschedule();\n    }\n    else {\n        lastCallTime = clock();\n        reschedule(undefined);\n    }\n    return { wake, stop };\n}\nexports.makeInterruptibleAsyncInterval = makeInterruptibleAsyncInterval;\n/** @internal */\nfunction hasAtomicOperators(doc) {\n    if (Array.isArray(doc)) {\n        for (const document of doc) {\n            if (hasAtomicOperators(document)) {\n                return true;\n            }\n        }\n        return false;\n    }\n    const keys = Object.keys(doc);\n    return keys.length > 0 && keys[0][0] === '$';\n}\nexports.hasAtomicOperators = hasAtomicOperators;\n/**\n * Merge inherited properties from parent into options, prioritizing values from options,\n * then values from parent.\n * @internal\n */\nfunction resolveOptions(parent, options) {\n    var _a, _b, _c;\n    const result = Object.assign({}, options, (0, bson_1.resolveBSONOptions)(options, parent));\n    // Users cannot pass a readConcern/writeConcern to operations in a transaction\n    const session = options === null || options === void 0 ? void 0 : options.session;\n    if (!(session === null || session === void 0 ? void 0 : session.inTransaction())) {\n        const readConcern = (_a = read_concern_1.ReadConcern.fromOptions(options)) !== null && _a !== void 0 ? _a : parent === null || parent === void 0 ? void 0 : parent.readConcern;\n        if (readConcern) {\n            result.readConcern = readConcern;\n        }\n        const writeConcern = (_b = write_concern_1.WriteConcern.fromOptions(options)) !== null && _b !== void 0 ? _b : parent === null || parent === void 0 ? void 0 : parent.writeConcern;\n        if (writeConcern) {\n            result.writeConcern = writeConcern;\n        }\n    }\n    const readPreference = (_c = read_preference_1.ReadPreference.fromOptions(options)) !== null && _c !== void 0 ? _c : parent === null || parent === void 0 ? void 0 : parent.readPreference;\n    if (readPreference) {\n        result.readPreference = readPreference;\n    }\n    return result;\n}\nexports.resolveOptions = resolveOptions;\nfunction isSuperset(set, subset) {\n    set = Array.isArray(set) ? new Set(set) : set;\n    subset = Array.isArray(subset) ? new Set(subset) : subset;\n    for (const elem of subset) {\n        if (!set.has(elem)) {\n            return false;\n        }\n    }\n    return true;\n}\nexports.isSuperset = isSuperset;\n/** Returns the items that are uniquely in setA */\nfunction setDifference(setA, setB) {\n    const difference = new Set(setA);\n    for (const elem of setB) {\n        difference.delete(elem);\n    }\n    return difference;\n}\nexports.setDifference = setDifference;\nfunction isRecord(value, requiredKeys = undefined) {\n    const toString = Object.prototype.toString;\n    const hasOwnProperty = Object.prototype.hasOwnProperty;\n    const isObject = (v) => toString.call(v) === '[object Object]';\n    if (!isObject(value)) {\n        return false;\n    }\n    const ctor = value.constructor;\n    if (ctor && ctor.prototype) {\n        if (!isObject(ctor.prototype)) {\n            return false;\n        }\n        // Check to see if some method exists from the Object exists\n        if (!hasOwnProperty.call(ctor.prototype, 'isPrototypeOf')) {\n            return false;\n        }\n    }\n    if (requiredKeys) {\n        const keys = Object.keys(value);\n        return isSuperset(keys, requiredKeys);\n    }\n    return true;\n}\nexports.isRecord = isRecord;\n/**\n * Make a deep copy of an object\n *\n * NOTE: This is not meant to be the perfect implementation of a deep copy,\n * but instead something that is good enough for the purposes of\n * command monitoring.\n */\nfunction deepCopy(value) {\n    if (value == null) {\n        return value;\n    }\n    else if (Array.isArray(value)) {\n        return value.map(item => deepCopy(item));\n    }\n    else if (isRecord(value)) {\n        const res = {};\n        for (const key in value) {\n            res[key] = deepCopy(value[key]);\n        }\n        return res;\n    }\n    const ctor = value.constructor;\n    if (ctor) {\n        switch (ctor.name.toLowerCase()) {\n            case 'date':\n                return new ctor(Number(value));\n            case 'map':\n                return new Map(value);\n            case 'set':\n                return new Set(value);\n            case 'buffer':\n                return Buffer.from(value);\n        }\n    }\n    return value;\n}\nexports.deepCopy = deepCopy;\n/** @internal */\nconst kBuffers = Symbol('buffers');\n/** @internal */\nconst kLength = Symbol('length');\n/**\n * A pool of Buffers which allow you to read them as if they were one\n * @internal\n */\nclass BufferPool {\n    constructor() {\n        this[kBuffers] = [];\n        this[kLength] = 0;\n    }\n    get length() {\n        return this[kLength];\n    }\n    /** Adds a buffer to the internal buffer pool list */\n    append(buffer) {\n        this[kBuffers].push(buffer);\n        this[kLength] += buffer.length;\n    }\n    /** Returns the requested number of bytes without consuming them */\n    peek(size) {\n        return this.read(size, false);\n    }\n    /** Reads the requested number of bytes, optionally consuming them */\n    read(size, consume = true) {\n        if (typeof size !== 'number' || size < 0) {\n            throw new error_1.MongoInvalidArgumentError('Argument \"size\" must be a non-negative number');\n        }\n        if (size > this[kLength]) {\n            return Buffer.alloc(0);\n        }\n        let result;\n        // read the whole buffer\n        if (size === this.length) {\n            result = Buffer.concat(this[kBuffers]);\n            if (consume) {\n                this[kBuffers] = [];\n                this[kLength] = 0;\n            }\n        }\n        // size is within first buffer, no need to concat\n        else if (size <= this[kBuffers][0].length) {\n            result = this[kBuffers][0].slice(0, size);\n            if (consume) {\n                this[kBuffers][0] = this[kBuffers][0].slice(size);\n                this[kLength] -= size;\n            }\n        }\n        // size is beyond first buffer, need to track and copy\n        else {\n            result = Buffer.allocUnsafe(size);\n            let idx;\n            let offset = 0;\n            let bytesToCopy = size;\n            for (idx = 0; idx < this[kBuffers].length; ++idx) {\n                let bytesCopied;\n                if (bytesToCopy > this[kBuffers][idx].length) {\n                    bytesCopied = this[kBuffers][idx].copy(result, offset, 0);\n                    offset += bytesCopied;\n                }\n                else {\n                    bytesCopied = this[kBuffers][idx].copy(result, offset, 0, bytesToCopy);\n                    if (consume) {\n                        this[kBuffers][idx] = this[kBuffers][idx].slice(bytesCopied);\n                    }\n                    offset += bytesCopied;\n                    break;\n                }\n                bytesToCopy -= bytesCopied;\n            }\n            // compact the internal buffer array\n            if (consume) {\n                this[kBuffers] = this[kBuffers].slice(idx);\n                this[kLength] -= size;\n            }\n        }\n        return result;\n    }\n}\nexports.BufferPool = BufferPool;\n/** @public */\nclass HostAddress {\n    constructor(hostString) {\n        const escapedHost = hostString.split(' ').join('%20'); // escape spaces, for socket path hosts\n        const { hostname, port } = new url_1.URL(`mongodb://${escapedHost}`);\n        if (hostname.endsWith('.sock')) {\n            // heuristically determine if we're working with a domain socket\n            this.socketPath = decodeURIComponent(hostname);\n        }\n        else if (typeof hostname === 'string') {\n            this.isIPv6 = false;\n            let normalized = decodeURIComponent(hostname).toLowerCase();\n            if (normalized.startsWith('[') && normalized.endsWith(']')) {\n                this.isIPv6 = true;\n                normalized = normalized.substring(1, hostname.length - 1);\n            }\n            this.host = normalized.toLowerCase();\n            if (typeof port === 'number') {\n                this.port = port;\n            }\n            else if (typeof port === 'string' && port !== '') {\n                this.port = Number.parseInt(port, 10);\n            }\n            else {\n                this.port = 27017;\n            }\n            if (this.port === 0) {\n                throw new error_1.MongoParseError('Invalid port (zero) with hostname');\n            }\n        }\n        else {\n            throw new error_1.MongoInvalidArgumentError('Either socketPath or host must be defined.');\n        }\n        Object.freeze(this);\n    }\n    [Symbol.for('nodejs.util.inspect.custom')]() {\n        return this.inspect();\n    }\n    inspect() {\n        return `new HostAddress('${this.toString(true)}')`;\n    }\n    /**\n     * @param ipv6Brackets - optionally request ipv6 bracket notation required for connection strings\n     */\n    toString(ipv6Brackets = false) {\n        if (typeof this.host === 'string') {\n            if (this.isIPv6 && ipv6Brackets) {\n                return `[${this.host}]:${this.port}`;\n            }\n            return `${this.host}:${this.port}`;\n        }\n        return `${this.socketPath}`;\n    }\n    static fromString(s) {\n        return new HostAddress(s);\n    }\n    static fromSrvRecord({ name, port }) {\n        return HostAddress.fromString(`${name}:${port}`);\n    }\n}\nexports.HostAddress = HostAddress;\nexports.DEFAULT_PK_FACTORY = {\n    // We prefer not to rely on ObjectId having a createPk method\n    createPk() {\n        return new bson_1.ObjectId();\n    }\n};\n/**\n * When the driver used emitWarning the code will be equal to this.\n * @public\n *\n * @example\n * ```js\n * process.on('warning', (warning) => {\n *  if (warning.code === MONGODB_WARNING_CODE) console.error('Ah an important warning! :)')\n * })\n * ```\n */\nexports.MONGODB_WARNING_CODE = 'MONGODB DRIVER';\n/** @internal */\nfunction emitWarning(message) {\n    return process.emitWarning(message, { code: exports.MONGODB_WARNING_CODE });\n}\nexports.emitWarning = emitWarning;\nconst emittedWarnings = new Set();\n/**\n * Will emit a warning once for the duration of the application.\n * Uses the message to identify if it has already been emitted\n * so using string interpolation can cause multiple emits\n * @internal\n */\nfunction emitWarningOnce(message) {\n    if (!emittedWarnings.has(message)) {\n        emittedWarnings.add(message);\n        return emitWarning(message);\n    }\n}\nexports.emitWarningOnce = emitWarningOnce;\n/**\n * Takes a JS object and joins the values into a string separated by ', '\n */\nfunction enumToString(en) {\n    return Object.values(en).join(', ');\n}\nexports.enumToString = enumToString;\n/**\n * Determine if a server supports retryable writes.\n *\n * @internal\n */\nfunction supportsRetryableWrites(server) {\n    return (!!server.loadBalanced ||\n        (server.description.maxWireVersion >= 6 &&\n            !!server.description.logicalSessionTimeoutMinutes &&\n            server.description.type !== common_1.ServerType.Standalone));\n}\nexports.supportsRetryableWrites = supportsRetryableWrites;\nfunction parsePackageVersion({ version }) {\n    const [major, minor, patch] = version.split('.').map((n) => Number.parseInt(n, 10));\n    return { major, minor, patch };\n}\nexports.parsePackageVersion = parsePackageVersion;\n/**\n * FisherYates Shuffle\n *\n * Reference: https://bost.ocks.org/mike/shuffle/\n * @param sequence - items to be shuffled\n * @param limit - Defaults to `0`. If nonzero shuffle will slice the randomized array e.g, `.slice(0, limit)` otherwise will return the entire randomized array.\n */\nfunction shuffle(sequence, limit = 0) {\n    const items = Array.from(sequence); // shallow copy in order to never shuffle the input\n    if (limit > items.length) {\n        throw new error_1.MongoRuntimeError('Limit must be less than the number of items');\n    }\n    let remainingItemsToShuffle = items.length;\n    const lowerBound = limit % items.length === 0 ? 1 : items.length - limit;\n    while (remainingItemsToShuffle > lowerBound) {\n        // Pick a remaining element\n        const randomIndex = Math.floor(Math.random() * remainingItemsToShuffle);\n        remainingItemsToShuffle -= 1;\n        // And swap it with the current element\n        const swapHold = items[remainingItemsToShuffle];\n        items[remainingItemsToShuffle] = items[randomIndex];\n        items[randomIndex] = swapHold;\n    }\n    return limit % items.length === 0 ? items : items.slice(lowerBound);\n}\nexports.shuffle = shuffle;\n//# sourceMappingURL=utils.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.PromiseProvider = void 0;\nconst error_1 = require(\"./error\");\n/** @internal */\nconst kPromise = Symbol('promise');\nconst store = {\n    [kPromise]: undefined\n};\n/**\n * Global promise store allowing user-provided promises\n * @public\n */\nclass PromiseProvider {\n    /** Validates the passed in promise library */\n    static validate(lib) {\n        if (typeof lib !== 'function')\n            throw new error_1.MongoInvalidArgumentError(`Promise must be a function, got ${lib}`);\n        return !!lib;\n    }\n    /** Sets the promise library */\n    static set(lib) {\n        if (!PromiseProvider.validate(lib)) {\n            // validate\n            return;\n        }\n        store[kPromise] = lib;\n    }\n    /** Get the stored promise library, or resolves passed in */\n    static get() {\n        return store[kPromise];\n    }\n}\nexports.PromiseProvider = PromiseProvider;\nPromiseProvider.set(global.Promise);\n//# sourceMappingURL=promise_provider.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.isResumableError = exports.isNetworkTimeoutError = exports.isSDAMUnrecoverableError = exports.isNodeShuttingDownError = exports.isRetryableError = exports.isRetryableWriteError = exports.isRetryableEndTransactionError = exports.MongoWriteConcernError = exports.MongoServerSelectionError = exports.MongoSystemError = exports.MongoMissingDependencyError = exports.MongoMissingCredentialsError = exports.MongoCompatibilityError = exports.MongoInvalidArgumentError = exports.MongoParseError = exports.MongoNetworkTimeoutError = exports.MongoNetworkError = exports.isNetworkErrorBeforeHandshake = exports.MongoTopologyClosedError = exports.MongoCursorExhaustedError = exports.MongoServerClosedError = exports.MongoCursorInUseError = exports.MongoGridFSChunkError = exports.MongoGridFSStreamError = exports.MongoTailableCursorError = exports.MongoChangeStreamError = exports.MongoKerberosError = exports.MongoExpiredSessionError = exports.MongoTransactionError = exports.MongoNotConnectedError = exports.MongoDecompressionError = exports.MongoBatchReExecutionError = exports.MongoRuntimeError = exports.MongoAPIError = exports.MongoDriverError = exports.MongoServerError = exports.MongoError = exports.GET_MORE_RESUMABLE_CODES = exports.MONGODB_ERROR_CODES = exports.NODE_IS_RECOVERING_ERROR_MESSAGE = exports.LEGACY_NOT_PRIMARY_OR_SECONDARY_ERROR_MESSAGE = exports.LEGACY_NOT_WRITABLE_PRIMARY_ERROR_MESSAGE = void 0;\n/** @internal */\nconst kErrorLabels = Symbol('errorLabels');\n/**\n * @internal\n * The legacy error message from the server that indicates the node is not a writable primary\n * https://github.com/mongodb/specifications/blob/b07c26dc40d04ac20349f989db531c9845fdd755/source/server-discovery-and-monitoring/server-discovery-and-monitoring.rst#not-writable-primary-and-node-is-recovering\n */\nexports.LEGACY_NOT_WRITABLE_PRIMARY_ERROR_MESSAGE = 'not master';\n/**\n * @internal\n * The legacy error message from the server that indicates the node is not a primary or secondary\n * https://github.com/mongodb/specifications/blob/b07c26dc40d04ac20349f989db531c9845fdd755/source/server-discovery-and-monitoring/server-discovery-and-monitoring.rst#not-writable-primary-and-node-is-recovering\n */\nexports.LEGACY_NOT_PRIMARY_OR_SECONDARY_ERROR_MESSAGE = 'not master or secondary';\n/**\n * @internal\n * The error message from the server that indicates the node is recovering\n * https://github.com/mongodb/specifications/blob/b07c26dc40d04ac20349f989db531c9845fdd755/source/server-discovery-and-monitoring/server-discovery-and-monitoring.rst#not-writable-primary-and-node-is-recovering\n */\nexports.NODE_IS_RECOVERING_ERROR_MESSAGE = 'node is recovering';\n/** @internal MongoDB Error Codes */\nexports.MONGODB_ERROR_CODES = Object.freeze({\n    HostUnreachable: 6,\n    HostNotFound: 7,\n    NetworkTimeout: 89,\n    ShutdownInProgress: 91,\n    PrimarySteppedDown: 189,\n    ExceededTimeLimit: 262,\n    SocketException: 9001,\n    NotWritablePrimary: 10107,\n    InterruptedAtShutdown: 11600,\n    InterruptedDueToReplStateChange: 11602,\n    NotPrimaryNoSecondaryOk: 13435,\n    NotPrimaryOrSecondary: 13436,\n    StaleShardVersion: 63,\n    StaleEpoch: 150,\n    StaleConfig: 13388,\n    RetryChangeStream: 234,\n    FailedToSatisfyReadPreference: 133,\n    CursorNotFound: 43,\n    LegacyNotPrimary: 10058,\n    WriteConcernFailed: 64,\n    NamespaceNotFound: 26,\n    IllegalOperation: 20,\n    MaxTimeMSExpired: 50,\n    UnknownReplWriteConcern: 79,\n    UnsatisfiableWriteConcern: 100\n});\n// From spec@https://github.com/mongodb/specifications/blob/f93d78191f3db2898a59013a7ed5650352ef6da8/source/change-streams/change-streams.rst#resumable-error\nexports.GET_MORE_RESUMABLE_CODES = new Set([\n    exports.MONGODB_ERROR_CODES.HostUnreachable,\n    exports.MONGODB_ERROR_CODES.HostNotFound,\n    exports.MONGODB_ERROR_CODES.NetworkTimeout,\n    exports.MONGODB_ERROR_CODES.ShutdownInProgress,\n    exports.MONGODB_ERROR_CODES.PrimarySteppedDown,\n    exports.MONGODB_ERROR_CODES.ExceededTimeLimit,\n    exports.MONGODB_ERROR_CODES.SocketException,\n    exports.MONGODB_ERROR_CODES.NotWritablePrimary,\n    exports.MONGODB_ERROR_CODES.InterruptedAtShutdown,\n    exports.MONGODB_ERROR_CODES.InterruptedDueToReplStateChange,\n    exports.MONGODB_ERROR_CODES.NotPrimaryNoSecondaryOk,\n    exports.MONGODB_ERROR_CODES.NotPrimaryOrSecondary,\n    exports.MONGODB_ERROR_CODES.StaleShardVersion,\n    exports.MONGODB_ERROR_CODES.StaleEpoch,\n    exports.MONGODB_ERROR_CODES.StaleConfig,\n    exports.MONGODB_ERROR_CODES.RetryChangeStream,\n    exports.MONGODB_ERROR_CODES.FailedToSatisfyReadPreference,\n    exports.MONGODB_ERROR_CODES.CursorNotFound\n]);\n/**\n * @public\n * @category Error\n *\n * @privateRemarks\n * CSFLE has a dependency on this error, it uses the constructor with a string argument\n */\nclass MongoError extends Error {\n    constructor(message) {\n        if (message instanceof Error) {\n            super(message.message);\n        }\n        else {\n            super(message);\n        }\n    }\n    get name() {\n        return 'MongoError';\n    }\n    /** Legacy name for server error responses */\n    get errmsg() {\n        return this.message;\n    }\n    /**\n     * Checks the error to see if it has an error label\n     *\n     * @param label - The error label to check for\n     * @returns returns true if the error has the provided error label\n     */\n    hasErrorLabel(label) {\n        if (this[kErrorLabels] == null) {\n            return false;\n        }\n        return this[kErrorLabels].has(label);\n    }\n    addErrorLabel(label) {\n        if (this[kErrorLabels] == null) {\n            this[kErrorLabels] = new Set();\n        }\n        this[kErrorLabels].add(label);\n    }\n    get errorLabels() {\n        return this[kErrorLabels] ? Array.from(this[kErrorLabels]) : [];\n    }\n}\nexports.MongoError = MongoError;\n/**\n * An error coming from the mongo server\n *\n * @public\n * @category Error\n */\nclass MongoServerError extends MongoError {\n    constructor(message) {\n        super(message.message || message.errmsg || message.$err || 'n/a');\n        if (message.errorLabels) {\n            this[kErrorLabels] = new Set(message.errorLabels);\n        }\n        for (const name in message) {\n            if (name !== 'errorLabels' && name !== 'errmsg' && name !== 'message')\n                this[name] = message[name];\n        }\n    }\n    get name() {\n        return 'MongoServerError';\n    }\n}\nexports.MongoServerError = MongoServerError;\n/**\n * An error generated by the driver\n *\n * @public\n * @category Error\n */\nclass MongoDriverError extends MongoError {\n    constructor(message) {\n        super(message);\n    }\n    get name() {\n        return 'MongoDriverError';\n    }\n}\nexports.MongoDriverError = MongoDriverError;\n/**\n * An error generated when the driver API is used incorrectly\n *\n * @privateRemarks\n * Should **never** be directly instantiated\n *\n * @public\n * @category Error\n */\nclass MongoAPIError extends MongoDriverError {\n    constructor(message) {\n        super(message);\n    }\n    get name() {\n        return 'MongoAPIError';\n    }\n}\nexports.MongoAPIError = MongoAPIError;\n/**\n * An error generated when the driver encounters unexpected input\n * or reaches an unexpected/invalid internal state\n *\n * @privateRemarks\n * Should **never** be directly instantiated.\n *\n * @public\n * @category Error\n */\nclass MongoRuntimeError extends MongoDriverError {\n    constructor(message) {\n        super(message);\n    }\n    get name() {\n        return 'MongoRuntimeError';\n    }\n}\nexports.MongoRuntimeError = MongoRuntimeError;\n/**\n * An error generated when a batch command is reexecuted after one of the commands in the batch\n * has failed\n *\n * @public\n * @category Error\n */\nclass MongoBatchReExecutionError extends MongoAPIError {\n    constructor(message = 'This batch has already been executed, create new batch to execute') {\n        super(message);\n    }\n    get name() {\n        return 'MongoBatchReExecutionError';\n    }\n}\nexports.MongoBatchReExecutionError = MongoBatchReExecutionError;\n/**\n * An error generated when the driver fails to decompress\n * data received from the server.\n *\n * @public\n * @category Error\n */\nclass MongoDecompressionError extends MongoRuntimeError {\n    constructor(message) {\n        super(message);\n    }\n    get name() {\n        return 'MongoDecompressionError';\n    }\n}\nexports.MongoDecompressionError = MongoDecompressionError;\n/**\n * An error thrown when the user attempts to operate on a database or collection through a MongoClient\n * that has not yet successfully called the \"connect\" method\n *\n * @public\n * @category Error\n */\nclass MongoNotConnectedError extends MongoAPIError {\n    constructor(message) {\n        super(message);\n    }\n    get name() {\n        return 'MongoNotConnectedError';\n    }\n}\nexports.MongoNotConnectedError = MongoNotConnectedError;\n/**\n * An error generated when the user makes a mistake in the usage of transactions.\n * (e.g. attempting to commit a transaction with a readPreference other than primary)\n *\n * @public\n * @category Error\n */\nclass MongoTransactionError extends MongoAPIError {\n    constructor(message) {\n        super(message);\n    }\n    get name() {\n        return 'MongoTransactionError';\n    }\n}\nexports.MongoTransactionError = MongoTransactionError;\n/**\n * An error generated when the user attempts to operate\n * on a session that has expired or has been closed.\n *\n * @public\n * @category Error\n */\nclass MongoExpiredSessionError extends MongoAPIError {\n    constructor(message = 'Cannot use a session that has ended') {\n        super(message);\n    }\n    get name() {\n        return 'MongoExpiredSessionError';\n    }\n}\nexports.MongoExpiredSessionError = MongoExpiredSessionError;\n/**\n * A error generated when the user attempts to authenticate\n * via Kerberos, but fails to connect to the Kerberos client.\n *\n * @public\n * @category Error\n */\nclass MongoKerberosError extends MongoRuntimeError {\n    constructor(message) {\n        super(message);\n    }\n    get name() {\n        return 'MongoKerberosError';\n    }\n}\nexports.MongoKerberosError = MongoKerberosError;\n/**\n * An error generated when a ChangeStream operation fails to execute.\n *\n * @public\n * @category Error\n */\nclass MongoChangeStreamError extends MongoRuntimeError {\n    constructor(message) {\n        super(message);\n    }\n    get name() {\n        return 'MongoChangeStreamError';\n    }\n}\nexports.MongoChangeStreamError = MongoChangeStreamError;\n/**\n * An error thrown when the user calls a function or method not supported on a tailable cursor\n *\n * @public\n * @category Error\n */\nclass MongoTailableCursorError extends MongoAPIError {\n    constructor(message = 'Tailable cursor does not support this operation') {\n        super(message);\n    }\n    get name() {\n        return 'MongoTailableCursorError';\n    }\n}\nexports.MongoTailableCursorError = MongoTailableCursorError;\n/** An error generated when a GridFSStream operation fails to execute.\n *\n * @public\n * @category Error\n */\nclass MongoGridFSStreamError extends MongoRuntimeError {\n    constructor(message) {\n        super(message);\n    }\n    get name() {\n        return 'MongoGridFSStreamError';\n    }\n}\nexports.MongoGridFSStreamError = MongoGridFSStreamError;\n/**\n * An error generated when a malformed or invalid chunk is\n * encountered when reading from a GridFSStream.\n *\n * @public\n * @category Error\n */\nclass MongoGridFSChunkError extends MongoRuntimeError {\n    constructor(message) {\n        super(message);\n    }\n    get name() {\n        return 'MongoGridFSChunkError';\n    }\n}\nexports.MongoGridFSChunkError = MongoGridFSChunkError;\n/**\n * An error thrown when the user attempts to add options to a cursor that has already been\n * initialized\n *\n * @public\n * @category Error\n */\nclass MongoCursorInUseError extends MongoAPIError {\n    constructor(message = 'Cursor is already initialized') {\n        super(message);\n    }\n    get name() {\n        return 'MongoCursorInUseError';\n    }\n}\nexports.MongoCursorInUseError = MongoCursorInUseError;\n/**\n * An error generated when an attempt is made to operate\n * on a closed/closing server.\n *\n * @public\n * @category Error\n */\nclass MongoServerClosedError extends MongoAPIError {\n    constructor(message = 'Server is closed') {\n        super(message);\n    }\n    get name() {\n        return 'MongoServerClosedError';\n    }\n}\nexports.MongoServerClosedError = MongoServerClosedError;\n/**\n * An error thrown when an attempt is made to read from a cursor that has been exhausted\n *\n * @public\n * @category Error\n */\nclass MongoCursorExhaustedError extends MongoAPIError {\n    constructor(message) {\n        super(message || 'Cursor is exhausted');\n    }\n    get name() {\n        return 'MongoCursorExhaustedError';\n    }\n}\nexports.MongoCursorExhaustedError = MongoCursorExhaustedError;\n/**\n * An error generated when an attempt is made to operate on a\n * dropped, or otherwise unavailable, database.\n *\n * @public\n * @category Error\n */\nclass MongoTopologyClosedError extends MongoAPIError {\n    constructor(message = 'Topology is closed') {\n        super(message);\n    }\n    get name() {\n        return 'MongoTopologyClosedError';\n    }\n}\nexports.MongoTopologyClosedError = MongoTopologyClosedError;\n/** @internal */\nconst kBeforeHandshake = Symbol('beforeHandshake');\nfunction isNetworkErrorBeforeHandshake(err) {\n    return err[kBeforeHandshake] === true;\n}\nexports.isNetworkErrorBeforeHandshake = isNetworkErrorBeforeHandshake;\n/**\n * An error indicating an issue with the network, including TCP errors and timeouts.\n * @public\n * @category Error\n */\nclass MongoNetworkError extends MongoError {\n    constructor(message, options) {\n        super(message);\n        if (options && typeof options.beforeHandshake === 'boolean') {\n            this[kBeforeHandshake] = options.beforeHandshake;\n        }\n    }\n    get name() {\n        return 'MongoNetworkError';\n    }\n}\nexports.MongoNetworkError = MongoNetworkError;\n/**\n * An error indicating a network timeout occurred\n * @public\n * @category Error\n *\n * @privateRemarks\n * CSFLE has a dependency on this error with an instanceof check\n */\nclass MongoNetworkTimeoutError extends MongoNetworkError {\n    constructor(message, options) {\n        super(message, options);\n    }\n    get name() {\n        return 'MongoNetworkTimeoutError';\n    }\n}\nexports.MongoNetworkTimeoutError = MongoNetworkTimeoutError;\n/**\n * An error used when attempting to parse a value (like a connection string)\n * @public\n * @category Error\n */\nclass MongoParseError extends MongoDriverError {\n    constructor(message) {\n        super(message);\n    }\n    get name() {\n        return 'MongoParseError';\n    }\n}\nexports.MongoParseError = MongoParseError;\n/**\n * An error generated when the user supplies malformed or unexpected arguments\n * or when a required argument or field is not provided.\n *\n *\n * @public\n * @category Error\n */\nclass MongoInvalidArgumentError extends MongoAPIError {\n    constructor(message) {\n        super(message);\n    }\n    get name() {\n        return 'MongoInvalidArgumentError';\n    }\n}\nexports.MongoInvalidArgumentError = MongoInvalidArgumentError;\n/**\n * An error generated when a feature that is not enabled or allowed for the current server\n * configuration is used\n *\n *\n * @public\n * @category Error\n */\nclass MongoCompatibilityError extends MongoAPIError {\n    constructor(message) {\n        super(message);\n    }\n    get name() {\n        return 'MongoCompatibilityError';\n    }\n}\nexports.MongoCompatibilityError = MongoCompatibilityError;\n/**\n * An error generated when the user fails to provide authentication credentials before attempting\n * to connect to a mongo server instance.\n *\n *\n * @public\n * @category Error\n */\nclass MongoMissingCredentialsError extends MongoAPIError {\n    constructor(message) {\n        super(message);\n    }\n    get name() {\n        return 'MongoMissingCredentialsError';\n    }\n}\nexports.MongoMissingCredentialsError = MongoMissingCredentialsError;\n/**\n * An error generated when a required module or dependency is not present in the local environment\n *\n * @public\n * @category Error\n */\nclass MongoMissingDependencyError extends MongoAPIError {\n    constructor(message) {\n        super(message);\n    }\n    get name() {\n        return 'MongoMissingDependencyError';\n    }\n}\nexports.MongoMissingDependencyError = MongoMissingDependencyError;\n/**\n * An error signifying a general system issue\n * @public\n * @category Error\n */\nclass MongoSystemError extends MongoError {\n    constructor(message, reason) {\n        if (reason && reason.error) {\n            super(reason.error.message || reason.error);\n        }\n        else {\n            super(message);\n        }\n        if (reason) {\n            this.reason = reason;\n        }\n    }\n    get name() {\n        return 'MongoSystemError';\n    }\n}\nexports.MongoSystemError = MongoSystemError;\n/**\n * An error signifying a client-side server selection error\n * @public\n * @category Error\n */\nclass MongoServerSelectionError extends MongoSystemError {\n    constructor(message, reason) {\n        super(message, reason);\n    }\n    get name() {\n        return 'MongoServerSelectionError';\n    }\n}\nexports.MongoServerSelectionError = MongoServerSelectionError;\nfunction makeWriteConcernResultObject(input) {\n    const output = Object.assign({}, input);\n    if (output.ok === 0) {\n        output.ok = 1;\n        delete output.errmsg;\n        delete output.code;\n        delete output.codeName;\n    }\n    return output;\n}\n/**\n * An error thrown when the server reports a writeConcernError\n * @public\n * @category Error\n */\nclass MongoWriteConcernError extends MongoServerError {\n    constructor(message, result) {\n        if (result && Array.isArray(result.errorLabels)) {\n            message.errorLabels = result.errorLabels;\n        }\n        super(message);\n        this.errInfo = message.errInfo;\n        if (result != null) {\n            this.result = makeWriteConcernResultObject(result);\n        }\n    }\n    get name() {\n        return 'MongoWriteConcernError';\n    }\n}\nexports.MongoWriteConcernError = MongoWriteConcernError;\n// see: https://github.com/mongodb/specifications/blob/master/source/retryable-writes/retryable-writes.rst#terms\nconst RETRYABLE_ERROR_CODES = new Set([\n    exports.MONGODB_ERROR_CODES.HostUnreachable,\n    exports.MONGODB_ERROR_CODES.HostNotFound,\n    exports.MONGODB_ERROR_CODES.NetworkTimeout,\n    exports.MONGODB_ERROR_CODES.ShutdownInProgress,\n    exports.MONGODB_ERROR_CODES.PrimarySteppedDown,\n    exports.MONGODB_ERROR_CODES.SocketException,\n    exports.MONGODB_ERROR_CODES.NotWritablePrimary,\n    exports.MONGODB_ERROR_CODES.InterruptedAtShutdown,\n    exports.MONGODB_ERROR_CODES.InterruptedDueToReplStateChange,\n    exports.MONGODB_ERROR_CODES.NotPrimaryNoSecondaryOk,\n    exports.MONGODB_ERROR_CODES.NotPrimaryOrSecondary\n]);\nconst RETRYABLE_WRITE_ERROR_CODES = new Set([\n    exports.MONGODB_ERROR_CODES.InterruptedAtShutdown,\n    exports.MONGODB_ERROR_CODES.InterruptedDueToReplStateChange,\n    exports.MONGODB_ERROR_CODES.NotWritablePrimary,\n    exports.MONGODB_ERROR_CODES.NotPrimaryNoSecondaryOk,\n    exports.MONGODB_ERROR_CODES.NotPrimaryOrSecondary,\n    exports.MONGODB_ERROR_CODES.PrimarySteppedDown,\n    exports.MONGODB_ERROR_CODES.ShutdownInProgress,\n    exports.MONGODB_ERROR_CODES.HostNotFound,\n    exports.MONGODB_ERROR_CODES.HostUnreachable,\n    exports.MONGODB_ERROR_CODES.NetworkTimeout,\n    exports.MONGODB_ERROR_CODES.SocketException,\n    exports.MONGODB_ERROR_CODES.ExceededTimeLimit\n]);\nfunction isRetryableEndTransactionError(error) {\n    return error.hasErrorLabel('RetryableWriteError');\n}\nexports.isRetryableEndTransactionError = isRetryableEndTransactionError;\nfunction isRetryableWriteError(error) {\n    var _a, _b, _c;\n    if (error instanceof MongoWriteConcernError) {\n        return RETRYABLE_WRITE_ERROR_CODES.has((_c = (_b = (_a = error.result) === null || _a === void 0 ? void 0 : _a.code) !== null && _b !== void 0 ? _b : error.code) !== null && _c !== void 0 ? _c : 0);\n    }\n    return typeof error.code === 'number' && RETRYABLE_WRITE_ERROR_CODES.has(error.code);\n}\nexports.isRetryableWriteError = isRetryableWriteError;\n/** Determines whether an error is something the driver should attempt to retry */\nfunction isRetryableError(error) {\n    return (\n    // eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n    (typeof error.code === 'number' && RETRYABLE_ERROR_CODES.has(error.code)) ||\n        error instanceof MongoNetworkError ||\n        !!error.message.match(new RegExp(exports.LEGACY_NOT_WRITABLE_PRIMARY_ERROR_MESSAGE)) ||\n        !!error.message.match(new RegExp(exports.NODE_IS_RECOVERING_ERROR_MESSAGE)));\n}\nexports.isRetryableError = isRetryableError;\nconst SDAM_RECOVERING_CODES = new Set([\n    exports.MONGODB_ERROR_CODES.ShutdownInProgress,\n    exports.MONGODB_ERROR_CODES.PrimarySteppedDown,\n    exports.MONGODB_ERROR_CODES.InterruptedAtShutdown,\n    exports.MONGODB_ERROR_CODES.InterruptedDueToReplStateChange,\n    exports.MONGODB_ERROR_CODES.NotPrimaryOrSecondary\n]);\nconst SDAM_NOTPRIMARY_CODES = new Set([\n    exports.MONGODB_ERROR_CODES.NotWritablePrimary,\n    exports.MONGODB_ERROR_CODES.NotPrimaryNoSecondaryOk,\n    exports.MONGODB_ERROR_CODES.LegacyNotPrimary\n]);\nconst SDAM_NODE_SHUTTING_DOWN_ERROR_CODES = new Set([\n    exports.MONGODB_ERROR_CODES.InterruptedAtShutdown,\n    exports.MONGODB_ERROR_CODES.ShutdownInProgress\n]);\nfunction isRecoveringError(err) {\n    if (typeof err.code === 'number') {\n        // If any error code exists, we ignore the error.message\n        return SDAM_RECOVERING_CODES.has(err.code);\n    }\n    return (new RegExp(exports.LEGACY_NOT_PRIMARY_OR_SECONDARY_ERROR_MESSAGE).test(err.message) ||\n        new RegExp(exports.NODE_IS_RECOVERING_ERROR_MESSAGE).test(err.message));\n}\nfunction isNotWritablePrimaryError(err) {\n    if (typeof err.code === 'number') {\n        // If any error code exists, we ignore the error.message\n        return SDAM_NOTPRIMARY_CODES.has(err.code);\n    }\n    if (isRecoveringError(err)) {\n        return false;\n    }\n    return new RegExp(exports.LEGACY_NOT_WRITABLE_PRIMARY_ERROR_MESSAGE).test(err.message);\n}\nfunction isNodeShuttingDownError(err) {\n    return !!(typeof err.code === 'number' && SDAM_NODE_SHUTTING_DOWN_ERROR_CODES.has(err.code));\n}\nexports.isNodeShuttingDownError = isNodeShuttingDownError;\n/**\n * Determines whether SDAM can recover from a given error. If it cannot\n * then the pool will be cleared, and server state will completely reset\n * locally.\n *\n * @see https://github.com/mongodb/specifications/blob/master/source/server-discovery-and-monitoring/server-discovery-and-monitoring.rst#not-master-and-node-is-recovering\n */\nfunction isSDAMUnrecoverableError(error) {\n    // NOTE: null check is here for a strictly pre-CMAP world, a timeout or\n    //       close event are considered unrecoverable\n    if (error instanceof MongoParseError || error == null) {\n        return true;\n    }\n    return isRecoveringError(error) || isNotWritablePrimaryError(error);\n}\nexports.isSDAMUnrecoverableError = isSDAMUnrecoverableError;\nfunction isNetworkTimeoutError(err) {\n    return !!(err instanceof MongoNetworkError && err.message.match(/timed out/));\n}\nexports.isNetworkTimeoutError = isNetworkTimeoutError;\n// From spec@https://github.com/mongodb/specifications/blob/7a2e93d85935ee4b1046a8d2ad3514c657dc74fa/source/change-streams/change-streams.rst#resumable-error:\n//\n// An error is considered resumable if it meets any of the following criteria:\n// - any error encountered which is not a server error (e.g. a timeout error or network error)\n// - any server error response from a getMore command excluding those containing the error label\n//   NonRetryableChangeStreamError and those containing the following error codes:\n//   - Interrupted: 11601\n//   - CappedPositionLost: 136\n//   - CursorKilled: 237\n//\n// An error on an aggregate command is not a resumable error. Only errors on a getMore command may be considered resumable errors.\nfunction isResumableError(error, wireVersion) {\n    if (error instanceof MongoNetworkError) {\n        return true;\n    }\n    if (wireVersion != null && wireVersion >= 9) {\n        // DRIVERS-1308: For 4.4 drivers running against 4.4 servers, drivers will add a special case to treat the CursorNotFound error code as resumable\n        if (error && error instanceof MongoError && error.code === 43) {\n            return true;\n        }\n        return error instanceof MongoError && error.hasErrorLabel('ResumableChangeStreamError');\n    }\n    if (error && typeof error.code === 'number') {\n        return exports.GET_MORE_RESUMABLE_CODES.has(error.code);\n    }\n    return false;\n}\nexports.isResumableError = isResumableError;\n//# sourceMappingURL=error.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.WriteConcern = exports.WRITE_CONCERN_KEYS = void 0;\nexports.WRITE_CONCERN_KEYS = ['w', 'wtimeout', 'j', 'journal', 'fsync'];\n/**\n * A MongoDB WriteConcern, which describes the level of acknowledgement\n * requested from MongoDB for write operations.\n * @public\n *\n * @see https://docs.mongodb.com/manual/reference/write-concern/\n */\nclass WriteConcern {\n    /**\n     * Constructs a WriteConcern from the write concern properties.\n     * @param w - request acknowledgment that the write operation has propagated to a specified number of mongod instances or to mongod instances with specified tags.\n     * @param wtimeout - specify a time limit to prevent write operations from blocking indefinitely\n     * @param j - request acknowledgment that the write operation has been written to the on-disk journal\n     * @param fsync - equivalent to the j option\n     */\n    constructor(w, wtimeout, j, fsync) {\n        if (w != null) {\n            if (!Number.isNaN(Number(w))) {\n                this.w = Number(w);\n            }\n            else {\n                this.w = w;\n            }\n        }\n        if (wtimeout != null) {\n            this.wtimeout = wtimeout;\n        }\n        if (j != null) {\n            this.j = j;\n        }\n        if (fsync != null) {\n            this.fsync = fsync;\n        }\n    }\n    /** Construct a WriteConcern given an options object. */\n    static fromOptions(options, inherit) {\n        if (options == null)\n            return undefined;\n        inherit = inherit !== null && inherit !== void 0 ? inherit : {};\n        let opts;\n        if (typeof options === 'string' || typeof options === 'number') {\n            opts = { w: options };\n        }\n        else if (options instanceof WriteConcern) {\n            opts = options;\n        }\n        else {\n            opts = options.writeConcern;\n        }\n        const parentOpts = inherit instanceof WriteConcern ? inherit : inherit.writeConcern;\n        const { w = undefined, wtimeout = undefined, j = undefined, fsync = undefined, journal = undefined, wtimeoutMS = undefined } = {\n            ...parentOpts,\n            ...opts\n        };\n        if (w != null ||\n            wtimeout != null ||\n            wtimeoutMS != null ||\n            j != null ||\n            journal != null ||\n            fsync != null) {\n            return new WriteConcern(w, wtimeout !== null && wtimeout !== void 0 ? wtimeout : wtimeoutMS, j !== null && j !== void 0 ? j : journal, fsync);\n        }\n        return undefined;\n    }\n}\nexports.WriteConcern = WriteConcern;\n//# sourceMappingURL=write_concern.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports._advanceClusterTime = exports.clearAndRemoveTimerFrom = exports.drainTimerQueue = exports.ServerType = exports.TopologyType = exports.STATE_CONNECTED = exports.STATE_CONNECTING = exports.STATE_CLOSED = exports.STATE_CLOSING = void 0;\n// shared state names\nexports.STATE_CLOSING = 'closing';\nexports.STATE_CLOSED = 'closed';\nexports.STATE_CONNECTING = 'connecting';\nexports.STATE_CONNECTED = 'connected';\n/**\n * An enumeration of topology types we know about\n * @public\n */\nexports.TopologyType = Object.freeze({\n    Single: 'Single',\n    ReplicaSetNoPrimary: 'ReplicaSetNoPrimary',\n    ReplicaSetWithPrimary: 'ReplicaSetWithPrimary',\n    Sharded: 'Sharded',\n    Unknown: 'Unknown',\n    LoadBalanced: 'LoadBalanced'\n});\n/**\n * An enumeration of server types we know about\n * @public\n */\nexports.ServerType = Object.freeze({\n    Standalone: 'Standalone',\n    Mongos: 'Mongos',\n    PossiblePrimary: 'PossiblePrimary',\n    RSPrimary: 'RSPrimary',\n    RSSecondary: 'RSSecondary',\n    RSArbiter: 'RSArbiter',\n    RSOther: 'RSOther',\n    RSGhost: 'RSGhost',\n    Unknown: 'Unknown',\n    LoadBalancer: 'LoadBalancer'\n});\n/** @internal */\nfunction drainTimerQueue(queue) {\n    queue.forEach(clearTimeout);\n    queue.clear();\n}\nexports.drainTimerQueue = drainTimerQueue;\n/** @internal */\nfunction clearAndRemoveTimerFrom(timer, timers) {\n    clearTimeout(timer);\n    return timers.delete(timer);\n}\nexports.clearAndRemoveTimerFrom = clearAndRemoveTimerFrom;\n/** Shared function to determine clusterTime for a given topology or session */\nfunction _advanceClusterTime(entity, $clusterTime) {\n    if (entity.clusterTime == null) {\n        entity.clusterTime = $clusterTime;\n    }\n    else {\n        if ($clusterTime.clusterTime.greaterThan(entity.clusterTime.clusterTime)) {\n            entity.clusterTime = $clusterTime;\n        }\n    }\n}\nexports._advanceClusterTime = _advanceClusterTime;\n//# sourceMappingURL=common.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.ReadConcern = exports.ReadConcernLevel = void 0;\n/** @public */\nexports.ReadConcernLevel = Object.freeze({\n    local: 'local',\n    majority: 'majority',\n    linearizable: 'linearizable',\n    available: 'available',\n    snapshot: 'snapshot'\n});\n/**\n * The MongoDB ReadConcern, which allows for control of the consistency and isolation properties\n * of the data read from replica sets and replica set shards.\n * @public\n *\n * @see https://docs.mongodb.com/manual/reference/read-concern/index.html\n */\nclass ReadConcern {\n    /** Constructs a ReadConcern from the read concern level.*/\n    constructor(level) {\n        var _a;\n        /**\n         * A spec test exists that allows level to be any string.\n         * \"invalid readConcern with out stage\"\n         * @see ./test/spec/crud/v2/aggregate-out-readConcern.json\n         * @see https://github.com/mongodb/specifications/blob/master/source/read-write-concern/read-write-concern.rst#unknown-levels-and-additional-options-for-string-based-readconcerns\n         */\n        this.level = (_a = exports.ReadConcernLevel[level]) !== null && _a !== void 0 ? _a : level;\n    }\n    /**\n     * Construct a ReadConcern given an options object.\n     *\n     * @param options - The options object from which to extract the write concern.\n     */\n    static fromOptions(options) {\n        if (options == null) {\n            return;\n        }\n        if (options.readConcern) {\n            const { readConcern } = options;\n            if (readConcern instanceof ReadConcern) {\n                return readConcern;\n            }\n            else if (typeof readConcern === 'string') {\n                return new ReadConcern(readConcern);\n            }\n            else if ('level' in readConcern && readConcern.level) {\n                return new ReadConcern(readConcern.level);\n            }\n        }\n        if (options.level) {\n            return new ReadConcern(options.level);\n        }\n    }\n    static get MAJORITY() {\n        return exports.ReadConcernLevel.majority;\n    }\n    static get AVAILABLE() {\n        return exports.ReadConcernLevel.available;\n    }\n    static get LINEARIZABLE() {\n        return exports.ReadConcernLevel.linearizable;\n    }\n    static get SNAPSHOT() {\n        return exports.ReadConcernLevel.snapshot;\n    }\n    toJSON() {\n        return { level: this.level };\n    }\n}\nexports.ReadConcern = ReadConcern;\n//# sourceMappingURL=read_concern.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.ReadPreference = exports.ReadPreferenceMode = void 0;\nconst error_1 = require(\"./error\");\n/** @public */\nexports.ReadPreferenceMode = Object.freeze({\n    primary: 'primary',\n    primaryPreferred: 'primaryPreferred',\n    secondary: 'secondary',\n    secondaryPreferred: 'secondaryPreferred',\n    nearest: 'nearest'\n});\n/**\n * The **ReadPreference** class is a class that represents a MongoDB ReadPreference and is\n * used to construct connections.\n * @public\n *\n * @see https://docs.mongodb.com/manual/core/read-preference/\n */\nclass ReadPreference {\n    /**\n     * @param mode - A string describing the read preference mode (primary|primaryPreferred|secondary|secondaryPreferred|nearest)\n     * @param tags - A tag set used to target reads to members with the specified tag(s). tagSet is not available if using read preference mode primary.\n     * @param options - Additional read preference options\n     */\n    constructor(mode, tags, options) {\n        if (!ReadPreference.isValid(mode)) {\n            throw new error_1.MongoInvalidArgumentError(`Invalid read preference mode ${JSON.stringify(mode)}`);\n        }\n        if (options == null && typeof tags === 'object' && !Array.isArray(tags)) {\n            options = tags;\n            tags = undefined;\n        }\n        else if (tags && !Array.isArray(tags)) {\n            throw new error_1.MongoInvalidArgumentError('ReadPreference tags must be an array');\n        }\n        this.mode = mode;\n        this.tags = tags;\n        this.hedge = options === null || options === void 0 ? void 0 : options.hedge;\n        this.maxStalenessSeconds = undefined;\n        this.minWireVersion = undefined;\n        options = options !== null && options !== void 0 ? options : {};\n        if (options.maxStalenessSeconds != null) {\n            if (options.maxStalenessSeconds <= 0) {\n                throw new error_1.MongoInvalidArgumentError('maxStalenessSeconds must be a positive integer');\n            }\n            this.maxStalenessSeconds = options.maxStalenessSeconds;\n            // NOTE: The minimum required wire version is 5 for this read preference. If the existing\n            //       topology has a lower value then a MongoError will be thrown during server selection.\n            this.minWireVersion = 5;\n        }\n        if (this.mode === ReadPreference.PRIMARY) {\n            if (this.tags && Array.isArray(this.tags) && this.tags.length > 0) {\n                throw new error_1.MongoInvalidArgumentError('Primary read preference cannot be combined with tags');\n            }\n            if (this.maxStalenessSeconds) {\n                throw new error_1.MongoInvalidArgumentError('Primary read preference cannot be combined with maxStalenessSeconds');\n            }\n            if (this.hedge) {\n                throw new error_1.MongoInvalidArgumentError('Primary read preference cannot be combined with hedge');\n            }\n        }\n    }\n    // Support the deprecated `preference` property introduced in the porcelain layer\n    get preference() {\n        return this.mode;\n    }\n    static fromString(mode) {\n        return new ReadPreference(mode);\n    }\n    /**\n     * Construct a ReadPreference given an options object.\n     *\n     * @param options - The options object from which to extract the read preference.\n     */\n    static fromOptions(options) {\n        var _a, _b, _c;\n        if (!options)\n            return;\n        const readPreference = (_a = options.readPreference) !== null && _a !== void 0 ? _a : (_b = options.session) === null || _b === void 0 ? void 0 : _b.transaction.options.readPreference;\n        const readPreferenceTags = options.readPreferenceTags;\n        if (readPreference == null) {\n            return;\n        }\n        if (typeof readPreference === 'string') {\n            return new ReadPreference(readPreference, readPreferenceTags, {\n                maxStalenessSeconds: options.maxStalenessSeconds,\n                hedge: options.hedge\n            });\n        }\n        else if (!(readPreference instanceof ReadPreference) && typeof readPreference === 'object') {\n            const mode = readPreference.mode || readPreference.preference;\n            if (mode && typeof mode === 'string') {\n                return new ReadPreference(mode, (_c = readPreference.tags) !== null && _c !== void 0 ? _c : readPreferenceTags, {\n                    maxStalenessSeconds: readPreference.maxStalenessSeconds,\n                    hedge: options.hedge\n                });\n            }\n        }\n        if (readPreferenceTags) {\n            readPreference.tags = readPreferenceTags;\n        }\n        return readPreference;\n    }\n    /**\n     * Replaces options.readPreference with a ReadPreference instance\n     */\n    static translate(options) {\n        if (options.readPreference == null)\n            return options;\n        const r = options.readPreference;\n        if (typeof r === 'string') {\n            options.readPreference = new ReadPreference(r);\n        }\n        else if (r && !(r instanceof ReadPreference) && typeof r === 'object') {\n            const mode = r.mode || r.preference;\n            if (mode && typeof mode === 'string') {\n                options.readPreference = new ReadPreference(mode, r.tags, {\n                    maxStalenessSeconds: r.maxStalenessSeconds\n                });\n            }\n        }\n        else if (!(r instanceof ReadPreference)) {\n            throw new error_1.MongoInvalidArgumentError(`Invalid read preference: ${r}`);\n        }\n        return options;\n    }\n    /**\n     * Validate if a mode is legal\n     *\n     * @param mode - The string representing the read preference mode.\n     */\n    static isValid(mode) {\n        const VALID_MODES = new Set([\n            ReadPreference.PRIMARY,\n            ReadPreference.PRIMARY_PREFERRED,\n            ReadPreference.SECONDARY,\n            ReadPreference.SECONDARY_PREFERRED,\n            ReadPreference.NEAREST,\n            null\n        ]);\n        return VALID_MODES.has(mode);\n    }\n    /**\n     * Validate if a mode is legal\n     *\n     * @param mode - The string representing the read preference mode.\n     */\n    isValid(mode) {\n        return ReadPreference.isValid(typeof mode === 'string' ? mode : this.mode);\n    }\n    /**\n     * Indicates that this readPreference needs the \"slaveOk\" bit when sent over the wire\n     *\n     * @see https://docs.mongodb.com/manual/reference/mongodb-wire-protocol/#op-query\n     */\n    slaveOk() {\n        const NEEDS_SLAVEOK = new Set([\n            ReadPreference.PRIMARY_PREFERRED,\n            ReadPreference.SECONDARY,\n            ReadPreference.SECONDARY_PREFERRED,\n            ReadPreference.NEAREST\n        ]);\n        return NEEDS_SLAVEOK.has(this.mode);\n    }\n    /**\n     * Check if the two ReadPreferences are equivalent\n     *\n     * @param readPreference - The read preference with which to check equality\n     */\n    equals(readPreference) {\n        return readPreference.mode === this.mode;\n    }\n    /** Return JSON representation */\n    toJSON() {\n        const readPreference = { mode: this.mode };\n        if (Array.isArray(this.tags))\n            readPreference.tags = this.tags;\n        if (this.maxStalenessSeconds)\n            readPreference.maxStalenessSeconds = this.maxStalenessSeconds;\n        if (this.hedge)\n            readPreference.hedge = this.hedge;\n        return readPreference;\n    }\n}\nexports.ReadPreference = ReadPreference;\nReadPreference.PRIMARY = exports.ReadPreferenceMode.primary;\nReadPreference.PRIMARY_PREFERRED = exports.ReadPreferenceMode.primaryPreferred;\nReadPreference.SECONDARY = exports.ReadPreferenceMode.secondary;\nReadPreference.SECONDARY_PREFERRED = exports.ReadPreferenceMode.secondaryPreferred;\nReadPreference.NEAREST = exports.ReadPreferenceMode.nearest;\nReadPreference.primary = new ReadPreference(exports.ReadPreferenceMode.primary);\nReadPreference.primaryPreferred = new ReadPreference(exports.ReadPreferenceMode.primaryPreferred);\nReadPreference.secondary = new ReadPreference(exports.ReadPreferenceMode.secondary);\nReadPreference.secondaryPreferred = new ReadPreference(exports.ReadPreferenceMode.secondaryPreferred);\nReadPreference.nearest = new ReadPreference(exports.ReadPreferenceMode.nearest);\n//# sourceMappingURL=read_preference.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.OP_MSG = exports.OP_COMPRESSED = exports.OP_KILL_CURSORS = exports.OP_DELETE = exports.OP_GETMORE = exports.OP_QUERY = exports.OP_INSERT = exports.OP_UPDATE = exports.OP_REPLY = exports.MAX_SUPPORTED_WIRE_VERSION = exports.MIN_SUPPORTED_WIRE_VERSION = exports.MAX_SUPPORTED_SERVER_VERSION = exports.MIN_SUPPORTED_SERVER_VERSION = void 0;\nexports.MIN_SUPPORTED_SERVER_VERSION = '3.6';\nexports.MAX_SUPPORTED_SERVER_VERSION = '5.1';\nexports.MIN_SUPPORTED_WIRE_VERSION = 6;\nexports.MAX_SUPPORTED_WIRE_VERSION = 14;\nexports.OP_REPLY = 1;\nexports.OP_UPDATE = 2001;\nexports.OP_INSERT = 2002;\nexports.OP_QUERY = 2004;\nexports.OP_GETMORE = 2005;\nexports.OP_DELETE = 2006;\nexports.OP_KILL_CURSORS = 2007;\nexports.OP_COMPRESSED = 2012;\nexports.OP_MSG = 2013;\n//# sourceMappingURL=constants.js.map","module.exports = {\n  \"_from\": \"mongodb@4.2.2\",\n  \"_id\": \"mongodb@4.2.2\",\n  \"_inBundle\": false,\n  \"_integrity\": \"sha512-zt8rCTnTKyMQppyt63qMnrLM5dbADgUk18ORPF1XbtHLIYCyc9hattaYHi0pqMvNxDpgGgUofSVzS+UQErgTug==\",\n  \"_location\": \"/mongodb\",\n  \"_phantomChildren\": {},\n  \"_requested\": {\n    \"type\": \"version\",\n    \"registry\": true,\n    \"raw\": \"mongodb@4.2.2\",\n    \"name\": \"mongodb\",\n    \"escapedName\": \"mongodb\",\n    \"rawSpec\": \"4.2.2\",\n    \"saveSpec\": null,\n    \"fetchSpec\": \"4.2.2\"\n  },\n  \"_requiredBy\": [\n    \"/mongoose\"\n  ],\n  \"_resolved\": \"https://registry.npmjs.org/mongodb/-/mongodb-4.2.2.tgz\",\n  \"_shasum\": \"cd70568bd96003877e35358ad17a0c5de35c6dfd\",\n  \"_spec\": \"mongodb@4.2.2\",\n  \"_where\": \"E:\\\\\\\\miniTopic\\\\server\\\\node_modules\\\\mongoose\",\n  \"author\": {\n    \"name\": \"The MongoDB NodeJS Team\",\n    \"email\": \"dbx-node@mongodb.com\"\n  },\n  \"bugs\": {\n    \"url\": \"https://jira.mongodb.org/projects/NODE/issues/\"\n  },\n  \"bundleDependencies\": false,\n  \"dependencies\": {\n    \"bson\": \"^4.6.0\",\n    \"denque\": \"^2.0.1\",\n    \"mongodb-connection-string-url\": \"^2.3.2\",\n    \"saslprep\": \"^1.0.3\"\n  },\n  \"deprecated\": false,\n  \"description\": \"The official MongoDB driver for Node.js\",\n  \"devDependencies\": {\n    \"@istanbuljs/nyc-config-typescript\": \"^1.0.2\",\n    \"@microsoft/api-extractor\": \"^7.18.21\",\n    \"@microsoft/tsdoc-config\": \"^0.15.2\",\n    \"@types/chai\": \"^4.3.0\",\n    \"@types/chai-subset\": \"^1.3.3\",\n    \"@types/kerberos\": \"^1.1.1\",\n    \"@types/mocha\": \"^9.0.0\",\n    \"@types/node\": \"^16.11.12\",\n    \"@types/saslprep\": \"^1.0.1\",\n    \"@types/semver\": \"^7.3.9\",\n    \"@types/sinon\": \"^10.0.6\",\n    \"@types/whatwg-url\": \"^8.2.1\",\n    \"@typescript-eslint/eslint-plugin\": \"^5.6.0\",\n    \"@typescript-eslint/parser\": \"^5.6.0\",\n    \"bluebird\": \"^3.7.2\",\n    \"chai\": \"^4.3.4\",\n    \"chai-subset\": \"^1.6.0\",\n    \"chalk\": \"^4.1.2\",\n    \"downlevel-dts\": \"^0.7.0\",\n    \"eslint\": \"^8.4.1\",\n    \"eslint-config-prettier\": \"^8.3.0\",\n    \"eslint-plugin-prettier\": \"^4.0.0\",\n    \"eslint-plugin-tsdoc\": \"^0.2.14\",\n    \"js-yaml\": \"^4.1.0\",\n    \"lodash.camelcase\": \"^4.3.0\",\n    \"mocha\": \"^9.1.3\",\n    \"mocha-sinon\": \"^2.1.2\",\n    \"nyc\": \"^15.1.0\",\n    \"prettier\": \"^2.5.1\",\n    \"rimraf\": \"^3.0.2\",\n    \"semver\": \"^7.3.5\",\n    \"sinon\": \"^12.0.1\",\n    \"sinon-chai\": \"^3.7.0\",\n    \"source-map-support\": \"^0.5.21\",\n    \"standard-version\": \"^9.3.2\",\n    \"ts-node\": \"^10.4.0\",\n    \"tsd\": \"^0.19.0\",\n    \"typescript\": \"^4.5.2\",\n    \"typescript-cached-transpile\": \"^0.0.6\",\n    \"xml2js\": \"^0.4.23\",\n    \"yargs\": \"^17.3.0\"\n  },\n  \"engines\": {\n    \"node\": \">=12.9.0\"\n  },\n  \"files\": [\n    \"lib\",\n    \"src\",\n    \"etc/prepare.js\",\n    \"mongodb.d.ts\",\n    \"mongodb.ts34.d.ts\"\n  ],\n  \"homepage\": \"https://github.com/mongodb/node-mongodb-native\",\n  \"keywords\": [\n    \"mongodb\",\n    \"driver\",\n    \"official\"\n  ],\n  \"license\": \"Apache-2.0\",\n  \"main\": \"lib/index.js\",\n  \"name\": \"mongodb\",\n  \"optionalDependencies\": {\n    \"saslprep\": \"^1.0.3\"\n  },\n  \"repository\": {\n    \"type\": \"git\",\n    \"url\": \"git+ssh://git@github.com/mongodb/node-mongodb-native.git\"\n  },\n  \"scripts\": {\n    \"build:docs\": \"typedoc\",\n    \"build:dts\": \"npm run build:ts && api-extractor run && rimraf 'lib/**/*.d.ts*' && downlevel-dts mongodb.d.ts mongodb.ts34.d.ts\",\n    \"build:evergreen\": \"node .evergreen/generate_evergreen_tasks.js\",\n    \"build:ts\": \"rimraf lib && ./node_modules/typescript/bin/tsc\",\n    \"check:adl\": \"mocha --file test/tools/runner test/manual/data_lake.test.js\",\n    \"check:atlas\": \"mocha --config \\\"test/manual/mocharc.json\\\" test/manual/atlas_connectivity.test.js\",\n    \"check:aws\": \"mocha --file test/tools/runner test/functional/mongodb_aws.test.js\",\n    \"check:bench\": \"node test/benchmarks/driverBench\",\n    \"check:coverage\": \"nyc npm run test:all\",\n    \"check:csfle\": \"mocha --file test/tools/runner test/integration/client-side-encryption\",\n    \"check:dts\": \"./node_modules/typescript/bin/tsc --noEmit mongodb.d.ts && tsd\",\n    \"check:eslint\": \"eslint -v && eslint --max-warnings=0 --ext '.js,.ts' src test\",\n    \"check:kerberos\": \"mocha --config \\\"test/manual/mocharc.json\\\" test/manual/kerberos.test.js\",\n    \"check:ldap\": \"mocha --config \\\"test/manual/mocharc.json\\\" test/manual/ldap.test.js\",\n    \"check:lint\": \"npm run build:dts && npm run check:dts && npm run check:eslint && npm run check:tsd\",\n    \"check:ocsp\": \"mocha --config \\\"test/manual/mocharc.json\\\" test/manual/ocsp_support.test.js\",\n    \"check:snappy\": \"mocha --file test/tools/runner test/functional/unit_snappy.test.js\",\n    \"check:test\": \"mocha --file test/tools/runner --recursive test/functional test/integration\",\n    \"check:tls\": \"mocha --config \\\"test/manual/mocharc.json\\\" test/manual/tls_support.test.js\",\n    \"check:ts\": \"./node_modules/typescript/bin/tsc -v && ./node_modules/typescript/bin/tsc --noEmit\",\n    \"check:tsd\": \"tsd --version && tsd\",\n    \"check:unit\": \"mocha --recursive test/unit/\",\n    \"prepare\": \"node etc/prepare.js\",\n    \"release\": \"standard-version -i HISTORY.md\",\n    \"test\": \"npm run check:lint && npm run test:all\",\n    \"test:all\": \"npm run check:unit && npm run check:test\"\n  },\n  \"tsd\": {\n    \"directory\": \"test/types\",\n    \"compilerOptions\": {\n      \"strict\": true,\n      \"target\": \"esnext\",\n      \"module\": \"commonjs\",\n      \"moduleResolution\": \"node\"\n    }\n  },\n  \"types\": \"mongodb.d.ts\",\n  \"typesVersions\": {\n    \"<=4.0.2\": {\n      \"mongodb.d.ts\": [\n        \"mongodb.ts34.d.ts\"\n      ]\n    }\n  },\n  \"version\": \"4.2.2\"\n}\n","\nvar _a;\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.updateSessionFromResponse = exports.applySession = exports.commandSupportsReadConcern = exports.ServerSessionPool = exports.ServerSession = exports.maybeClearPinnedConnection = exports.ClientSession = void 0;\nconst promise_provider_1 = require(\"./promise_provider\");\nconst bson_1 = require(\"./bson\");\nconst read_preference_1 = require(\"./read_preference\");\nconst transactions_1 = require(\"./transactions\");\nconst common_1 = require(\"./sdam/common\");\nconst shared_1 = require(\"./cmap/wire_protocol/shared\");\nconst error_1 = require(\"./error\");\nconst utils_1 = require(\"./utils\");\nconst execute_operation_1 = require(\"./operations/execute_operation\");\nconst run_command_1 = require(\"./operations/run_command\");\nconst connection_1 = require(\"./cmap/connection\");\nconst metrics_1 = require(\"./cmap/metrics\");\nconst mongo_types_1 = require(\"./mongo_types\");\nconst read_concern_1 = require(\"./read_concern\");\nconst minWireVersionForShardedTransactions = 8;\nfunction assertAlive(session, callback) {\n    if (session.serverSession == null) {\n        const error = new error_1.MongoExpiredSessionError();\n        if (typeof callback === 'function') {\n            callback(error);\n            return false;\n        }\n        throw error;\n    }\n    return true;\n}\n/** @internal */\nconst kServerSession = Symbol('serverSession');\n/** @internal */\nconst kSnapshotTime = Symbol('snapshotTime');\n/** @internal */\nconst kSnapshotEnabled = Symbol('snapshotEnabled');\n/** @internal */\nconst kPinnedConnection = Symbol('pinnedConnection');\n/**\n * A class representing a client session on the server\n *\n * NOTE: not meant to be instantiated directly.\n * @public\n */\nclass ClientSession extends mongo_types_1.TypedEventEmitter {\n    /**\n     * Create a client session.\n     * @internal\n     * @param topology - The current client's topology (Internal Class)\n     * @param sessionPool - The server session pool (Internal Class)\n     * @param options - Optional settings\n     * @param clientOptions - Optional settings provided when creating a MongoClient\n     */\n    constructor(topology, sessionPool, options, clientOptions) {\n        super();\n        /** @internal */\n        this[_a] = false;\n        if (topology == null) {\n            // TODO(NODE-3483)\n            throw new error_1.MongoRuntimeError('ClientSession requires a topology');\n        }\n        if (sessionPool == null || !(sessionPool instanceof ServerSessionPool)) {\n            // TODO(NODE-3483)\n            throw new error_1.MongoRuntimeError('ClientSession requires a ServerSessionPool');\n        }\n        options = options !== null && options !== void 0 ? options : {};\n        if (options.snapshot === true) {\n            this[kSnapshotEnabled] = true;\n            if (options.causalConsistency === true) {\n                throw new error_1.MongoInvalidArgumentError('Properties \"causalConsistency\" and \"snapshot\" are mutually exclusive');\n            }\n        }\n        this.topology = topology;\n        this.sessionPool = sessionPool;\n        this.hasEnded = false;\n        this.clientOptions = clientOptions;\n        this[kServerSession] = undefined;\n        this.supports = {\n            causalConsistency: options.snapshot !== true && options.causalConsistency !== false\n        };\n        this.clusterTime = options.initialClusterTime;\n        this.operationTime = undefined;\n        this.explicit = !!options.explicit;\n        this.owner = options.owner;\n        this.defaultTransactionOptions = Object.assign({}, options.defaultTransactionOptions);\n        this.transaction = new transactions_1.Transaction();\n    }\n    /** The server id associated with this session */\n    get id() {\n        var _b;\n        return (_b = this.serverSession) === null || _b === void 0 ? void 0 : _b.id;\n    }\n    get serverSession() {\n        if (this[kServerSession] == null) {\n            this[kServerSession] = this.sessionPool.acquire();\n        }\n        // eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n        return this[kServerSession];\n    }\n    /** Whether or not this session is configured for snapshot reads */\n    get snapshotEnabled() {\n        return this[kSnapshotEnabled];\n    }\n    get loadBalanced() {\n        return this.topology.description.type === common_1.TopologyType.LoadBalanced;\n    }\n    /** @internal */\n    get pinnedConnection() {\n        return this[kPinnedConnection];\n    }\n    /** @internal */\n    pin(conn) {\n        if (this[kPinnedConnection]) {\n            throw TypeError('Cannot pin multiple connections to the same session');\n        }\n        this[kPinnedConnection] = conn;\n        conn.emit(connection_1.Connection.PINNED, this.inTransaction() ? metrics_1.ConnectionPoolMetrics.TXN : metrics_1.ConnectionPoolMetrics.CURSOR);\n    }\n    /** @internal */\n    unpin(options) {\n        if (this.loadBalanced) {\n            return maybeClearPinnedConnection(this, options);\n        }\n        this.transaction.unpinServer();\n    }\n    get isPinned() {\n        return this.loadBalanced ? !!this[kPinnedConnection] : this.transaction.isPinned;\n    }\n    endSession(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        const finalOptions = { force: true, ...options };\n        return (0, utils_1.maybePromise)(callback, done => {\n            if (this.hasEnded) {\n                maybeClearPinnedConnection(this, finalOptions);\n                return done();\n            }\n            const completeEndSession = () => {\n                maybeClearPinnedConnection(this, finalOptions);\n                // release the server session back to the pool\n                this.sessionPool.release(this.serverSession);\n                this[kServerSession] = undefined;\n                // mark the session as ended, and emit a signal\n                this.hasEnded = true;\n                this.emit('ended', this);\n                // spec indicates that we should ignore all errors for `endSessions`\n                done();\n            };\n            if (this.serverSession && this.inTransaction()) {\n                this.abortTransaction(err => {\n                    if (err)\n                        return done(err);\n                    completeEndSession();\n                });\n                return;\n            }\n            completeEndSession();\n        });\n    }\n    /**\n     * Advances the operationTime for a ClientSession.\n     *\n     * @param operationTime - the `BSON.Timestamp` of the operation type it is desired to advance to\n     */\n    advanceOperationTime(operationTime) {\n        if (this.operationTime == null) {\n            this.operationTime = operationTime;\n            return;\n        }\n        if (operationTime.greaterThan(this.operationTime)) {\n            this.operationTime = operationTime;\n        }\n    }\n    /**\n     * Advances the clusterTime for a ClientSession to the provided clusterTime of another ClientSession\n     *\n     * @param clusterTime - the $clusterTime returned by the server from another session in the form of a document containing the `BSON.Timestamp` clusterTime and signature\n     */\n    advanceClusterTime(clusterTime) {\n        var _b, _c;\n        if (!clusterTime || typeof clusterTime !== 'object') {\n            throw new error_1.MongoInvalidArgumentError('input cluster time must be an object');\n        }\n        if (!clusterTime.clusterTime || clusterTime.clusterTime._bsontype !== 'Timestamp') {\n            throw new error_1.MongoInvalidArgumentError('input cluster time \"clusterTime\" property must be a valid BSON Timestamp');\n        }\n        if (!clusterTime.signature ||\n            ((_b = clusterTime.signature.hash) === null || _b === void 0 ? void 0 : _b._bsontype) !== 'Binary' ||\n            (typeof clusterTime.signature.keyId !== 'number' &&\n                ((_c = clusterTime.signature.keyId) === null || _c === void 0 ? void 0 : _c._bsontype) !== 'Long') // apparently we decode the key to number?\n        ) {\n            throw new error_1.MongoInvalidArgumentError('input cluster time must have a valid \"signature\" property with BSON Binary hash and BSON Long keyId');\n        }\n        (0, common_1._advanceClusterTime)(this, clusterTime);\n    }\n    /**\n     * Used to determine if this session equals another\n     *\n     * @param session - The session to compare to\n     */\n    equals(session) {\n        if (!(session instanceof ClientSession)) {\n            return false;\n        }\n        if (this.id == null || session.id == null) {\n            return false;\n        }\n        return this.id.id.buffer.equals(session.id.id.buffer);\n    }\n    /** Increment the transaction number on the internal ServerSession */\n    incrementTransactionNumber() {\n        if (this.serverSession) {\n            this.serverSession.txnNumber =\n                typeof this.serverSession.txnNumber === 'number' ? this.serverSession.txnNumber + 1 : 0;\n        }\n    }\n    /** @returns whether this session is currently in a transaction or not */\n    inTransaction() {\n        return this.transaction.isActive;\n    }\n    /**\n     * Starts a new transaction with the given options.\n     *\n     * @param options - Options for the transaction\n     */\n    startTransaction(options) {\n        var _b, _c, _d, _e, _f, _g, _h, _j, _k, _l;\n        if (this[kSnapshotEnabled]) {\n            throw new error_1.MongoCompatibilityError('Transactions are not allowed with snapshot sessions');\n        }\n        assertAlive(this);\n        if (this.inTransaction()) {\n            throw new error_1.MongoTransactionError('Transaction already in progress');\n        }\n        if (this.isPinned && this.transaction.isCommitted) {\n            this.unpin();\n        }\n        const topologyMaxWireVersion = (0, utils_1.maxWireVersion)(this.topology);\n        if ((0, shared_1.isSharded)(this.topology) &&\n            topologyMaxWireVersion != null &&\n            topologyMaxWireVersion < minWireVersionForShardedTransactions) {\n            throw new error_1.MongoCompatibilityError('Transactions are not supported on sharded clusters in MongoDB < 4.2.');\n        }\n        // increment txnNumber\n        this.incrementTransactionNumber();\n        // create transaction state\n        this.transaction = new transactions_1.Transaction({\n            readConcern: (_c = (_b = options === null || options === void 0 ? void 0 : options.readConcern) !== null && _b !== void 0 ? _b : this.defaultTransactionOptions.readConcern) !== null && _c !== void 0 ? _c : (_d = this.clientOptions) === null || _d === void 0 ? void 0 : _d.readConcern,\n            writeConcern: (_f = (_e = options === null || options === void 0 ? void 0 : options.writeConcern) !== null && _e !== void 0 ? _e : this.defaultTransactionOptions.writeConcern) !== null && _f !== void 0 ? _f : (_g = this.clientOptions) === null || _g === void 0 ? void 0 : _g.writeConcern,\n            readPreference: (_j = (_h = options === null || options === void 0 ? void 0 : options.readPreference) !== null && _h !== void 0 ? _h : this.defaultTransactionOptions.readPreference) !== null && _j !== void 0 ? _j : (_k = this.clientOptions) === null || _k === void 0 ? void 0 : _k.readPreference,\n            maxCommitTimeMS: (_l = options === null || options === void 0 ? void 0 : options.maxCommitTimeMS) !== null && _l !== void 0 ? _l : this.defaultTransactionOptions.maxCommitTimeMS\n        });\n        this.transaction.transition(transactions_1.TxnState.STARTING_TRANSACTION);\n    }\n    commitTransaction(callback) {\n        return (0, utils_1.maybePromise)(callback, cb => endTransaction(this, 'commitTransaction', cb));\n    }\n    abortTransaction(callback) {\n        return (0, utils_1.maybePromise)(callback, cb => endTransaction(this, 'abortTransaction', cb));\n    }\n    /**\n     * This is here to ensure that ClientSession is never serialized to BSON.\n     */\n    toBSON() {\n        throw new error_1.MongoRuntimeError('ClientSession cannot be serialized to BSON.');\n    }\n    /**\n     * Runs a provided lambda within a transaction, retrying either the commit operation\n     * or entire transaction as needed (and when the error permits) to better ensure that\n     * the transaction can complete successfully.\n     *\n     * IMPORTANT: This method requires the user to return a Promise, all lambdas that do not\n     * return a Promise will result in undefined behavior.\n     *\n     * @param fn - A lambda to run within a transaction\n     * @param options - Optional settings for the transaction\n     */\n    withTransaction(fn, options) {\n        const startTime = (0, utils_1.now)();\n        return attemptTransaction(this, startTime, fn, options);\n    }\n}\nexports.ClientSession = ClientSession;\n_a = kSnapshotEnabled;\nconst MAX_WITH_TRANSACTION_TIMEOUT = 120000;\nconst NON_DETERMINISTIC_WRITE_CONCERN_ERRORS = new Set([\n    'CannotSatisfyWriteConcern',\n    'UnknownReplWriteConcern',\n    'UnsatisfiableWriteConcern'\n]);\nfunction hasNotTimedOut(startTime, max) {\n    return (0, utils_1.calculateDurationInMs)(startTime) < max;\n}\nfunction isUnknownTransactionCommitResult(err) {\n    const isNonDeterministicWriteConcernError = err instanceof error_1.MongoServerError &&\n        err.codeName &&\n        NON_DETERMINISTIC_WRITE_CONCERN_ERRORS.has(err.codeName);\n    return (isMaxTimeMSExpiredError(err) ||\n        (!isNonDeterministicWriteConcernError &&\n            err.code !== error_1.MONGODB_ERROR_CODES.UnsatisfiableWriteConcern &&\n            err.code !== error_1.MONGODB_ERROR_CODES.UnknownReplWriteConcern));\n}\nfunction maybeClearPinnedConnection(session, options) {\n    // unpin a connection if it has been pinned\n    const conn = session[kPinnedConnection];\n    const error = options === null || options === void 0 ? void 0 : options.error;\n    if (session.inTransaction() &&\n        error &&\n        error instanceof error_1.MongoError &&\n        error.hasErrorLabel('TransientTransactionError')) {\n        return;\n    }\n    // NOTE: the spec talks about what to do on a network error only, but the tests seem to\n    //       to validate that we don't unpin on _all_ errors?\n    if (conn) {\n        const servers = Array.from(session.topology.s.servers.values());\n        const loadBalancer = servers[0];\n        if ((options === null || options === void 0 ? void 0 : options.error) == null || (options === null || options === void 0 ? void 0 : options.force)) {\n            loadBalancer.s.pool.checkIn(conn);\n            conn.emit(connection_1.Connection.UNPINNED, session.transaction.state !== transactions_1.TxnState.NO_TRANSACTION\n                ? metrics_1.ConnectionPoolMetrics.TXN\n                : metrics_1.ConnectionPoolMetrics.CURSOR);\n            if (options === null || options === void 0 ? void 0 : options.forceClear) {\n                loadBalancer.s.pool.clear(conn.serviceId);\n            }\n        }\n        session[kPinnedConnection] = undefined;\n    }\n}\nexports.maybeClearPinnedConnection = maybeClearPinnedConnection;\nfunction isMaxTimeMSExpiredError(err) {\n    if (err == null || !(err instanceof error_1.MongoServerError)) {\n        return false;\n    }\n    return (err.code === error_1.MONGODB_ERROR_CODES.MaxTimeMSExpired ||\n        (err.writeConcernError && err.writeConcernError.code === error_1.MONGODB_ERROR_CODES.MaxTimeMSExpired));\n}\nfunction attemptTransactionCommit(session, startTime, fn, options) {\n    return session.commitTransaction().catch((err) => {\n        if (err instanceof error_1.MongoError &&\n            hasNotTimedOut(startTime, MAX_WITH_TRANSACTION_TIMEOUT) &&\n            !isMaxTimeMSExpiredError(err)) {\n            if (err.hasErrorLabel('UnknownTransactionCommitResult')) {\n                return attemptTransactionCommit(session, startTime, fn, options);\n            }\n            if (err.hasErrorLabel('TransientTransactionError')) {\n                return attemptTransaction(session, startTime, fn, options);\n            }\n        }\n        throw err;\n    });\n}\nconst USER_EXPLICIT_TXN_END_STATES = new Set([\n    transactions_1.TxnState.NO_TRANSACTION,\n    transactions_1.TxnState.TRANSACTION_COMMITTED,\n    transactions_1.TxnState.TRANSACTION_ABORTED\n]);\nfunction userExplicitlyEndedTransaction(session) {\n    return USER_EXPLICIT_TXN_END_STATES.has(session.transaction.state);\n}\nfunction attemptTransaction(session, startTime, fn, options) {\n    const Promise = promise_provider_1.PromiseProvider.get();\n    session.startTransaction(options);\n    let promise;\n    try {\n        promise = fn(session);\n    }\n    catch (err) {\n        promise = Promise.reject(err);\n    }\n    if (!(0, utils_1.isPromiseLike)(promise)) {\n        session.abortTransaction();\n        throw new error_1.MongoInvalidArgumentError('Function provided to `withTransaction` must return a Promise');\n    }\n    return promise.then(() => {\n        if (userExplicitlyEndedTransaction(session)) {\n            return;\n        }\n        return attemptTransactionCommit(session, startTime, fn, options);\n    }, err => {\n        function maybeRetryOrThrow(err) {\n            if (err instanceof error_1.MongoError &&\n                err.hasErrorLabel('TransientTransactionError') &&\n                hasNotTimedOut(startTime, MAX_WITH_TRANSACTION_TIMEOUT)) {\n                return attemptTransaction(session, startTime, fn, options);\n            }\n            if (isMaxTimeMSExpiredError(err)) {\n                err.addErrorLabel('UnknownTransactionCommitResult');\n            }\n            throw err;\n        }\n        if (session.transaction.isActive) {\n            return session.abortTransaction().then(() => maybeRetryOrThrow(err));\n        }\n        return maybeRetryOrThrow(err);\n    });\n}\nfunction endTransaction(session, commandName, callback) {\n    if (!assertAlive(session, callback)) {\n        // checking result in case callback was called\n        return;\n    }\n    // handle any initial problematic cases\n    const txnState = session.transaction.state;\n    if (txnState === transactions_1.TxnState.NO_TRANSACTION) {\n        callback(new error_1.MongoTransactionError('No transaction started'));\n        return;\n    }\n    if (commandName === 'commitTransaction') {\n        if (txnState === transactions_1.TxnState.STARTING_TRANSACTION ||\n            txnState === transactions_1.TxnState.TRANSACTION_COMMITTED_EMPTY) {\n            // the transaction was never started, we can safely exit here\n            session.transaction.transition(transactions_1.TxnState.TRANSACTION_COMMITTED_EMPTY);\n            callback();\n            return;\n        }\n        if (txnState === transactions_1.TxnState.TRANSACTION_ABORTED) {\n            callback(new error_1.MongoTransactionError('Cannot call commitTransaction after calling abortTransaction'));\n            return;\n        }\n    }\n    else {\n        if (txnState === transactions_1.TxnState.STARTING_TRANSACTION) {\n            // the transaction was never started, we can safely exit here\n            session.transaction.transition(transactions_1.TxnState.TRANSACTION_ABORTED);\n            callback();\n            return;\n        }\n        if (txnState === transactions_1.TxnState.TRANSACTION_ABORTED) {\n            callback(new error_1.MongoTransactionError('Cannot call abortTransaction twice'));\n            return;\n        }\n        if (txnState === transactions_1.TxnState.TRANSACTION_COMMITTED ||\n            txnState === transactions_1.TxnState.TRANSACTION_COMMITTED_EMPTY) {\n            callback(new error_1.MongoTransactionError('Cannot call abortTransaction after calling commitTransaction'));\n            return;\n        }\n    }\n    // construct and send the command\n    const command = { [commandName]: 1 };\n    // apply a writeConcern if specified\n    let writeConcern;\n    if (session.transaction.options.writeConcern) {\n        writeConcern = Object.assign({}, session.transaction.options.writeConcern);\n    }\n    else if (session.clientOptions && session.clientOptions.writeConcern) {\n        writeConcern = { w: session.clientOptions.writeConcern.w };\n    }\n    if (txnState === transactions_1.TxnState.TRANSACTION_COMMITTED) {\n        writeConcern = Object.assign({ wtimeout: 10000 }, writeConcern, { w: 'majority' });\n    }\n    if (writeConcern) {\n        Object.assign(command, { writeConcern });\n    }\n    if (commandName === 'commitTransaction' && session.transaction.options.maxTimeMS) {\n        Object.assign(command, { maxTimeMS: session.transaction.options.maxTimeMS });\n    }\n    function commandHandler(e, r) {\n        if (commandName !== 'commitTransaction') {\n            session.transaction.transition(transactions_1.TxnState.TRANSACTION_ABORTED);\n            if (session.loadBalanced) {\n                maybeClearPinnedConnection(session, { force: false });\n            }\n            // The spec indicates that we should ignore all errors on `abortTransaction`\n            return callback();\n        }\n        session.transaction.transition(transactions_1.TxnState.TRANSACTION_COMMITTED);\n        if (e) {\n            if (e instanceof error_1.MongoNetworkError ||\n                e instanceof error_1.MongoWriteConcernError ||\n                (0, error_1.isRetryableError)(e) ||\n                isMaxTimeMSExpiredError(e)) {\n                if (isUnknownTransactionCommitResult(e)) {\n                    e.addErrorLabel('UnknownTransactionCommitResult');\n                    // per txns spec, must unpin session in this case\n                    session.unpin({ error: e });\n                }\n            }\n            else if (e.hasErrorLabel('TransientTransactionError')) {\n                session.unpin({ error: e });\n            }\n        }\n        callback(e, r);\n    }\n    // Assumption here that commandName is \"commitTransaction\" or \"abortTransaction\"\n    if (session.transaction.recoveryToken) {\n        command.recoveryToken = session.transaction.recoveryToken;\n    }\n    // send the command\n    (0, execute_operation_1.executeOperation)(session.topology, new run_command_1.RunAdminCommandOperation(undefined, command, {\n        session,\n        readPreference: read_preference_1.ReadPreference.primary,\n        bypassPinningCheck: true\n    }), (err, reply) => {\n        if (command.abortTransaction) {\n            // always unpin on abort regardless of command outcome\n            session.unpin();\n        }\n        if (err && (0, error_1.isRetryableEndTransactionError)(err)) {\n            // SPEC-1185: apply majority write concern when retrying commitTransaction\n            if (command.commitTransaction) {\n                // per txns spec, must unpin session in this case\n                session.unpin({ force: true });\n                command.writeConcern = Object.assign({ wtimeout: 10000 }, command.writeConcern, {\n                    w: 'majority'\n                });\n            }\n            return (0, execute_operation_1.executeOperation)(session.topology, new run_command_1.RunAdminCommandOperation(undefined, command, {\n                session,\n                readPreference: read_preference_1.ReadPreference.primary,\n                bypassPinningCheck: true\n            }), (_err, _reply) => commandHandler(_err, _reply));\n        }\n        commandHandler(err, reply);\n    });\n}\n/**\n * Reflects the existence of a session on the server. Can be reused by the session pool.\n * WARNING: not meant to be instantiated directly. For internal use only.\n * @public\n */\nclass ServerSession {\n    /** @internal */\n    constructor() {\n        this.id = { id: new bson_1.Binary((0, utils_1.uuidV4)(), bson_1.Binary.SUBTYPE_UUID) };\n        this.lastUse = (0, utils_1.now)();\n        this.txnNumber = 0;\n        this.isDirty = false;\n    }\n    /**\n     * Determines if the server session has timed out.\n     *\n     * @param sessionTimeoutMinutes - The server's \"logicalSessionTimeoutMinutes\"\n     */\n    hasTimedOut(sessionTimeoutMinutes) {\n        // Take the difference of the lastUse timestamp and now, which will result in a value in\n        // milliseconds, and then convert milliseconds to minutes to compare to `sessionTimeoutMinutes`\n        const idleTimeMinutes = Math.round((((0, utils_1.calculateDurationInMs)(this.lastUse) % 86400000) % 3600000) / 60000);\n        return idleTimeMinutes > sessionTimeoutMinutes - 1;\n    }\n}\nexports.ServerSession = ServerSession;\n/**\n * Maintains a pool of Server Sessions.\n * For internal use only\n * @internal\n */\nclass ServerSessionPool {\n    constructor(topology) {\n        if (topology == null) {\n            throw new error_1.MongoRuntimeError('ServerSessionPool requires a topology');\n        }\n        this.topology = topology;\n        this.sessions = [];\n    }\n    /** Ends all sessions in the session pool */\n    endAllPooledSessions(callback) {\n        if (this.sessions.length) {\n            this.topology.endSessions(this.sessions.map((session) => session.id), () => {\n                this.sessions = [];\n                if (typeof callback === 'function') {\n                    callback();\n                }\n            });\n            return;\n        }\n        if (typeof callback === 'function') {\n            callback();\n        }\n    }\n    /**\n     * Acquire a Server Session from the pool.\n     * Iterates through each session in the pool, removing any stale sessions\n     * along the way. The first non-stale session found is removed from the\n     * pool and returned. If no non-stale session is found, a new ServerSession is created.\n     */\n    acquire() {\n        const sessionTimeoutMinutes = this.topology.logicalSessionTimeoutMinutes || 10;\n        while (this.sessions.length) {\n            const session = this.sessions.shift();\n            if (session && (this.topology.loadBalanced || !session.hasTimedOut(sessionTimeoutMinutes))) {\n                return session;\n            }\n        }\n        return new ServerSession();\n    }\n    /**\n     * Release a session to the session pool\n     * Adds the session back to the session pool if the session has not timed out yet.\n     * This method also removes any stale sessions from the pool.\n     *\n     * @param session - The session to release to the pool\n     */\n    release(session) {\n        const sessionTimeoutMinutes = this.topology.logicalSessionTimeoutMinutes;\n        if (this.topology.loadBalanced && !sessionTimeoutMinutes) {\n            this.sessions.unshift(session);\n        }\n        if (!sessionTimeoutMinutes) {\n            return;\n        }\n        while (this.sessions.length) {\n            const pooledSession = this.sessions[this.sessions.length - 1];\n            if (pooledSession.hasTimedOut(sessionTimeoutMinutes)) {\n                this.sessions.pop();\n            }\n            else {\n                break;\n            }\n        }\n        if (!session.hasTimedOut(sessionTimeoutMinutes)) {\n            if (session.isDirty) {\n                return;\n            }\n            // otherwise, readd this session to the session pool\n            this.sessions.unshift(session);\n        }\n    }\n}\nexports.ServerSessionPool = ServerSessionPool;\n// TODO: this should be codified in command construction\n// @see https://github.com/mongodb/specifications/blob/master/source/read-write-concern/read-write-concern.rst#read-concern\nfunction commandSupportsReadConcern(command, options) {\n    if (command.aggregate || command.count || command.distinct || command.find || command.geoNear) {\n        return true;\n    }\n    if (command.mapReduce &&\n        options &&\n        options.out &&\n        (options.out.inline === 1 || options.out === 'inline')) {\n        return true;\n    }\n    return false;\n}\nexports.commandSupportsReadConcern = commandSupportsReadConcern;\n/**\n * Optionally decorate a command with sessions specific keys\n *\n * @param session - the session tracking transaction state\n * @param command - the command to decorate\n * @param options - Optional settings passed to calling operation\n */\nfunction applySession(session, command, options) {\n    var _b;\n    // TODO: merge this with `assertAlive`, did not want to throw a try/catch here\n    if (session.hasEnded) {\n        return new error_1.MongoExpiredSessionError();\n    }\n    const serverSession = session.serverSession;\n    if (serverSession == null) {\n        return new error_1.MongoRuntimeError('Unable to acquire server session');\n    }\n    // SPEC-1019: silently ignore explicit session with unacknowledged write for backwards compatibility\n    // FIXME: NODE-2781, this check for write concern shouldn't be happening here, but instead during command construction\n    if (options && options.writeConcern && options.writeConcern.w === 0) {\n        if (session && session.explicit) {\n            return new error_1.MongoAPIError('Cannot have explicit session with unacknowledged writes');\n        }\n        return;\n    }\n    // mark the last use of this session, and apply the `lsid`\n    serverSession.lastUse = (0, utils_1.now)();\n    command.lsid = serverSession.id;\n    // first apply non-transaction-specific sessions data\n    const inTransaction = session.inTransaction() || (0, transactions_1.isTransactionCommand)(command);\n    const isRetryableWrite = (options === null || options === void 0 ? void 0 : options.willRetryWrite) || false;\n    if (serverSession.txnNumber && (isRetryableWrite || inTransaction)) {\n        command.txnNumber = bson_1.Long.fromNumber(serverSession.txnNumber);\n    }\n    if (!inTransaction) {\n        if (session.transaction.state !== transactions_1.TxnState.NO_TRANSACTION) {\n            session.transaction.transition(transactions_1.TxnState.NO_TRANSACTION);\n        }\n        if (session.supports.causalConsistency &&\n            session.operationTime &&\n            commandSupportsReadConcern(command, options)) {\n            command.readConcern = command.readConcern || {};\n            Object.assign(command.readConcern, { afterClusterTime: session.operationTime });\n        }\n        else if (session[kSnapshotEnabled]) {\n            command.readConcern = command.readConcern || { level: read_concern_1.ReadConcernLevel.snapshot };\n            if (session[kSnapshotTime] != null) {\n                Object.assign(command.readConcern, { atClusterTime: session[kSnapshotTime] });\n            }\n        }\n        return;\n    }\n    // now attempt to apply transaction-specific sessions data\n    // `autocommit` must always be false to differentiate from retryable writes\n    command.autocommit = false;\n    if (session.transaction.state === transactions_1.TxnState.STARTING_TRANSACTION) {\n        session.transaction.transition(transactions_1.TxnState.TRANSACTION_IN_PROGRESS);\n        command.startTransaction = true;\n        const readConcern = session.transaction.options.readConcern || ((_b = session === null || session === void 0 ? void 0 : session.clientOptions) === null || _b === void 0 ? void 0 : _b.readConcern);\n        if (readConcern) {\n            command.readConcern = readConcern;\n        }\n        if (session.supports.causalConsistency && session.operationTime) {\n            command.readConcern = command.readConcern || {};\n            Object.assign(command.readConcern, { afterClusterTime: session.operationTime });\n        }\n    }\n}\nexports.applySession = applySession;\nfunction updateSessionFromResponse(session, document) {\n    var _b;\n    if (document.$clusterTime) {\n        (0, common_1._advanceClusterTime)(session, document.$clusterTime);\n    }\n    if (document.operationTime && session && session.supports.causalConsistency) {\n        session.advanceOperationTime(document.operationTime);\n    }\n    if (document.recoveryToken && session && session.inTransaction()) {\n        session.transaction._recoveryToken = document.recoveryToken;\n    }\n    if ((session === null || session === void 0 ? void 0 : session[kSnapshotEnabled]) && session[kSnapshotTime] == null) {\n        // find and aggregate commands return atClusterTime on the cursor\n        // distinct includes it in the response body\n        const atClusterTime = ((_b = document.cursor) === null || _b === void 0 ? void 0 : _b.atClusterTime) || document.atClusterTime;\n        if (atClusterTime) {\n            session[kSnapshotTime] = atClusterTime;\n        }\n    }\n}\nexports.updateSessionFromResponse = updateSessionFromResponse;\n//# sourceMappingURL=sessions.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.isTransactionCommand = exports.Transaction = exports.TxnState = void 0;\nconst read_preference_1 = require(\"./read_preference\");\nconst error_1 = require(\"./error\");\nconst read_concern_1 = require(\"./read_concern\");\nconst write_concern_1 = require(\"./write_concern\");\n/** @internal */\nexports.TxnState = Object.freeze({\n    NO_TRANSACTION: 'NO_TRANSACTION',\n    STARTING_TRANSACTION: 'STARTING_TRANSACTION',\n    TRANSACTION_IN_PROGRESS: 'TRANSACTION_IN_PROGRESS',\n    TRANSACTION_COMMITTED: 'TRANSACTION_COMMITTED',\n    TRANSACTION_COMMITTED_EMPTY: 'TRANSACTION_COMMITTED_EMPTY',\n    TRANSACTION_ABORTED: 'TRANSACTION_ABORTED'\n});\nconst stateMachine = {\n    [exports.TxnState.NO_TRANSACTION]: [exports.TxnState.NO_TRANSACTION, exports.TxnState.STARTING_TRANSACTION],\n    [exports.TxnState.STARTING_TRANSACTION]: [\n        exports.TxnState.TRANSACTION_IN_PROGRESS,\n        exports.TxnState.TRANSACTION_COMMITTED,\n        exports.TxnState.TRANSACTION_COMMITTED_EMPTY,\n        exports.TxnState.TRANSACTION_ABORTED\n    ],\n    [exports.TxnState.TRANSACTION_IN_PROGRESS]: [\n        exports.TxnState.TRANSACTION_IN_PROGRESS,\n        exports.TxnState.TRANSACTION_COMMITTED,\n        exports.TxnState.TRANSACTION_ABORTED\n    ],\n    [exports.TxnState.TRANSACTION_COMMITTED]: [\n        exports.TxnState.TRANSACTION_COMMITTED,\n        exports.TxnState.TRANSACTION_COMMITTED_EMPTY,\n        exports.TxnState.STARTING_TRANSACTION,\n        exports.TxnState.NO_TRANSACTION\n    ],\n    [exports.TxnState.TRANSACTION_ABORTED]: [exports.TxnState.STARTING_TRANSACTION, exports.TxnState.NO_TRANSACTION],\n    [exports.TxnState.TRANSACTION_COMMITTED_EMPTY]: [\n        exports.TxnState.TRANSACTION_COMMITTED_EMPTY,\n        exports.TxnState.NO_TRANSACTION\n    ]\n};\nconst ACTIVE_STATES = new Set([\n    exports.TxnState.STARTING_TRANSACTION,\n    exports.TxnState.TRANSACTION_IN_PROGRESS\n]);\nconst COMMITTED_STATES = new Set([\n    exports.TxnState.TRANSACTION_COMMITTED,\n    exports.TxnState.TRANSACTION_COMMITTED_EMPTY,\n    exports.TxnState.TRANSACTION_ABORTED\n]);\n/**\n * @public\n * A class maintaining state related to a server transaction. Internal Only\n */\nclass Transaction {\n    /** Create a transaction @internal */\n    constructor(options) {\n        options = options !== null && options !== void 0 ? options : {};\n        this.state = exports.TxnState.NO_TRANSACTION;\n        this.options = {};\n        const writeConcern = write_concern_1.WriteConcern.fromOptions(options);\n        if (writeConcern) {\n            if (writeConcern.w === 0) {\n                throw new error_1.MongoTransactionError('Transactions do not support unacknowledged write concern');\n            }\n            this.options.writeConcern = writeConcern;\n        }\n        if (options.readConcern) {\n            this.options.readConcern = read_concern_1.ReadConcern.fromOptions(options);\n        }\n        if (options.readPreference) {\n            this.options.readPreference = read_preference_1.ReadPreference.fromOptions(options);\n        }\n        if (options.maxCommitTimeMS) {\n            this.options.maxTimeMS = options.maxCommitTimeMS;\n        }\n        // TODO: This isn't technically necessary\n        this._pinnedServer = undefined;\n        this._recoveryToken = undefined;\n    }\n    /** @internal */\n    get server() {\n        return this._pinnedServer;\n    }\n    get recoveryToken() {\n        return this._recoveryToken;\n    }\n    get isPinned() {\n        return !!this.server;\n    }\n    /** @returns Whether the transaction has started */\n    get isStarting() {\n        return this.state === exports.TxnState.STARTING_TRANSACTION;\n    }\n    /**\n     * @returns Whether this session is presently in a transaction\n     */\n    get isActive() {\n        return ACTIVE_STATES.has(this.state);\n    }\n    get isCommitted() {\n        return COMMITTED_STATES.has(this.state);\n    }\n    /**\n     * Transition the transaction in the state machine\n     * @internal\n     * @param nextState - The new state to transition to\n     */\n    transition(nextState) {\n        const nextStates = stateMachine[this.state];\n        if (nextStates && nextStates.includes(nextState)) {\n            this.state = nextState;\n            if (this.state === exports.TxnState.NO_TRANSACTION ||\n                this.state === exports.TxnState.STARTING_TRANSACTION ||\n                this.state === exports.TxnState.TRANSACTION_ABORTED) {\n                this.unpinServer();\n            }\n            return;\n        }\n        throw new error_1.MongoRuntimeError(`Attempted illegal state transition from [${this.state}] to [${nextState}]`);\n    }\n    /** @internal */\n    pinServer(server) {\n        if (this.isActive) {\n            this._pinnedServer = server;\n        }\n    }\n    /** @internal */\n    unpinServer() {\n        this._pinnedServer = undefined;\n    }\n}\nexports.Transaction = Transaction;\nfunction isTransactionCommand(command) {\n    return !!(command.commitTransaction || command.abortTransaction);\n}\nexports.isTransactionCommand = isTransactionCommand;\n//# sourceMappingURL=transactions.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.isSharded = exports.applyCommonQueryOptions = exports.getReadPreference = void 0;\nconst common_1 = require(\"../../sdam/common\");\nconst topology_description_1 = require(\"../../sdam/topology_description\");\nconst error_1 = require(\"../../error\");\nconst read_preference_1 = require(\"../../read_preference\");\nfunction getReadPreference(cmd, options) {\n    // Default to command version of the readPreference\n    let readPreference = cmd.readPreference || read_preference_1.ReadPreference.primary;\n    // If we have an option readPreference override the command one\n    if (options === null || options === void 0 ? void 0 : options.readPreference) {\n        readPreference = options.readPreference;\n    }\n    if (typeof readPreference === 'string') {\n        readPreference = read_preference_1.ReadPreference.fromString(readPreference);\n    }\n    if (!(readPreference instanceof read_preference_1.ReadPreference)) {\n        throw new error_1.MongoInvalidArgumentError('Option \"readPreference\" must be a ReadPreference instance');\n    }\n    return readPreference;\n}\nexports.getReadPreference = getReadPreference;\nfunction applyCommonQueryOptions(queryOptions, options) {\n    Object.assign(queryOptions, {\n        raw: typeof options.raw === 'boolean' ? options.raw : false,\n        promoteLongs: typeof options.promoteLongs === 'boolean' ? options.promoteLongs : true,\n        promoteValues: typeof options.promoteValues === 'boolean' ? options.promoteValues : true,\n        promoteBuffers: typeof options.promoteBuffers === 'boolean' ? options.promoteBuffers : false,\n        bsonRegExp: typeof options.bsonRegExp === 'boolean' ? options.bsonRegExp : false\n    });\n    if (options.session) {\n        queryOptions.session = options.session;\n    }\n    return queryOptions;\n}\nexports.applyCommonQueryOptions = applyCommonQueryOptions;\nfunction isSharded(topologyOrServer) {\n    if (topologyOrServer.description && topologyOrServer.description.type === common_1.ServerType.Mongos) {\n        return true;\n    }\n    // NOTE: This is incredibly inefficient, and should be removed once command construction\n    //       happens based on `Server` not `Topology`.\n    if (topologyOrServer.description && topologyOrServer.description instanceof topology_description_1.TopologyDescription) {\n        const servers = Array.from(topologyOrServer.description.servers.values());\n        return servers.some((server) => server.type === common_1.ServerType.Mongos);\n    }\n    return false;\n}\nexports.isSharded = isSharded;\n//# sourceMappingURL=shared.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.TopologyDescription = void 0;\nconst server_description_1 = require(\"./server_description\");\nconst WIRE_CONSTANTS = require(\"../cmap/wire_protocol/constants\");\nconst common_1 = require(\"./common\");\nconst error_1 = require(\"../error\");\nconst utils_1 = require(\"../utils\");\n// constants related to compatibility checks\nconst MIN_SUPPORTED_SERVER_VERSION = WIRE_CONSTANTS.MIN_SUPPORTED_SERVER_VERSION;\nconst MAX_SUPPORTED_SERVER_VERSION = WIRE_CONSTANTS.MAX_SUPPORTED_SERVER_VERSION;\nconst MIN_SUPPORTED_WIRE_VERSION = WIRE_CONSTANTS.MIN_SUPPORTED_WIRE_VERSION;\nconst MAX_SUPPORTED_WIRE_VERSION = WIRE_CONSTANTS.MAX_SUPPORTED_WIRE_VERSION;\nconst MONGOS_OR_UNKNOWN = new Set([common_1.ServerType.Mongos, common_1.ServerType.Unknown]);\nconst MONGOS_OR_STANDALONE = new Set([common_1.ServerType.Mongos, common_1.ServerType.Standalone]);\nconst NON_PRIMARY_RS_MEMBERS = new Set([\n    common_1.ServerType.RSSecondary,\n    common_1.ServerType.RSArbiter,\n    common_1.ServerType.RSOther\n]);\n/**\n * Representation of a deployment of servers\n * @public\n */\nclass TopologyDescription {\n    /**\n     * Create a TopologyDescription\n     */\n    constructor(topologyType, serverDescriptions, setName, maxSetVersion, maxElectionId, commonWireVersion, options) {\n        var _a, _b;\n        options = options !== null && options !== void 0 ? options : {};\n        // TODO: consider assigning all these values to a temporary value `s` which\n        //       we use `Object.freeze` on, ensuring the internal state of this type\n        //       is immutable.\n        this.type = topologyType !== null && topologyType !== void 0 ? topologyType : common_1.TopologyType.Unknown;\n        this.servers = serverDescriptions !== null && serverDescriptions !== void 0 ? serverDescriptions : new Map();\n        this.stale = false;\n        this.compatible = true;\n        this.heartbeatFrequencyMS = (_a = options.heartbeatFrequencyMS) !== null && _a !== void 0 ? _a : 0;\n        this.localThresholdMS = (_b = options.localThresholdMS) !== null && _b !== void 0 ? _b : 0;\n        if (setName) {\n            this.setName = setName;\n        }\n        if (maxSetVersion) {\n            this.maxSetVersion = maxSetVersion;\n        }\n        if (maxElectionId) {\n            this.maxElectionId = maxElectionId;\n        }\n        if (commonWireVersion) {\n            this.commonWireVersion = commonWireVersion;\n        }\n        // determine server compatibility\n        for (const serverDescription of this.servers.values()) {\n            // Load balancer mode is always compatible.\n            if (serverDescription.type === common_1.ServerType.Unknown ||\n                serverDescription.type === common_1.ServerType.LoadBalancer) {\n                continue;\n            }\n            if (serverDescription.minWireVersion > MAX_SUPPORTED_WIRE_VERSION) {\n                this.compatible = false;\n                this.compatibilityError = `Server at ${serverDescription.address} requires wire version ${serverDescription.minWireVersion}, but this version of the driver only supports up to ${MAX_SUPPORTED_WIRE_VERSION} (MongoDB ${MAX_SUPPORTED_SERVER_VERSION})`;\n            }\n            if (serverDescription.maxWireVersion < MIN_SUPPORTED_WIRE_VERSION) {\n                this.compatible = false;\n                this.compatibilityError = `Server at ${serverDescription.address} reports wire version ${serverDescription.maxWireVersion}, but this version of the driver requires at least ${MIN_SUPPORTED_WIRE_VERSION} (MongoDB ${MIN_SUPPORTED_SERVER_VERSION}).`;\n                break;\n            }\n        }\n        // Whenever a client updates the TopologyDescription from an ismaster response, it MUST set\n        // TopologyDescription.logicalSessionTimeoutMinutes to the smallest logicalSessionTimeoutMinutes\n        // value among ServerDescriptions of all data-bearing server types. If any have a null\n        // logicalSessionTimeoutMinutes, then TopologyDescription.logicalSessionTimeoutMinutes MUST be\n        // set to null.\n        this.logicalSessionTimeoutMinutes = undefined;\n        for (const [, server] of this.servers) {\n            if (server.isReadable) {\n                if (server.logicalSessionTimeoutMinutes == null) {\n                    // If any of the servers have a null logicalSessionsTimeout, then the whole topology does\n                    this.logicalSessionTimeoutMinutes = undefined;\n                    break;\n                }\n                if (this.logicalSessionTimeoutMinutes == null) {\n                    // First server with a non null logicalSessionsTimeout\n                    this.logicalSessionTimeoutMinutes = server.logicalSessionTimeoutMinutes;\n                    continue;\n                }\n                // Always select the smaller of the:\n                // current server logicalSessionsTimeout and the topologies logicalSessionsTimeout\n                this.logicalSessionTimeoutMinutes = Math.min(this.logicalSessionTimeoutMinutes, server.logicalSessionTimeoutMinutes);\n            }\n        }\n    }\n    /**\n     * Returns a new TopologyDescription based on the SrvPollingEvent\n     * @internal\n     */\n    updateFromSrvPollingEvent(ev, srvMaxHosts = 0) {\n        /** The SRV addresses defines the set of addresses we should be using */\n        const incomingHostnames = ev.hostnames();\n        const currentHostnames = new Set(this.servers.keys());\n        const hostnamesToAdd = new Set(incomingHostnames);\n        const hostnamesToRemove = new Set();\n        for (const hostname of currentHostnames) {\n            // filter hostnamesToAdd (made from incomingHostnames) down to what is *not* present in currentHostnames\n            hostnamesToAdd.delete(hostname);\n            if (!incomingHostnames.has(hostname)) {\n                // If the SRV Records no longer include this hostname\n                // we have to stop using it\n                hostnamesToRemove.add(hostname);\n            }\n        }\n        if (hostnamesToAdd.size === 0 && hostnamesToRemove.size === 0) {\n            // No new hosts to add and none to remove\n            return this;\n        }\n        const serverDescriptions = new Map(this.servers);\n        for (const removedHost of hostnamesToRemove) {\n            serverDescriptions.delete(removedHost);\n        }\n        if (hostnamesToAdd.size > 0) {\n            if (srvMaxHosts === 0) {\n                // Add all!\n                for (const hostToAdd of hostnamesToAdd) {\n                    serverDescriptions.set(hostToAdd, new server_description_1.ServerDescription(hostToAdd));\n                }\n            }\n            else if (serverDescriptions.size < srvMaxHosts) {\n                // Add only the amount needed to get us back to srvMaxHosts\n                const selectedHosts = (0, utils_1.shuffle)(hostnamesToAdd, srvMaxHosts - serverDescriptions.size);\n                for (const selectedHostToAdd of selectedHosts) {\n                    serverDescriptions.set(selectedHostToAdd, new server_description_1.ServerDescription(selectedHostToAdd));\n                }\n            }\n        }\n        return new TopologyDescription(this.type, serverDescriptions, this.setName, this.maxSetVersion, this.maxElectionId, this.commonWireVersion, { heartbeatFrequencyMS: this.heartbeatFrequencyMS, localThresholdMS: this.localThresholdMS });\n    }\n    /**\n     * Returns a copy of this description updated with a given ServerDescription\n     * @internal\n     */\n    update(serverDescription) {\n        const address = serverDescription.address;\n        // potentially mutated values\n        let { type: topologyType, setName, maxSetVersion, maxElectionId, commonWireVersion } = this;\n        if (serverDescription.setName && setName && serverDescription.setName !== setName) {\n            serverDescription = new server_description_1.ServerDescription(address, undefined);\n        }\n        const serverType = serverDescription.type;\n        const serverDescriptions = new Map(this.servers);\n        // update common wire version\n        if (serverDescription.maxWireVersion !== 0) {\n            if (commonWireVersion == null) {\n                commonWireVersion = serverDescription.maxWireVersion;\n            }\n            else {\n                commonWireVersion = Math.min(commonWireVersion, serverDescription.maxWireVersion);\n            }\n        }\n        // update the actual server description\n        serverDescriptions.set(address, serverDescription);\n        if (topologyType === common_1.TopologyType.Single) {\n            // once we are defined as single, that never changes\n            return new TopologyDescription(common_1.TopologyType.Single, serverDescriptions, setName, maxSetVersion, maxElectionId, commonWireVersion, { heartbeatFrequencyMS: this.heartbeatFrequencyMS, localThresholdMS: this.localThresholdMS });\n        }\n        if (topologyType === common_1.TopologyType.Unknown) {\n            if (serverType === common_1.ServerType.Standalone && this.servers.size !== 1) {\n                serverDescriptions.delete(address);\n            }\n            else {\n                topologyType = topologyTypeForServerType(serverType);\n            }\n        }\n        if (topologyType === common_1.TopologyType.Sharded) {\n            if (!MONGOS_OR_UNKNOWN.has(serverType)) {\n                serverDescriptions.delete(address);\n            }\n        }\n        if (topologyType === common_1.TopologyType.ReplicaSetNoPrimary) {\n            if (MONGOS_OR_STANDALONE.has(serverType)) {\n                serverDescriptions.delete(address);\n            }\n            if (serverType === common_1.ServerType.RSPrimary) {\n                const result = updateRsFromPrimary(serverDescriptions, serverDescription, setName, maxSetVersion, maxElectionId);\n                topologyType = result[0];\n                setName = result[1];\n                maxSetVersion = result[2];\n                maxElectionId = result[3];\n            }\n            else if (NON_PRIMARY_RS_MEMBERS.has(serverType)) {\n                const result = updateRsNoPrimaryFromMember(serverDescriptions, serverDescription, setName);\n                topologyType = result[0];\n                setName = result[1];\n            }\n        }\n        if (topologyType === common_1.TopologyType.ReplicaSetWithPrimary) {\n            if (MONGOS_OR_STANDALONE.has(serverType)) {\n                serverDescriptions.delete(address);\n                topologyType = checkHasPrimary(serverDescriptions);\n            }\n            else if (serverType === common_1.ServerType.RSPrimary) {\n                const result = updateRsFromPrimary(serverDescriptions, serverDescription, setName, maxSetVersion, maxElectionId);\n                topologyType = result[0];\n                setName = result[1];\n                maxSetVersion = result[2];\n                maxElectionId = result[3];\n            }\n            else if (NON_PRIMARY_RS_MEMBERS.has(serverType)) {\n                topologyType = updateRsWithPrimaryFromMember(serverDescriptions, serverDescription, setName);\n            }\n            else {\n                topologyType = checkHasPrimary(serverDescriptions);\n            }\n        }\n        return new TopologyDescription(topologyType, serverDescriptions, setName, maxSetVersion, maxElectionId, commonWireVersion, { heartbeatFrequencyMS: this.heartbeatFrequencyMS, localThresholdMS: this.localThresholdMS });\n    }\n    get error() {\n        const descriptionsWithError = Array.from(this.servers.values()).filter((sd) => sd.error);\n        if (descriptionsWithError.length > 0) {\n            return descriptionsWithError[0].error;\n        }\n    }\n    /**\n     * Determines if the topology description has any known servers\n     */\n    get hasKnownServers() {\n        return Array.from(this.servers.values()).some((sd) => sd.type !== common_1.ServerType.Unknown);\n    }\n    /**\n     * Determines if this topology description has a data-bearing server available.\n     */\n    get hasDataBearingServers() {\n        return Array.from(this.servers.values()).some((sd) => sd.isDataBearing);\n    }\n    /**\n     * Determines if the topology has a definition for the provided address\n     * @internal\n     */\n    hasServer(address) {\n        return this.servers.has(address);\n    }\n}\nexports.TopologyDescription = TopologyDescription;\nfunction topologyTypeForServerType(serverType) {\n    switch (serverType) {\n        case common_1.ServerType.Standalone:\n            return common_1.TopologyType.Single;\n        case common_1.ServerType.Mongos:\n            return common_1.TopologyType.Sharded;\n        case common_1.ServerType.RSPrimary:\n            return common_1.TopologyType.ReplicaSetWithPrimary;\n        case common_1.ServerType.RSOther:\n        case common_1.ServerType.RSSecondary:\n            return common_1.TopologyType.ReplicaSetNoPrimary;\n        default:\n            return common_1.TopologyType.Unknown;\n    }\n}\n// TODO: improve these docs when ObjectId is properly typed\nfunction compareObjectId(oid1, oid2) {\n    if (oid1 == null) {\n        return -1;\n    }\n    if (oid2 == null) {\n        return 1;\n    }\n    if (oid1.id instanceof Buffer && oid2.id instanceof Buffer) {\n        const oid1Buffer = oid1.id;\n        const oid2Buffer = oid2.id;\n        return oid1Buffer.compare(oid2Buffer);\n    }\n    const oid1String = oid1.toString();\n    const oid2String = oid2.toString();\n    return oid1String.localeCompare(oid2String);\n}\nfunction updateRsFromPrimary(serverDescriptions, serverDescription, setName, maxSetVersion, maxElectionId) {\n    setName = setName || serverDescription.setName;\n    if (setName !== serverDescription.setName) {\n        serverDescriptions.delete(serverDescription.address);\n        return [checkHasPrimary(serverDescriptions), setName, maxSetVersion, maxElectionId];\n    }\n    const electionId = serverDescription.electionId ? serverDescription.electionId : null;\n    if (serverDescription.setVersion && electionId) {\n        if (maxSetVersion && maxElectionId) {\n            if (maxSetVersion > serverDescription.setVersion ||\n                compareObjectId(maxElectionId, electionId) > 0) {\n                // this primary is stale, we must remove it\n                serverDescriptions.set(serverDescription.address, new server_description_1.ServerDescription(serverDescription.address));\n                return [checkHasPrimary(serverDescriptions), setName, maxSetVersion, maxElectionId];\n            }\n        }\n        maxElectionId = serverDescription.electionId;\n    }\n    if (serverDescription.setVersion != null &&\n        (maxSetVersion == null || serverDescription.setVersion > maxSetVersion)) {\n        maxSetVersion = serverDescription.setVersion;\n    }\n    // We've heard from the primary. Is it the same primary as before?\n    for (const [address, server] of serverDescriptions) {\n        if (server.type === common_1.ServerType.RSPrimary && server.address !== serverDescription.address) {\n            // Reset old primary's type to Unknown.\n            serverDescriptions.set(address, new server_description_1.ServerDescription(server.address));\n            // There can only be one primary\n            break;\n        }\n    }\n    // Discover new hosts from this primary's response.\n    serverDescription.allHosts.forEach((address) => {\n        if (!serverDescriptions.has(address)) {\n            serverDescriptions.set(address, new server_description_1.ServerDescription(address));\n        }\n    });\n    // Remove hosts not in the response.\n    const currentAddresses = Array.from(serverDescriptions.keys());\n    const responseAddresses = serverDescription.allHosts;\n    currentAddresses\n        .filter((addr) => responseAddresses.indexOf(addr) === -1)\n        .forEach((address) => {\n        serverDescriptions.delete(address);\n    });\n    return [checkHasPrimary(serverDescriptions), setName, maxSetVersion, maxElectionId];\n}\nfunction updateRsWithPrimaryFromMember(serverDescriptions, serverDescription, setName) {\n    if (setName == null) {\n        // TODO(NODE-3483): should be an appropriate runtime error\n        throw new error_1.MongoRuntimeError('Argument \"setName\" is required if connected to a replica set');\n    }\n    if (setName !== serverDescription.setName ||\n        (serverDescription.me && serverDescription.address !== serverDescription.me)) {\n        serverDescriptions.delete(serverDescription.address);\n    }\n    return checkHasPrimary(serverDescriptions);\n}\nfunction updateRsNoPrimaryFromMember(serverDescriptions, serverDescription, setName) {\n    const topologyType = common_1.TopologyType.ReplicaSetNoPrimary;\n    setName = setName || serverDescription.setName;\n    if (setName !== serverDescription.setName) {\n        serverDescriptions.delete(serverDescription.address);\n        return [topologyType, setName];\n    }\n    serverDescription.allHosts.forEach((address) => {\n        if (!serverDescriptions.has(address)) {\n            serverDescriptions.set(address, new server_description_1.ServerDescription(address));\n        }\n    });\n    if (serverDescription.me && serverDescription.address !== serverDescription.me) {\n        serverDescriptions.delete(serverDescription.address);\n    }\n    return [topologyType, setName];\n}\nfunction checkHasPrimary(serverDescriptions) {\n    for (const serverDescription of serverDescriptions.values()) {\n        if (serverDescription.type === common_1.ServerType.RSPrimary) {\n            return common_1.TopologyType.ReplicaSetWithPrimary;\n        }\n    }\n    return common_1.TopologyType.ReplicaSetNoPrimary;\n}\n//# sourceMappingURL=topology_description.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.compareTopologyVersion = exports.parseServerType = exports.ServerDescription = void 0;\nconst utils_1 = require(\"../utils\");\nconst common_1 = require(\"./common\");\nconst bson_1 = require(\"../bson\");\nconst WRITABLE_SERVER_TYPES = new Set([\n    common_1.ServerType.RSPrimary,\n    common_1.ServerType.Standalone,\n    common_1.ServerType.Mongos,\n    common_1.ServerType.LoadBalancer\n]);\nconst DATA_BEARING_SERVER_TYPES = new Set([\n    common_1.ServerType.RSPrimary,\n    common_1.ServerType.RSSecondary,\n    common_1.ServerType.Mongos,\n    common_1.ServerType.Standalone,\n    common_1.ServerType.LoadBalancer\n]);\n/**\n * The client's view of a single server, based on the most recent ismaster outcome.\n *\n * Internal type, not meant to be directly instantiated\n * @public\n */\nclass ServerDescription {\n    /**\n     * Create a ServerDescription\n     * @internal\n     *\n     * @param address - The address of the server\n     * @param ismaster - An optional ismaster response for this server\n     */\n    constructor(address, ismaster, options) {\n        var _a, _b, _c, _d, _e, _f, _g, _h, _j, _k, _l, _m;\n        if (typeof address === 'string') {\n            this._hostAddress = new utils_1.HostAddress(address);\n            this.address = this._hostAddress.toString();\n        }\n        else {\n            this._hostAddress = address;\n            this.address = this._hostAddress.toString();\n        }\n        this.type = parseServerType(ismaster, options);\n        this.hosts = (_b = (_a = ismaster === null || ismaster === void 0 ? void 0 : ismaster.hosts) === null || _a === void 0 ? void 0 : _a.map((host) => host.toLowerCase())) !== null && _b !== void 0 ? _b : [];\n        this.passives = (_d = (_c = ismaster === null || ismaster === void 0 ? void 0 : ismaster.passives) === null || _c === void 0 ? void 0 : _c.map((host) => host.toLowerCase())) !== null && _d !== void 0 ? _d : [];\n        this.arbiters = (_f = (_e = ismaster === null || ismaster === void 0 ? void 0 : ismaster.arbiters) === null || _e === void 0 ? void 0 : _e.map((host) => host.toLowerCase())) !== null && _f !== void 0 ? _f : [];\n        this.tags = (_g = ismaster === null || ismaster === void 0 ? void 0 : ismaster.tags) !== null && _g !== void 0 ? _g : {};\n        this.minWireVersion = (_h = ismaster === null || ismaster === void 0 ? void 0 : ismaster.minWireVersion) !== null && _h !== void 0 ? _h : 0;\n        this.maxWireVersion = (_j = ismaster === null || ismaster === void 0 ? void 0 : ismaster.maxWireVersion) !== null && _j !== void 0 ? _j : 0;\n        this.roundTripTime = (_k = options === null || options === void 0 ? void 0 : options.roundTripTime) !== null && _k !== void 0 ? _k : -1;\n        this.lastUpdateTime = (0, utils_1.now)();\n        this.lastWriteDate = (_m = (_l = ismaster === null || ismaster === void 0 ? void 0 : ismaster.lastWrite) === null || _l === void 0 ? void 0 : _l.lastWriteDate) !== null && _m !== void 0 ? _m : 0;\n        if (options === null || options === void 0 ? void 0 : options.topologyVersion) {\n            this.topologyVersion = options.topologyVersion;\n        }\n        else if (ismaster === null || ismaster === void 0 ? void 0 : ismaster.topologyVersion) {\n            this.topologyVersion = ismaster.topologyVersion;\n        }\n        if (options === null || options === void 0 ? void 0 : options.error) {\n            this.error = options.error;\n        }\n        if (ismaster === null || ismaster === void 0 ? void 0 : ismaster.primary) {\n            this.primary = ismaster.primary;\n        }\n        if (ismaster === null || ismaster === void 0 ? void 0 : ismaster.me) {\n            this.me = ismaster.me.toLowerCase();\n        }\n        if (ismaster === null || ismaster === void 0 ? void 0 : ismaster.setName) {\n            this.setName = ismaster.setName;\n        }\n        if (ismaster === null || ismaster === void 0 ? void 0 : ismaster.setVersion) {\n            this.setVersion = ismaster.setVersion;\n        }\n        if (ismaster === null || ismaster === void 0 ? void 0 : ismaster.electionId) {\n            this.electionId = ismaster.electionId;\n        }\n        if (ismaster === null || ismaster === void 0 ? void 0 : ismaster.logicalSessionTimeoutMinutes) {\n            this.logicalSessionTimeoutMinutes = ismaster.logicalSessionTimeoutMinutes;\n        }\n        if (ismaster === null || ismaster === void 0 ? void 0 : ismaster.$clusterTime) {\n            this.$clusterTime = ismaster.$clusterTime;\n        }\n    }\n    get hostAddress() {\n        if (this._hostAddress)\n            return this._hostAddress;\n        else\n            return new utils_1.HostAddress(this.address);\n    }\n    get allHosts() {\n        return this.hosts.concat(this.arbiters).concat(this.passives);\n    }\n    /** Is this server available for reads*/\n    get isReadable() {\n        return this.type === common_1.ServerType.RSSecondary || this.isWritable;\n    }\n    /** Is this server data bearing */\n    get isDataBearing() {\n        return DATA_BEARING_SERVER_TYPES.has(this.type);\n    }\n    /** Is this server available for writes */\n    get isWritable() {\n        return WRITABLE_SERVER_TYPES.has(this.type);\n    }\n    get host() {\n        const chopLength = `:${this.port}`.length;\n        return this.address.slice(0, -chopLength);\n    }\n    get port() {\n        const port = this.address.split(':').pop();\n        return port ? Number.parseInt(port, 10) : 27017;\n    }\n    /**\n     * Determines if another `ServerDescription` is equal to this one per the rules defined\n     * in the {@link https://github.com/mongodb/specifications/blob/master/source/server-discovery-and-monitoring/server-discovery-and-monitoring.rst#serverdescription|SDAM spec}\n     */\n    equals(other) {\n        const topologyVersionsEqual = this.topologyVersion === other.topologyVersion ||\n            compareTopologyVersion(this.topologyVersion, other.topologyVersion) === 0;\n        const electionIdsEqual = this.electionId && other.electionId\n            ? other.electionId && this.electionId.equals(other.electionId)\n            : this.electionId === other.electionId;\n        return (other != null &&\n            (0, utils_1.errorStrictEqual)(this.error, other.error) &&\n            this.type === other.type &&\n            this.minWireVersion === other.minWireVersion &&\n            (0, utils_1.arrayStrictEqual)(this.hosts, other.hosts) &&\n            tagsStrictEqual(this.tags, other.tags) &&\n            this.setName === other.setName &&\n            this.setVersion === other.setVersion &&\n            electionIdsEqual &&\n            this.primary === other.primary &&\n            this.logicalSessionTimeoutMinutes === other.logicalSessionTimeoutMinutes &&\n            topologyVersionsEqual);\n    }\n}\nexports.ServerDescription = ServerDescription;\n// Parses an `ismaster` message and determines the server type\nfunction parseServerType(ismaster, options) {\n    if (options === null || options === void 0 ? void 0 : options.loadBalanced) {\n        return common_1.ServerType.LoadBalancer;\n    }\n    if (!ismaster || !ismaster.ok) {\n        return common_1.ServerType.Unknown;\n    }\n    if (ismaster.isreplicaset) {\n        return common_1.ServerType.RSGhost;\n    }\n    if (ismaster.msg && ismaster.msg === 'isdbgrid') {\n        return common_1.ServerType.Mongos;\n    }\n    if (ismaster.setName) {\n        if (ismaster.hidden) {\n            return common_1.ServerType.RSOther;\n        }\n        else if (ismaster.ismaster || ismaster.isWritablePrimary) {\n            return common_1.ServerType.RSPrimary;\n        }\n        else if (ismaster.secondary) {\n            return common_1.ServerType.RSSecondary;\n        }\n        else if (ismaster.arbiterOnly) {\n            return common_1.ServerType.RSArbiter;\n        }\n        else {\n            return common_1.ServerType.RSOther;\n        }\n    }\n    return common_1.ServerType.Standalone;\n}\nexports.parseServerType = parseServerType;\nfunction tagsStrictEqual(tags, tags2) {\n    const tagsKeys = Object.keys(tags);\n    const tags2Keys = Object.keys(tags2);\n    return (tagsKeys.length === tags2Keys.length &&\n        tagsKeys.every((key) => tags2[key] === tags[key]));\n}\n/**\n * Compares two topology versions.\n *\n * @returns A negative number if `lhs` is older than `rhs`; positive if `lhs` is newer than `rhs`; 0 if they are equivalent.\n */\nfunction compareTopologyVersion(lhs, rhs) {\n    if (lhs == null || rhs == null) {\n        return -1;\n    }\n    if (lhs.processId.equals(rhs.processId)) {\n        // tests mock counter as just number, but in a real situation counter should always be a Long\n        const lhsCounter = bson_1.Long.isLong(lhs.counter) ? lhs.counter : bson_1.Long.fromNumber(lhs.counter);\n        const rhsCounter = bson_1.Long.isLong(rhs.counter) ? lhs.counter : bson_1.Long.fromNumber(rhs.counter);\n        return lhsCounter.compare(rhsCounter);\n    }\n    return -1;\n}\nexports.compareTopologyVersion = compareTopologyVersion;\n//# sourceMappingURL=server_description.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.executeOperation = void 0;\nconst read_preference_1 = require(\"../read_preference\");\nconst error_1 = require(\"../error\");\nconst operation_1 = require(\"./operation\");\nconst utils_1 = require(\"../utils\");\nconst utils_2 = require(\"../utils\");\nconst server_selection_1 = require(\"../sdam/server_selection\");\nconst MMAPv1_RETRY_WRITES_ERROR_CODE = error_1.MONGODB_ERROR_CODES.IllegalOperation;\nconst MMAPv1_RETRY_WRITES_ERROR_MESSAGE = 'This MongoDB deployment does not support retryable writes. Please add retryWrites=false to your connection string.';\nfunction executeOperation(topology, operation, callback) {\n    if (!(operation instanceof operation_1.AbstractOperation)) {\n        // TODO(NODE-3483)\n        throw new error_1.MongoRuntimeError('This method requires a valid operation instance');\n    }\n    return (0, utils_1.maybePromise)(callback, cb => {\n        if (topology.shouldCheckForSessionSupport()) {\n            return topology.selectServer(read_preference_1.ReadPreference.primaryPreferred, err => {\n                if (err)\n                    return cb(err);\n                executeOperation(topology, operation, cb);\n            });\n        }\n        // The driver sessions spec mandates that we implicitly create sessions for operations\n        // that are not explicitly provided with a session.\n        let session = operation.session;\n        let owner;\n        if (topology.hasSessionSupport()) {\n            if (session == null) {\n                owner = Symbol();\n                session = topology.startSession({ owner, explicit: false });\n            }\n            else if (session.hasEnded) {\n                return cb(new error_1.MongoExpiredSessionError('Use of expired sessions is not permitted'));\n            }\n            else if (session.snapshotEnabled && !topology.capabilities.supportsSnapshotReads) {\n                return cb(new error_1.MongoCompatibilityError('Snapshot reads require MongoDB 5.0 or later'));\n            }\n        }\n        else if (session) {\n            // If the user passed an explicit session and we are still, after server selection,\n            // trying to run against a topology that doesn't support sessions we error out.\n            return cb(new error_1.MongoCompatibilityError('Current topology does not support sessions'));\n        }\n        try {\n            executeWithServerSelection(topology, session, operation, (err, result) => {\n                if (session && session.owner && session.owner === owner) {\n                    return session.endSession(err2 => cb(err2 || err, result));\n                }\n                cb(err, result);\n            });\n        }\n        catch (e) {\n            if (session && session.owner && session.owner === owner) {\n                session.endSession();\n            }\n            throw e;\n        }\n    });\n}\nexports.executeOperation = executeOperation;\nfunction supportsRetryableReads(server) {\n    return (0, utils_1.maxWireVersion)(server) >= 6;\n}\nfunction executeWithServerSelection(topology, session, operation, callback) {\n    var _a;\n    const readPreference = operation.readPreference || read_preference_1.ReadPreference.primary;\n    const inTransaction = session && session.inTransaction();\n    if (inTransaction && !readPreference.equals(read_preference_1.ReadPreference.primary)) {\n        callback(new error_1.MongoTransactionError(`Read preference in a transaction must be primary, not: ${readPreference.mode}`));\n        return;\n    }\n    if (session &&\n        session.isPinned &&\n        session.transaction.isCommitted &&\n        !operation.bypassPinningCheck) {\n        session.unpin();\n    }\n    let selector;\n    if (operation.hasAspect(operation_1.Aspect.CURSOR_ITERATING)) {\n        // Get more operations must always select the same server, but run through\n        // server selection to potentially force monitor checks if the server is\n        // in an unknown state.\n        selector = (0, server_selection_1.sameServerSelector)((_a = operation.server) === null || _a === void 0 ? void 0 : _a.description);\n    }\n    else if (operation.trySecondaryWrite) {\n        // If operation should try to write to secondary use the custom server selector\n        // otherwise provide the read preference.\n        selector = (0, server_selection_1.secondaryWritableServerSelector)(topology.commonWireVersion, readPreference);\n    }\n    else {\n        selector = readPreference;\n    }\n    const serverSelectionOptions = { session };\n    function callbackWithRetry(err, result) {\n        if (err == null) {\n            return callback(undefined, result);\n        }\n        const hasReadAspect = operation.hasAspect(operation_1.Aspect.READ_OPERATION);\n        const hasWriteAspect = operation.hasAspect(operation_1.Aspect.WRITE_OPERATION);\n        const itShouldRetryWrite = shouldRetryWrite(err);\n        if ((hasReadAspect && !(0, error_1.isRetryableError)(err)) || (hasWriteAspect && !itShouldRetryWrite)) {\n            return callback(err);\n        }\n        if (hasWriteAspect &&\n            itShouldRetryWrite &&\n            err.code === MMAPv1_RETRY_WRITES_ERROR_CODE &&\n            err.errmsg.match(/Transaction numbers/)) {\n            callback(new error_1.MongoServerError({\n                message: MMAPv1_RETRY_WRITES_ERROR_MESSAGE,\n                errmsg: MMAPv1_RETRY_WRITES_ERROR_MESSAGE,\n                originalError: err\n            }));\n            return;\n        }\n        // select a new server, and attempt to retry the operation\n        topology.selectServer(selector, serverSelectionOptions, (e, server) => {\n            if (e ||\n                (operation.hasAspect(operation_1.Aspect.READ_OPERATION) && !supportsRetryableReads(server)) ||\n                (operation.hasAspect(operation_1.Aspect.WRITE_OPERATION) && !(0, utils_2.supportsRetryableWrites)(server))) {\n                callback(e);\n                return;\n            }\n            // If we have a cursor and the initial command fails with a network error,\n            // we can retry it on another connection. So we need to check it back in, clear the\n            // pool for the service id, and retry again.\n            if (err &&\n                err instanceof error_1.MongoNetworkError &&\n                server.loadBalanced &&\n                session &&\n                session.isPinned &&\n                !session.inTransaction() &&\n                operation.hasAspect(operation_1.Aspect.CURSOR_CREATING)) {\n                session.unpin({ force: true, forceClear: true });\n            }\n            operation.execute(server, session, callback);\n        });\n    }\n    if (readPreference &&\n        !readPreference.equals(read_preference_1.ReadPreference.primary) &&\n        session &&\n        session.inTransaction()) {\n        callback(new error_1.MongoTransactionError(`Read preference in a transaction must be primary, not: ${readPreference.mode}`));\n        return;\n    }\n    // select a server, and execute the operation against it\n    topology.selectServer(selector, serverSelectionOptions, (err, server) => {\n        if (err) {\n            callback(err);\n            return;\n        }\n        if (session && operation.hasAspect(operation_1.Aspect.RETRYABLE)) {\n            const willRetryRead = topology.s.options.retryReads !== false &&\n                !inTransaction &&\n                supportsRetryableReads(server) &&\n                operation.canRetryRead;\n            const willRetryWrite = topology.s.options.retryWrites === true &&\n                !inTransaction &&\n                (0, utils_2.supportsRetryableWrites)(server) &&\n                operation.canRetryWrite;\n            const hasReadAspect = operation.hasAspect(operation_1.Aspect.READ_OPERATION);\n            const hasWriteAspect = operation.hasAspect(operation_1.Aspect.WRITE_OPERATION);\n            if ((hasReadAspect && willRetryRead) || (hasWriteAspect && willRetryWrite)) {\n                if (hasWriteAspect && willRetryWrite) {\n                    operation.options.willRetryWrite = true;\n                    session.incrementTransactionNumber();\n                }\n                operation.execute(server, session, callbackWithRetry);\n                return;\n            }\n        }\n        operation.execute(server, session, callback);\n    });\n}\nfunction shouldRetryWrite(err) {\n    return err instanceof error_1.MongoError && err.hasErrorLabel('RetryableWriteError');\n}\n//# sourceMappingURL=execute_operation.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.defineAspects = exports.AbstractOperation = exports.Aspect = void 0;\nconst read_preference_1 = require(\"../read_preference\");\nconst bson_1 = require(\"../bson\");\nexports.Aspect = {\n    READ_OPERATION: Symbol('READ_OPERATION'),\n    WRITE_OPERATION: Symbol('WRITE_OPERATION'),\n    RETRYABLE: Symbol('RETRYABLE'),\n    EXPLAINABLE: Symbol('EXPLAINABLE'),\n    SKIP_COLLATION: Symbol('SKIP_COLLATION'),\n    CURSOR_CREATING: Symbol('CURSOR_CREATING'),\n    CURSOR_ITERATING: Symbol('CURSOR_ITERATING')\n};\n/** @internal */\nconst kSession = Symbol('session');\n/**\n * This class acts as a parent class for any operation and is responsible for setting this.options,\n * as well as setting and getting a session.\n * Additionally, this class implements `hasAspect`, which determines whether an operation has\n * a specific aspect.\n * @internal\n */\nclass AbstractOperation {\n    constructor(options = {}) {\n        var _a;\n        this.readPreference = this.hasAspect(exports.Aspect.WRITE_OPERATION)\n            ? read_preference_1.ReadPreference.primary\n            : (_a = read_preference_1.ReadPreference.fromOptions(options)) !== null && _a !== void 0 ? _a : read_preference_1.ReadPreference.primary;\n        // Pull the BSON serialize options from the already-resolved options\n        this.bsonOptions = (0, bson_1.resolveBSONOptions)(options);\n        if (options.session) {\n            this[kSession] = options.session;\n        }\n        this.options = options;\n        this.bypassPinningCheck = !!options.bypassPinningCheck;\n        this.trySecondaryWrite = false;\n    }\n    hasAspect(aspect) {\n        const ctor = this.constructor;\n        if (ctor.aspects == null) {\n            return false;\n        }\n        return ctor.aspects.has(aspect);\n    }\n    get session() {\n        return this[kSession];\n    }\n    get canRetryRead() {\n        return true;\n    }\n    get canRetryWrite() {\n        return true;\n    }\n}\nexports.AbstractOperation = AbstractOperation;\nfunction defineAspects(operation, aspects) {\n    if (!Array.isArray(aspects) && !(aspects instanceof Set)) {\n        aspects = [aspects];\n    }\n    aspects = new Set(aspects);\n    Object.defineProperty(operation, 'aspects', {\n        value: aspects,\n        writable: false\n    });\n    return aspects;\n}\nexports.defineAspects = defineAspects;\n//# sourceMappingURL=operation.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.readPreferenceServerSelector = exports.secondaryWritableServerSelector = exports.sameServerSelector = exports.writableServerSelector = exports.MIN_SECONDARY_WRITE_WIRE_VERSION = void 0;\nconst common_1 = require(\"./common\");\nconst read_preference_1 = require(\"../read_preference\");\nconst error_1 = require(\"../error\");\n// max staleness constants\nconst IDLE_WRITE_PERIOD = 10000;\nconst SMALLEST_MAX_STALENESS_SECONDS = 90;\n//  Minimum version to try writes on secondaries.\nexports.MIN_SECONDARY_WRITE_WIRE_VERSION = 13;\n/**\n * Returns a server selector that selects for writable servers\n */\nfunction writableServerSelector() {\n    return (topologyDescription, servers) => latencyWindowReducer(topologyDescription, servers.filter((s) => s.isWritable));\n}\nexports.writableServerSelector = writableServerSelector;\n/**\n * The purpose of this selector is to select the same server, only\n * if it is in a state that it can have commands sent to it.\n */\nfunction sameServerSelector(description) {\n    return (topologyDescription, servers) => {\n        if (!description)\n            return [];\n        // Filter the servers to match the provided description only if\n        // the type is not unknown.\n        return servers.filter(sd => {\n            return sd.address === description.address && sd.type !== common_1.ServerType.Unknown;\n        });\n    };\n}\nexports.sameServerSelector = sameServerSelector;\n/**\n * Returns a server selector that uses a read preference to select a\n * server potentially for a write on a secondary.\n */\nfunction secondaryWritableServerSelector(wireVersion, readPreference) {\n    // If server version < 5.0, read preference always primary.\n    // If server version >= 5.0...\n    // - If read preference is supplied, use that.\n    // - If no read preference is supplied, use primary.\n    if (!readPreference ||\n        !wireVersion ||\n        (wireVersion && wireVersion < exports.MIN_SECONDARY_WRITE_WIRE_VERSION)) {\n        return readPreferenceServerSelector(read_preference_1.ReadPreference.primary);\n    }\n    return readPreferenceServerSelector(readPreference);\n}\nexports.secondaryWritableServerSelector = secondaryWritableServerSelector;\n/**\n * Reduces the passed in array of servers by the rules of the \"Max Staleness\" specification\n * found here: https://github.com/mongodb/specifications/blob/master/source/max-staleness/max-staleness.rst\n *\n * @param readPreference - The read preference providing max staleness guidance\n * @param topologyDescription - The topology description\n * @param servers - The list of server descriptions to be reduced\n * @returns The list of servers that satisfy the requirements of max staleness\n */\nfunction maxStalenessReducer(readPreference, topologyDescription, servers) {\n    if (readPreference.maxStalenessSeconds == null || readPreference.maxStalenessSeconds < 0) {\n        return servers;\n    }\n    const maxStaleness = readPreference.maxStalenessSeconds;\n    const maxStalenessVariance = (topologyDescription.heartbeatFrequencyMS + IDLE_WRITE_PERIOD) / 1000;\n    if (maxStaleness < maxStalenessVariance) {\n        throw new error_1.MongoInvalidArgumentError(`Option \"maxStalenessSeconds\" must be at least ${maxStalenessVariance} seconds`);\n    }\n    if (maxStaleness < SMALLEST_MAX_STALENESS_SECONDS) {\n        throw new error_1.MongoInvalidArgumentError(`Option \"maxStalenessSeconds\" must be at least ${SMALLEST_MAX_STALENESS_SECONDS} seconds`);\n    }\n    if (topologyDescription.type === common_1.TopologyType.ReplicaSetWithPrimary) {\n        const primary = Array.from(topologyDescription.servers.values()).filter(primaryFilter)[0];\n        return servers.reduce((result, server) => {\n            var _a;\n            const stalenessMS = server.lastUpdateTime -\n                server.lastWriteDate -\n                (primary.lastUpdateTime - primary.lastWriteDate) +\n                topologyDescription.heartbeatFrequencyMS;\n            const staleness = stalenessMS / 1000;\n            const maxStalenessSeconds = (_a = readPreference.maxStalenessSeconds) !== null && _a !== void 0 ? _a : 0;\n            if (staleness <= maxStalenessSeconds) {\n                result.push(server);\n            }\n            return result;\n        }, []);\n    }\n    if (topologyDescription.type === common_1.TopologyType.ReplicaSetNoPrimary) {\n        if (servers.length === 0) {\n            return servers;\n        }\n        const sMax = servers.reduce((max, s) => s.lastWriteDate > max.lastWriteDate ? s : max);\n        return servers.reduce((result, server) => {\n            var _a;\n            const stalenessMS = sMax.lastWriteDate - server.lastWriteDate + topologyDescription.heartbeatFrequencyMS;\n            const staleness = stalenessMS / 1000;\n            const maxStalenessSeconds = (_a = readPreference.maxStalenessSeconds) !== null && _a !== void 0 ? _a : 0;\n            if (staleness <= maxStalenessSeconds) {\n                result.push(server);\n            }\n            return result;\n        }, []);\n    }\n    return servers;\n}\n/**\n * Determines whether a server's tags match a given set of tags\n *\n * @param tagSet - The requested tag set to match\n * @param serverTags - The server's tags\n */\nfunction tagSetMatch(tagSet, serverTags) {\n    const keys = Object.keys(tagSet);\n    const serverTagKeys = Object.keys(serverTags);\n    for (let i = 0; i < keys.length; ++i) {\n        const key = keys[i];\n        if (serverTagKeys.indexOf(key) === -1 || serverTags[key] !== tagSet[key]) {\n            return false;\n        }\n    }\n    return true;\n}\n/**\n * Reduces a set of server descriptions based on tags requested by the read preference\n *\n * @param readPreference - The read preference providing the requested tags\n * @param servers - The list of server descriptions to reduce\n * @returns The list of servers matching the requested tags\n */\nfunction tagSetReducer(readPreference, servers) {\n    if (readPreference.tags == null ||\n        (Array.isArray(readPreference.tags) && readPreference.tags.length === 0)) {\n        return servers;\n    }\n    for (let i = 0; i < readPreference.tags.length; ++i) {\n        const tagSet = readPreference.tags[i];\n        const serversMatchingTagset = servers.reduce((matched, server) => {\n            if (tagSetMatch(tagSet, server.tags))\n                matched.push(server);\n            return matched;\n        }, []);\n        if (serversMatchingTagset.length) {\n            return serversMatchingTagset;\n        }\n    }\n    return [];\n}\n/**\n * Reduces a list of servers to ensure they fall within an acceptable latency window. This is\n * further specified in the \"Server Selection\" specification, found here:\n * https://github.com/mongodb/specifications/blob/master/source/server-selection/server-selection.rst\n *\n * @param topologyDescription - The topology description\n * @param servers - The list of servers to reduce\n * @returns The servers which fall within an acceptable latency window\n */\nfunction latencyWindowReducer(topologyDescription, servers) {\n    const low = servers.reduce((min, server) => min === -1 ? server.roundTripTime : Math.min(server.roundTripTime, min), -1);\n    const high = low + topologyDescription.localThresholdMS;\n    return servers.reduce((result, server) => {\n        if (server.roundTripTime <= high && server.roundTripTime >= low)\n            result.push(server);\n        return result;\n    }, []);\n}\n// filters\nfunction primaryFilter(server) {\n    return server.type === common_1.ServerType.RSPrimary;\n}\nfunction secondaryFilter(server) {\n    return server.type === common_1.ServerType.RSSecondary;\n}\nfunction nearestFilter(server) {\n    return server.type === common_1.ServerType.RSSecondary || server.type === common_1.ServerType.RSPrimary;\n}\nfunction knownFilter(server) {\n    return server.type !== common_1.ServerType.Unknown;\n}\nfunction loadBalancerFilter(server) {\n    return server.type === common_1.ServerType.LoadBalancer;\n}\n/**\n * Returns a function which selects servers based on a provided read preference\n *\n * @param readPreference - The read preference to select with\n */\nfunction readPreferenceServerSelector(readPreference) {\n    if (!readPreference.isValid()) {\n        throw new error_1.MongoInvalidArgumentError('Invalid read preference specified');\n    }\n    return (topologyDescription, servers) => {\n        const commonWireVersion = topologyDescription.commonWireVersion;\n        if (commonWireVersion &&\n            readPreference.minWireVersion &&\n            readPreference.minWireVersion > commonWireVersion) {\n            throw new error_1.MongoCompatibilityError(`Minimum wire version '${readPreference.minWireVersion}' required, but found '${commonWireVersion}'`);\n        }\n        if (topologyDescription.type === common_1.TopologyType.LoadBalanced) {\n            return servers.filter(loadBalancerFilter);\n        }\n        if (topologyDescription.type === common_1.TopologyType.Unknown) {\n            return [];\n        }\n        if (topologyDescription.type === common_1.TopologyType.Single ||\n            topologyDescription.type === common_1.TopologyType.Sharded) {\n            return latencyWindowReducer(topologyDescription, servers.filter(knownFilter));\n        }\n        const mode = readPreference.mode;\n        if (mode === read_preference_1.ReadPreference.PRIMARY) {\n            return servers.filter(primaryFilter);\n        }\n        if (mode === read_preference_1.ReadPreference.PRIMARY_PREFERRED) {\n            const result = servers.filter(primaryFilter);\n            if (result.length) {\n                return result;\n            }\n        }\n        const filter = mode === read_preference_1.ReadPreference.NEAREST ? nearestFilter : secondaryFilter;\n        const selectedServers = latencyWindowReducer(topologyDescription, tagSetReducer(readPreference, maxStalenessReducer(readPreference, topologyDescription, servers.filter(filter))));\n        if (mode === read_preference_1.ReadPreference.SECONDARY_PREFERRED && selectedServers.length === 0) {\n            return servers.filter(primaryFilter);\n        }\n        return selectedServers;\n    };\n}\nexports.readPreferenceServerSelector = readPreferenceServerSelector;\n//# sourceMappingURL=server_selection.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.RunAdminCommandOperation = exports.RunCommandOperation = void 0;\nconst command_1 = require(\"./command\");\nconst utils_1 = require(\"../utils\");\n/** @internal */\nclass RunCommandOperation extends command_1.CommandOperation {\n    constructor(parent, command, options) {\n        super(parent, options);\n        this.options = options !== null && options !== void 0 ? options : {};\n        this.command = command;\n    }\n    execute(server, session, callback) {\n        const command = this.command;\n        this.executeCommand(server, session, command, callback);\n    }\n}\nexports.RunCommandOperation = RunCommandOperation;\nclass RunAdminCommandOperation extends RunCommandOperation {\n    constructor(parent, command, options) {\n        super(parent, command, options);\n        this.ns = new utils_1.MongoDBNamespace('admin');\n    }\n}\nexports.RunAdminCommandOperation = RunAdminCommandOperation;\n//# sourceMappingURL=run_command.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.CommandOperation = void 0;\nconst operation_1 = require(\"./operation\");\nconst read_concern_1 = require(\"../read_concern\");\nconst write_concern_1 = require(\"../write_concern\");\nconst utils_1 = require(\"../utils\");\nconst sessions_1 = require(\"../sessions\");\nconst error_1 = require(\"../error\");\nconst explain_1 = require(\"../explain\");\nconst server_selection_1 = require(\"../sdam/server_selection\");\nconst SUPPORTS_WRITE_CONCERN_AND_COLLATION = 5;\n/** @internal */\nclass CommandOperation extends operation_1.AbstractOperation {\n    constructor(parent, options) {\n        super(options);\n        this.options = options !== null && options !== void 0 ? options : {};\n        // NOTE: this was explicitly added for the add/remove user operations, it's likely\n        //       something we'd want to reconsider. Perhaps those commands can use `Admin`\n        //       as a parent?\n        const dbNameOverride = (options === null || options === void 0 ? void 0 : options.dbName) || (options === null || options === void 0 ? void 0 : options.authdb);\n        if (dbNameOverride) {\n            this.ns = new utils_1.MongoDBNamespace(dbNameOverride, '$cmd');\n        }\n        else {\n            this.ns = parent\n                ? parent.s.namespace.withCollection('$cmd')\n                : new utils_1.MongoDBNamespace('admin', '$cmd');\n        }\n        this.readConcern = read_concern_1.ReadConcern.fromOptions(options);\n        this.writeConcern = write_concern_1.WriteConcern.fromOptions(options);\n        // TODO(NODE-2056): make logger another \"inheritable\" property\n        if (parent && parent.logger) {\n            this.logger = parent.logger;\n        }\n        if (this.hasAspect(operation_1.Aspect.EXPLAINABLE)) {\n            this.explain = explain_1.Explain.fromOptions(options);\n        }\n        else if ((options === null || options === void 0 ? void 0 : options.explain) != null) {\n            throw new error_1.MongoInvalidArgumentError(`Option \"explain\" is not supported on this command`);\n        }\n    }\n    get canRetryWrite() {\n        if (this.hasAspect(operation_1.Aspect.EXPLAINABLE)) {\n            return this.explain == null;\n        }\n        return true;\n    }\n    executeCommand(server, session, cmd, callback) {\n        // TODO: consider making this a non-enumerable property\n        this.server = server;\n        const options = {\n            ...this.options,\n            ...this.bsonOptions,\n            readPreference: this.readPreference,\n            session\n        };\n        const serverWireVersion = (0, utils_1.maxWireVersion)(server);\n        const inTransaction = this.session && this.session.inTransaction();\n        if (this.readConcern && (0, sessions_1.commandSupportsReadConcern)(cmd) && !inTransaction) {\n            Object.assign(cmd, { readConcern: this.readConcern });\n        }\n        if (this.trySecondaryWrite && serverWireVersion < server_selection_1.MIN_SECONDARY_WRITE_WIRE_VERSION) {\n            options.omitReadPreference = true;\n        }\n        if (options.collation && serverWireVersion < SUPPORTS_WRITE_CONCERN_AND_COLLATION) {\n            callback(new error_1.MongoCompatibilityError(`Server ${server.name}, which reports wire version ${serverWireVersion}, does not support collation`));\n            return;\n        }\n        if (this.writeConcern && this.hasAspect(operation_1.Aspect.WRITE_OPERATION) && !inTransaction) {\n            Object.assign(cmd, { writeConcern: this.writeConcern });\n        }\n        if (serverWireVersion >= SUPPORTS_WRITE_CONCERN_AND_COLLATION) {\n            if (options.collation &&\n                typeof options.collation === 'object' &&\n                !this.hasAspect(operation_1.Aspect.SKIP_COLLATION)) {\n                Object.assign(cmd, { collation: options.collation });\n            }\n        }\n        if (typeof options.maxTimeMS === 'number') {\n            cmd.maxTimeMS = options.maxTimeMS;\n        }\n        if (typeof options.comment === 'string') {\n            cmd.comment = options.comment;\n        }\n        if (this.hasAspect(operation_1.Aspect.EXPLAINABLE) && this.explain) {\n            if (serverWireVersion < 6 && cmd.aggregate) {\n                // Prior to 3.6, with aggregate, verbosity is ignored, and we must pass in \"explain: true\"\n                cmd.explain = true;\n            }\n            else {\n                cmd = (0, utils_1.decorateWithExplain)(cmd, this.explain);\n            }\n        }\n        server.command(this.ns, cmd, options, callback);\n    }\n}\nexports.CommandOperation = CommandOperation;\n//# sourceMappingURL=command.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.Explain = exports.ExplainVerbosity = void 0;\nconst error_1 = require(\"./error\");\n/** @public */\nexports.ExplainVerbosity = Object.freeze({\n    queryPlanner: 'queryPlanner',\n    queryPlannerExtended: 'queryPlannerExtended',\n    executionStats: 'executionStats',\n    allPlansExecution: 'allPlansExecution'\n});\n/** @internal */\nclass Explain {\n    constructor(verbosity) {\n        if (typeof verbosity === 'boolean') {\n            this.verbosity = verbosity\n                ? exports.ExplainVerbosity.allPlansExecution\n                : exports.ExplainVerbosity.queryPlanner;\n        }\n        else {\n            this.verbosity = verbosity;\n        }\n    }\n    static fromOptions(options) {\n        if ((options === null || options === void 0 ? void 0 : options.explain) == null)\n            return;\n        const explain = options.explain;\n        if (typeof explain === 'boolean' || typeof explain === 'string') {\n            return new Explain(explain);\n        }\n        throw new error_1.MongoInvalidArgumentError('Field \"explain\" must be a string or a boolean');\n    }\n}\nexports.Explain = Explain;\n//# sourceMappingURL=explain.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.hasSessionSupport = exports.CryptoConnection = exports.APM_EVENTS = exports.Connection = void 0;\nconst message_stream_1 = require(\"./message_stream\");\nconst stream_description_1 = require(\"./stream_description\");\nconst command_monitoring_events_1 = require(\"./command_monitoring_events\");\nconst sessions_1 = require(\"../sessions\");\nconst utils_1 = require(\"../utils\");\nconst error_1 = require(\"../error\");\nconst commands_1 = require(\"./commands\");\nconst bson_1 = require(\"../bson\");\nconst shared_1 = require(\"./wire_protocol/shared\");\nconst read_preference_1 = require(\"../read_preference\");\nconst mongo_types_1 = require(\"../mongo_types\");\n/** @internal */\nconst kStream = Symbol('stream');\n/** @internal */\nconst kQueue = Symbol('queue');\n/** @internal */\nconst kMessageStream = Symbol('messageStream');\n/** @internal */\nconst kGeneration = Symbol('generation');\n/** @internal */\nconst kLastUseTime = Symbol('lastUseTime');\n/** @internal */\nconst kClusterTime = Symbol('clusterTime');\n/** @internal */\nconst kDescription = Symbol('description');\n/** @internal */\nconst kIsMaster = Symbol('ismaster');\n/** @internal */\nconst kAutoEncrypter = Symbol('autoEncrypter');\n/** @internal */\nconst kFullResult = Symbol('fullResult');\n/** @internal */\nclass Connection extends mongo_types_1.TypedEventEmitter {\n    constructor(stream, options) {\n        var _a, _b;\n        super();\n        this.id = options.id;\n        this.address = streamIdentifier(stream);\n        this.socketTimeoutMS = (_a = options.socketTimeoutMS) !== null && _a !== void 0 ? _a : 0;\n        this.monitorCommands = options.monitorCommands;\n        this.serverApi = options.serverApi;\n        this.closed = false;\n        this.destroyed = false;\n        this[kDescription] = new stream_description_1.StreamDescription(this.address, options);\n        this[kGeneration] = options.generation;\n        this[kLastUseTime] = (0, utils_1.now)();\n        // setup parser stream and message handling\n        this[kQueue] = new Map();\n        this[kMessageStream] = new message_stream_1.MessageStream({\n            ...options,\n            maxBsonMessageSize: (_b = this.ismaster) === null || _b === void 0 ? void 0 : _b.maxBsonMessageSize\n        });\n        this[kMessageStream].on('message', messageHandler(this));\n        this[kStream] = stream;\n        stream.on('error', () => {\n            /* ignore errors, listen to `close` instead */\n        });\n        this[kMessageStream].on('error', error => this.handleIssue({ destroy: error }));\n        stream.on('close', () => this.handleIssue({ isClose: true }));\n        stream.on('timeout', () => this.handleIssue({ isTimeout: true, destroy: true }));\n        // hook the message stream up to the passed in stream\n        stream.pipe(this[kMessageStream]);\n        this[kMessageStream].pipe(stream);\n    }\n    get description() {\n        return this[kDescription];\n    }\n    get ismaster() {\n        return this[kIsMaster];\n    }\n    // the `connect` method stores the result of the handshake ismaster on the connection\n    set ismaster(response) {\n        this[kDescription].receiveResponse(response);\n        this[kDescription] = Object.freeze(this[kDescription]);\n        // TODO: remove this, and only use the `StreamDescription` in the future\n        this[kIsMaster] = response;\n    }\n    get serviceId() {\n        var _a;\n        return (_a = this.ismaster) === null || _a === void 0 ? void 0 : _a.serviceId;\n    }\n    get loadBalanced() {\n        return this.description.loadBalanced;\n    }\n    get generation() {\n        return this[kGeneration] || 0;\n    }\n    set generation(generation) {\n        this[kGeneration] = generation;\n    }\n    get idleTime() {\n        return (0, utils_1.calculateDurationInMs)(this[kLastUseTime]);\n    }\n    get clusterTime() {\n        return this[kClusterTime];\n    }\n    get stream() {\n        return this[kStream];\n    }\n    markAvailable() {\n        this[kLastUseTime] = (0, utils_1.now)();\n    }\n    handleIssue(issue) {\n        if (this.closed) {\n            return;\n        }\n        if (issue.destroy) {\n            this[kStream].destroy(typeof issue.destroy === 'boolean' ? undefined : issue.destroy);\n        }\n        this.closed = true;\n        for (const [, op] of this[kQueue]) {\n            if (issue.isTimeout) {\n                op.cb(new error_1.MongoNetworkTimeoutError(`connection ${this.id} to ${this.address} timed out`, {\n                    beforeHandshake: this.ismaster == null\n                }));\n            }\n            else if (issue.isClose) {\n                op.cb(new error_1.MongoNetworkError(`connection ${this.id} to ${this.address} closed`));\n            }\n            else {\n                op.cb(typeof issue.destroy === 'boolean' ? undefined : issue.destroy);\n            }\n        }\n        this[kQueue].clear();\n        this.emit(Connection.CLOSE);\n    }\n    destroy(options, callback) {\n        if (typeof options === 'function') {\n            callback = options;\n            options = { force: false };\n        }\n        this.removeAllListeners(Connection.PINNED);\n        this.removeAllListeners(Connection.UNPINNED);\n        options = Object.assign({ force: false }, options);\n        if (this[kStream] == null || this.destroyed) {\n            this.destroyed = true;\n            if (typeof callback === 'function') {\n                callback();\n            }\n            return;\n        }\n        if (options.force) {\n            this[kStream].destroy();\n            this.destroyed = true;\n            if (typeof callback === 'function') {\n                callback();\n            }\n            return;\n        }\n        this[kStream].end(() => {\n            this.destroyed = true;\n            if (typeof callback === 'function') {\n                callback();\n            }\n        });\n    }\n    /** @internal */\n    command(ns, cmd, options, callback) {\n        if (!(ns instanceof utils_1.MongoDBNamespace)) {\n            // TODO(NODE-3483): Replace this with a MongoCommandError\n            throw new error_1.MongoRuntimeError('Must provide a MongoDBNamespace instance');\n        }\n        const readPreference = (0, shared_1.getReadPreference)(cmd, options);\n        const shouldUseOpMsg = supportsOpMsg(this);\n        const session = options === null || options === void 0 ? void 0 : options.session;\n        let clusterTime = this.clusterTime;\n        let finalCmd = Object.assign({}, cmd);\n        if (this.serverApi) {\n            const { version, strict, deprecationErrors } = this.serverApi;\n            finalCmd.apiVersion = version;\n            if (strict != null)\n                finalCmd.apiStrict = strict;\n            if (deprecationErrors != null)\n                finalCmd.apiDeprecationErrors = deprecationErrors;\n        }\n        if (hasSessionSupport(this) && session) {\n            if (session.clusterTime &&\n                clusterTime &&\n                session.clusterTime.clusterTime.greaterThan(clusterTime.clusterTime)) {\n                clusterTime = session.clusterTime;\n            }\n            const err = (0, sessions_1.applySession)(session, finalCmd, options);\n            if (err) {\n                return callback(err);\n            }\n        }\n        // if we have a known cluster time, gossip it\n        if (clusterTime) {\n            finalCmd.$clusterTime = clusterTime;\n        }\n        if ((0, shared_1.isSharded)(this) && !shouldUseOpMsg && readPreference && readPreference.mode !== 'primary') {\n            finalCmd = {\n                $query: finalCmd,\n                $readPreference: readPreference.toJSON()\n            };\n        }\n        const commandOptions = Object.assign({\n            command: true,\n            numberToSkip: 0,\n            numberToReturn: -1,\n            checkKeys: false,\n            // This value is not overridable\n            slaveOk: readPreference.slaveOk()\n        }, options);\n        const cmdNs = `${ns.db}.$cmd`;\n        const message = shouldUseOpMsg\n            ? new commands_1.Msg(cmdNs, finalCmd, commandOptions)\n            : new commands_1.Query(cmdNs, finalCmd, commandOptions);\n        try {\n            write(this, message, commandOptions, callback);\n        }\n        catch (err) {\n            callback(err);\n        }\n    }\n    /** @internal */\n    query(ns, cmd, options, callback) {\n        var _a;\n        const isExplain = cmd.$explain != null;\n        const readPreference = (_a = options.readPreference) !== null && _a !== void 0 ? _a : read_preference_1.ReadPreference.primary;\n        const batchSize = options.batchSize || 0;\n        const limit = options.limit;\n        const numberToSkip = options.skip || 0;\n        let numberToReturn = 0;\n        if (limit &&\n            (limit < 0 || (limit !== 0 && limit < batchSize) || (limit > 0 && batchSize === 0))) {\n            numberToReturn = limit;\n        }\n        else {\n            numberToReturn = batchSize;\n        }\n        if (isExplain) {\n            // nToReturn must be 0 (match all) or negative (match N and close cursor)\n            // nToReturn > 0 will give explain results equivalent to limit(0)\n            numberToReturn = -Math.abs(limit || 0);\n        }\n        const queryOptions = {\n            numberToSkip,\n            numberToReturn,\n            pre32Limit: typeof limit === 'number' ? limit : undefined,\n            checkKeys: false,\n            slaveOk: readPreference.slaveOk()\n        };\n        if (options.projection) {\n            queryOptions.returnFieldSelector = options.projection;\n        }\n        const query = new commands_1.Query(ns.toString(), cmd, queryOptions);\n        if (typeof options.tailable === 'boolean') {\n            query.tailable = options.tailable;\n        }\n        if (typeof options.oplogReplay === 'boolean') {\n            query.oplogReplay = options.oplogReplay;\n        }\n        if (typeof options.timeout === 'boolean') {\n            query.noCursorTimeout = !options.timeout;\n        }\n        else if (typeof options.noCursorTimeout === 'boolean') {\n            query.noCursorTimeout = options.noCursorTimeout;\n        }\n        if (typeof options.awaitData === 'boolean') {\n            query.awaitData = options.awaitData;\n        }\n        if (typeof options.partial === 'boolean') {\n            query.partial = options.partial;\n        }\n        write(this, query, { [kFullResult]: true, ...(0, bson_1.pluckBSONSerializeOptions)(options) }, (err, result) => {\n            if (err || !result)\n                return callback(err, result);\n            if (isExplain && result.documents && result.documents[0]) {\n                return callback(undefined, result.documents[0]);\n            }\n            callback(undefined, result);\n        });\n    }\n    /** @internal */\n    getMore(ns, cursorId, options, callback) {\n        const fullResult = !!options[kFullResult];\n        const wireVersion = (0, utils_1.maxWireVersion)(this);\n        if (!cursorId) {\n            // TODO(NODE-3483): Replace this with a MongoCommandError\n            callback(new error_1.MongoRuntimeError('Invalid internal cursor state, no known cursor id'));\n            return;\n        }\n        if (wireVersion < 4) {\n            const getMoreOp = new commands_1.GetMore(ns.toString(), cursorId, { numberToReturn: options.batchSize });\n            const queryOptions = (0, shared_1.applyCommonQueryOptions)({}, Object.assign(options, { ...(0, bson_1.pluckBSONSerializeOptions)(options) }));\n            queryOptions[kFullResult] = true;\n            queryOptions.command = true;\n            write(this, getMoreOp, queryOptions, (err, response) => {\n                if (fullResult)\n                    return callback(err, response);\n                if (err)\n                    return callback(err);\n                callback(undefined, { cursor: { id: response.cursorId, nextBatch: response.documents } });\n            });\n            return;\n        }\n        const getMoreCmd = {\n            getMore: cursorId,\n            collection: ns.collection\n        };\n        if (typeof options.batchSize === 'number') {\n            getMoreCmd.batchSize = Math.abs(options.batchSize);\n        }\n        if (typeof options.maxAwaitTimeMS === 'number') {\n            getMoreCmd.maxTimeMS = options.maxAwaitTimeMS;\n        }\n        const commandOptions = Object.assign({\n            returnFieldSelector: null,\n            documentsReturnedIn: 'nextBatch'\n        }, options);\n        this.command(ns, getMoreCmd, commandOptions, callback);\n    }\n    /** @internal */\n    killCursors(ns, cursorIds, options, callback) {\n        if (!cursorIds || !Array.isArray(cursorIds)) {\n            // TODO(NODE-3483): Replace this with a MongoCommandError\n            throw new error_1.MongoRuntimeError(`Invalid list of cursor ids provided: ${cursorIds}`);\n        }\n        if ((0, utils_1.maxWireVersion)(this) < 4) {\n            try {\n                write(this, new commands_1.KillCursor(ns.toString(), cursorIds), { noResponse: true, ...options }, callback);\n            }\n            catch (err) {\n                callback(err);\n            }\n            return;\n        }\n        this.command(ns, { killCursors: ns.collection, cursors: cursorIds }, { [kFullResult]: true, ...options }, (err, response) => {\n            if (err || !response)\n                return callback(err);\n            if (response.cursorNotFound) {\n                return callback(new error_1.MongoNetworkError('cursor killed or timed out'), null);\n            }\n            if (!Array.isArray(response.documents) || response.documents.length === 0) {\n                return callback(\n                // TODO(NODE-3483)\n                new error_1.MongoRuntimeError(`invalid killCursors result returned for cursor id ${cursorIds[0]}`));\n            }\n            callback(undefined, response.documents[0]);\n        });\n    }\n}\nexports.Connection = Connection;\n/** @event */\nConnection.COMMAND_STARTED = 'commandStarted';\n/** @event */\nConnection.COMMAND_SUCCEEDED = 'commandSucceeded';\n/** @event */\nConnection.COMMAND_FAILED = 'commandFailed';\n/** @event */\nConnection.CLUSTER_TIME_RECEIVED = 'clusterTimeReceived';\n/** @event */\nConnection.CLOSE = 'close';\n/** @event */\nConnection.MESSAGE = 'message';\n/** @event */\nConnection.PINNED = 'pinned';\n/** @event */\nConnection.UNPINNED = 'unpinned';\n/** @public */\nexports.APM_EVENTS = [\n    Connection.COMMAND_STARTED,\n    Connection.COMMAND_SUCCEEDED,\n    Connection.COMMAND_FAILED\n];\n/** @internal */\nclass CryptoConnection extends Connection {\n    constructor(stream, options) {\n        super(stream, options);\n        this[kAutoEncrypter] = options.autoEncrypter;\n    }\n    /** @internal @override */\n    command(ns, cmd, options, callback) {\n        const autoEncrypter = this[kAutoEncrypter];\n        if (!autoEncrypter) {\n            return callback(new error_1.MongoMissingDependencyError('No AutoEncrypter available for encryption'));\n        }\n        const serverWireVersion = (0, utils_1.maxWireVersion)(this);\n        if (serverWireVersion === 0) {\n            // This means the initial handshake hasn't happened yet\n            return super.command(ns, cmd, options, callback);\n        }\n        if (serverWireVersion < 8) {\n            callback(new error_1.MongoCompatibilityError('Auto-encryption requires a minimum MongoDB version of 4.2'));\n            return;\n        }\n        autoEncrypter.encrypt(ns.toString(), cmd, options, (err, encrypted) => {\n            if (err || encrypted == null) {\n                callback(err, null);\n                return;\n            }\n            super.command(ns, encrypted, options, (err, response) => {\n                if (err || response == null) {\n                    callback(err, response);\n                    return;\n                }\n                autoEncrypter.decrypt(response, options, callback);\n            });\n        });\n    }\n}\nexports.CryptoConnection = CryptoConnection;\n/** @internal */\nfunction hasSessionSupport(conn) {\n    const description = conn.description;\n    return description.logicalSessionTimeoutMinutes != null || !!description.loadBalanced;\n}\nexports.hasSessionSupport = hasSessionSupport;\nfunction supportsOpMsg(conn) {\n    const description = conn.description;\n    if (description == null) {\n        return false;\n    }\n    return (0, utils_1.maxWireVersion)(conn) >= 6 && !description.__nodejs_mock_server__;\n}\nfunction messageHandler(conn) {\n    return function messageHandler(message) {\n        // always emit the message, in case we are streaming\n        conn.emit('message', message);\n        const operationDescription = conn[kQueue].get(message.responseTo);\n        if (!operationDescription) {\n            return;\n        }\n        const callback = operationDescription.cb;\n        // SERVER-45775: For exhaust responses we should be able to use the same requestId to\n        // track response, however the server currently synthetically produces remote requests\n        // making the `responseTo` change on each response\n        conn[kQueue].delete(message.responseTo);\n        if ('moreToCome' in message && message.moreToCome) {\n            // requeue the callback for next synthetic request\n            conn[kQueue].set(message.requestId, operationDescription);\n        }\n        else if (operationDescription.socketTimeoutOverride) {\n            conn[kStream].setTimeout(conn.socketTimeoutMS);\n        }\n        try {\n            // Pass in the entire description because it has BSON parsing options\n            message.parse(operationDescription);\n        }\n        catch (err) {\n            // If this error is generated by our own code, it will already have the correct class applied\n            // if it is not, then it is coming from a catastrophic data parse failure or the BSON library\n            // in either case, it should not be wrapped\n            callback(err);\n            return;\n        }\n        if (message.documents[0]) {\n            const document = message.documents[0];\n            const session = operationDescription.session;\n            if (session) {\n                (0, sessions_1.updateSessionFromResponse)(session, document);\n            }\n            if (document.$clusterTime) {\n                conn[kClusterTime] = document.$clusterTime;\n                conn.emit(Connection.CLUSTER_TIME_RECEIVED, document.$clusterTime);\n            }\n            if (operationDescription.command) {\n                if (document.writeConcernError) {\n                    callback(new error_1.MongoWriteConcernError(document.writeConcernError, document));\n                    return;\n                }\n                if (document.ok === 0 || document.$err || document.errmsg || document.code) {\n                    callback(new error_1.MongoServerError(document));\n                    return;\n                }\n            }\n            else {\n                // Pre 3.2 support\n                if (document.ok === 0 || document.$err || document.errmsg) {\n                    callback(new error_1.MongoServerError(document));\n                    return;\n                }\n            }\n        }\n        callback(undefined, operationDescription.fullResult ? message : message.documents[0]);\n    };\n}\nfunction streamIdentifier(stream) {\n    if (typeof stream.address === 'function') {\n        return `${stream.remoteAddress}:${stream.remotePort}`;\n    }\n    return (0, utils_1.uuidV4)().toString('hex');\n}\nfunction write(conn, command, options, callback) {\n    if (typeof options === 'function') {\n        callback = options;\n    }\n    options = options !== null && options !== void 0 ? options : {};\n    const operationDescription = {\n        requestId: command.requestId,\n        cb: callback,\n        session: options.session,\n        fullResult: !!options[kFullResult],\n        noResponse: typeof options.noResponse === 'boolean' ? options.noResponse : false,\n        documentsReturnedIn: options.documentsReturnedIn,\n        command: !!options.command,\n        // for BSON parsing\n        promoteLongs: typeof options.promoteLongs === 'boolean' ? options.promoteLongs : true,\n        promoteValues: typeof options.promoteValues === 'boolean' ? options.promoteValues : true,\n        promoteBuffers: typeof options.promoteBuffers === 'boolean' ? options.promoteBuffers : false,\n        bsonRegExp: typeof options.bsonRegExp === 'boolean' ? options.bsonRegExp : false,\n        raw: typeof options.raw === 'boolean' ? options.raw : false,\n        started: 0\n    };\n    if (conn[kDescription] && conn[kDescription].compressor) {\n        operationDescription.agreedCompressor = conn[kDescription].compressor;\n        if (conn[kDescription].zlibCompressionLevel) {\n            operationDescription.zlibCompressionLevel = conn[kDescription].zlibCompressionLevel;\n        }\n    }\n    if (typeof options.socketTimeoutMS === 'number') {\n        operationDescription.socketTimeoutOverride = true;\n        conn[kStream].setTimeout(options.socketTimeoutMS);\n    }\n    // if command monitoring is enabled we need to modify the callback here\n    if (conn.monitorCommands) {\n        conn.emit(Connection.COMMAND_STARTED, new command_monitoring_events_1.CommandStartedEvent(conn, command));\n        operationDescription.started = (0, utils_1.now)();\n        operationDescription.cb = (err, reply) => {\n            if (err) {\n                conn.emit(Connection.COMMAND_FAILED, new command_monitoring_events_1.CommandFailedEvent(conn, command, err, operationDescription.started));\n            }\n            else {\n                if (reply && (reply.ok === 0 || reply.$err)) {\n                    conn.emit(Connection.COMMAND_FAILED, new command_monitoring_events_1.CommandFailedEvent(conn, command, reply, operationDescription.started));\n                }\n                else {\n                    conn.emit(Connection.COMMAND_SUCCEEDED, new command_monitoring_events_1.CommandSucceededEvent(conn, command, reply, operationDescription.started));\n                }\n            }\n            if (typeof callback === 'function') {\n                callback(err, reply);\n            }\n        };\n    }\n    if (!operationDescription.noResponse) {\n        conn[kQueue].set(operationDescription.requestId, operationDescription);\n    }\n    try {\n        conn[kMessageStream].writeCommand(command, operationDescription);\n    }\n    catch (e) {\n        if (!operationDescription.noResponse) {\n            conn[kQueue].delete(operationDescription.requestId);\n            operationDescription.cb(e);\n            return;\n        }\n    }\n    if (operationDescription.noResponse) {\n        operationDescription.cb();\n    }\n}\n//# sourceMappingURL=connection.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.MessageStream = void 0;\nconst stream_1 = require(\"stream\");\nconst commands_1 = require(\"./commands\");\nconst error_1 = require(\"../error\");\nconst constants_1 = require(\"./wire_protocol/constants\");\nconst compression_1 = require(\"./wire_protocol/compression\");\nconst utils_1 = require(\"../utils\");\nconst MESSAGE_HEADER_SIZE = 16;\nconst COMPRESSION_DETAILS_SIZE = 9; // originalOpcode + uncompressedSize, compressorID\nconst kDefaultMaxBsonMessageSize = 1024 * 1024 * 16 * 4;\n/** @internal */\nconst kBuffer = Symbol('buffer');\n/**\n * A duplex stream that is capable of reading and writing raw wire protocol messages, with\n * support for optional compression\n * @internal\n */\nclass MessageStream extends stream_1.Duplex {\n    constructor(options = {}) {\n        super(options);\n        this.maxBsonMessageSize = options.maxBsonMessageSize || kDefaultMaxBsonMessageSize;\n        this[kBuffer] = new utils_1.BufferPool();\n    }\n    _write(chunk, _, callback) {\n        this[kBuffer].append(chunk);\n        processIncomingData(this, callback);\n    }\n    _read( /* size */) {\n        // NOTE: This implementation is empty because we explicitly push data to be read\n        //       when `writeMessage` is called.\n        return;\n    }\n    writeCommand(command, operationDescription) {\n        // TODO: agreed compressor should live in `StreamDescription`\n        const compressorName = operationDescription && operationDescription.agreedCompressor\n            ? operationDescription.agreedCompressor\n            : 'none';\n        if (compressorName === 'none' || !canCompress(command)) {\n            const data = command.toBin();\n            this.push(Array.isArray(data) ? Buffer.concat(data) : data);\n            return;\n        }\n        // otherwise, compress the message\n        const concatenatedOriginalCommandBuffer = Buffer.concat(command.toBin());\n        const messageToBeCompressed = concatenatedOriginalCommandBuffer.slice(MESSAGE_HEADER_SIZE);\n        // Extract information needed for OP_COMPRESSED from the uncompressed message\n        const originalCommandOpCode = concatenatedOriginalCommandBuffer.readInt32LE(12);\n        // Compress the message body\n        (0, compression_1.compress)({ options: operationDescription }, messageToBeCompressed, (err, compressedMessage) => {\n            if (err || !compressedMessage) {\n                operationDescription.cb(err);\n                return;\n            }\n            // Create the msgHeader of OP_COMPRESSED\n            const msgHeader = Buffer.alloc(MESSAGE_HEADER_SIZE);\n            msgHeader.writeInt32LE(MESSAGE_HEADER_SIZE + COMPRESSION_DETAILS_SIZE + compressedMessage.length, 0); // messageLength\n            msgHeader.writeInt32LE(command.requestId, 4); // requestID\n            msgHeader.writeInt32LE(0, 8); // responseTo (zero)\n            msgHeader.writeInt32LE(constants_1.OP_COMPRESSED, 12); // opCode\n            // Create the compression details of OP_COMPRESSED\n            const compressionDetails = Buffer.alloc(COMPRESSION_DETAILS_SIZE);\n            compressionDetails.writeInt32LE(originalCommandOpCode, 0); // originalOpcode\n            compressionDetails.writeInt32LE(messageToBeCompressed.length, 4); // Size of the uncompressed compressedMessage, excluding the MsgHeader\n            compressionDetails.writeUInt8(compression_1.Compressor[compressorName], 8); // compressorID\n            this.push(Buffer.concat([msgHeader, compressionDetails, compressedMessage]));\n        });\n    }\n}\nexports.MessageStream = MessageStream;\n// Return whether a command contains an uncompressible command term\n// Will return true if command contains no uncompressible command terms\nfunction canCompress(command) {\n    const commandDoc = command instanceof commands_1.Msg ? command.command : command.query;\n    const commandName = Object.keys(commandDoc)[0];\n    return !compression_1.uncompressibleCommands.has(commandName);\n}\nfunction processIncomingData(stream, callback) {\n    const buffer = stream[kBuffer];\n    if (buffer.length < 4) {\n        callback();\n        return;\n    }\n    const sizeOfMessage = buffer.peek(4).readInt32LE();\n    if (sizeOfMessage < 0) {\n        callback(new error_1.MongoParseError(`Invalid message size: ${sizeOfMessage}`));\n        return;\n    }\n    if (sizeOfMessage > stream.maxBsonMessageSize) {\n        callback(new error_1.MongoParseError(`Invalid message size: ${sizeOfMessage}, max allowed: ${stream.maxBsonMessageSize}`));\n        return;\n    }\n    if (sizeOfMessage > buffer.length) {\n        callback();\n        return;\n    }\n    const message = buffer.read(sizeOfMessage);\n    const messageHeader = {\n        length: message.readInt32LE(0),\n        requestId: message.readInt32LE(4),\n        responseTo: message.readInt32LE(8),\n        opCode: message.readInt32LE(12)\n    };\n    let ResponseType = messageHeader.opCode === constants_1.OP_MSG ? commands_1.BinMsg : commands_1.Response;\n    if (messageHeader.opCode !== constants_1.OP_COMPRESSED) {\n        const messageBody = message.slice(MESSAGE_HEADER_SIZE);\n        stream.emit('message', new ResponseType(message, messageHeader, messageBody));\n        if (buffer.length >= 4) {\n            processIncomingData(stream, callback);\n        }\n        else {\n            callback();\n        }\n        return;\n    }\n    messageHeader.fromCompressed = true;\n    messageHeader.opCode = message.readInt32LE(MESSAGE_HEADER_SIZE);\n    messageHeader.length = message.readInt32LE(MESSAGE_HEADER_SIZE + 4);\n    const compressorID = message[MESSAGE_HEADER_SIZE + 8];\n    const compressedBuffer = message.slice(MESSAGE_HEADER_SIZE + 9);\n    // recalculate based on wrapped opcode\n    ResponseType = messageHeader.opCode === constants_1.OP_MSG ? commands_1.BinMsg : commands_1.Response;\n    (0, compression_1.decompress)(compressorID, compressedBuffer, (err, messageBody) => {\n        if (err || !messageBody) {\n            callback(err);\n            return;\n        }\n        if (messageBody.length !== messageHeader.length) {\n            callback(new error_1.MongoDecompressionError('Message body and message header must be the same length'));\n            return;\n        }\n        stream.emit('message', new ResponseType(message, messageHeader, messageBody));\n        if (buffer.length >= 4) {\n            processIncomingData(stream, callback);\n        }\n        else {\n            callback();\n        }\n    });\n}\n//# sourceMappingURL=message_stream.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.BinMsg = exports.Msg = exports.Response = exports.KillCursor = exports.GetMore = exports.Query = void 0;\nconst read_preference_1 = require(\"../read_preference\");\nconst BSON = require(\"../bson\");\nconst utils_1 = require(\"../utils\");\nconst constants_1 = require(\"./wire_protocol/constants\");\nconst error_1 = require(\"../error\");\n// Incrementing request id\nlet _requestId = 0;\n// Query flags\nconst OPTS_TAILABLE_CURSOR = 2;\nconst OPTS_SLAVE = 4;\nconst OPTS_OPLOG_REPLAY = 8;\nconst OPTS_NO_CURSOR_TIMEOUT = 16;\nconst OPTS_AWAIT_DATA = 32;\nconst OPTS_EXHAUST = 64;\nconst OPTS_PARTIAL = 128;\n// Response flags\nconst CURSOR_NOT_FOUND = 1;\nconst QUERY_FAILURE = 2;\nconst SHARD_CONFIG_STALE = 4;\nconst AWAIT_CAPABLE = 8;\n/**************************************************************\n * QUERY\n **************************************************************/\n/** @internal */\nclass Query {\n    constructor(ns, query, options) {\n        // Basic options needed to be passed in\n        // TODO(NODE-3483): Replace with MongoCommandError\n        if (ns == null)\n            throw new error_1.MongoRuntimeError('Namespace must be specified for query');\n        // TODO(NODE-3483): Replace with MongoCommandError\n        if (query == null)\n            throw new error_1.MongoRuntimeError('A query document must be specified for query');\n        // Validate that we are not passing 0x00 in the collection name\n        if (ns.indexOf('\\x00') !== -1) {\n            // TODO(NODE-3483): Use MongoNamespace static method\n            throw new error_1.MongoRuntimeError('Namespace cannot contain a null character');\n        }\n        // Basic options\n        this.ns = ns;\n        this.query = query;\n        // Additional options\n        this.numberToSkip = options.numberToSkip || 0;\n        this.numberToReturn = options.numberToReturn || 0;\n        this.returnFieldSelector = options.returnFieldSelector || undefined;\n        this.requestId = Query.getRequestId();\n        // special case for pre-3.2 find commands, delete ASAP\n        this.pre32Limit = options.pre32Limit;\n        // Serialization option\n        this.serializeFunctions =\n            typeof options.serializeFunctions === 'boolean' ? options.serializeFunctions : false;\n        this.ignoreUndefined =\n            typeof options.ignoreUndefined === 'boolean' ? options.ignoreUndefined : false;\n        this.maxBsonSize = options.maxBsonSize || 1024 * 1024 * 16;\n        this.checkKeys = typeof options.checkKeys === 'boolean' ? options.checkKeys : false;\n        this.batchSize = this.numberToReturn;\n        // Flags\n        this.tailable = false;\n        this.slaveOk = typeof options.slaveOk === 'boolean' ? options.slaveOk : false;\n        this.oplogReplay = false;\n        this.noCursorTimeout = false;\n        this.awaitData = false;\n        this.exhaust = false;\n        this.partial = false;\n    }\n    /** Assign next request Id. */\n    incRequestId() {\n        this.requestId = _requestId++;\n    }\n    /** Peek next request Id. */\n    nextRequestId() {\n        return _requestId + 1;\n    }\n    /** Increment then return next request Id. */\n    static getRequestId() {\n        return ++_requestId;\n    }\n    // Uses a single allocated buffer for the process, avoiding multiple memory allocations\n    toBin() {\n        const buffers = [];\n        let projection = null;\n        // Set up the flags\n        let flags = 0;\n        if (this.tailable) {\n            flags |= OPTS_TAILABLE_CURSOR;\n        }\n        if (this.slaveOk) {\n            flags |= OPTS_SLAVE;\n        }\n        if (this.oplogReplay) {\n            flags |= OPTS_OPLOG_REPLAY;\n        }\n        if (this.noCursorTimeout) {\n            flags |= OPTS_NO_CURSOR_TIMEOUT;\n        }\n        if (this.awaitData) {\n            flags |= OPTS_AWAIT_DATA;\n        }\n        if (this.exhaust) {\n            flags |= OPTS_EXHAUST;\n        }\n        if (this.partial) {\n            flags |= OPTS_PARTIAL;\n        }\n        // If batchSize is different to this.numberToReturn\n        if (this.batchSize !== this.numberToReturn)\n            this.numberToReturn = this.batchSize;\n        // Allocate write protocol header buffer\n        const header = Buffer.alloc(4 * 4 + // Header\n            4 + // Flags\n            Buffer.byteLength(this.ns) +\n            1 + // namespace\n            4 + // numberToSkip\n            4 // numberToReturn\n        );\n        // Add header to buffers\n        buffers.push(header);\n        // Serialize the query\n        const query = BSON.serialize(this.query, {\n            checkKeys: this.checkKeys,\n            serializeFunctions: this.serializeFunctions,\n            ignoreUndefined: this.ignoreUndefined\n        });\n        // Add query document\n        buffers.push(query);\n        if (this.returnFieldSelector && Object.keys(this.returnFieldSelector).length > 0) {\n            // Serialize the projection document\n            projection = BSON.serialize(this.returnFieldSelector, {\n                checkKeys: this.checkKeys,\n                serializeFunctions: this.serializeFunctions,\n                ignoreUndefined: this.ignoreUndefined\n            });\n            // Add projection document\n            buffers.push(projection);\n        }\n        // Total message size\n        const totalLength = header.length + query.length + (projection ? projection.length : 0);\n        // Set up the index\n        let index = 4;\n        // Write total document length\n        header[3] = (totalLength >> 24) & 0xff;\n        header[2] = (totalLength >> 16) & 0xff;\n        header[1] = (totalLength >> 8) & 0xff;\n        header[0] = totalLength & 0xff;\n        // Write header information requestId\n        header[index + 3] = (this.requestId >> 24) & 0xff;\n        header[index + 2] = (this.requestId >> 16) & 0xff;\n        header[index + 1] = (this.requestId >> 8) & 0xff;\n        header[index] = this.requestId & 0xff;\n        index = index + 4;\n        // Write header information responseTo\n        header[index + 3] = (0 >> 24) & 0xff;\n        header[index + 2] = (0 >> 16) & 0xff;\n        header[index + 1] = (0 >> 8) & 0xff;\n        header[index] = 0 & 0xff;\n        index = index + 4;\n        // Write header information OP_QUERY\n        header[index + 3] = (constants_1.OP_QUERY >> 24) & 0xff;\n        header[index + 2] = (constants_1.OP_QUERY >> 16) & 0xff;\n        header[index + 1] = (constants_1.OP_QUERY >> 8) & 0xff;\n        header[index] = constants_1.OP_QUERY & 0xff;\n        index = index + 4;\n        // Write header information flags\n        header[index + 3] = (flags >> 24) & 0xff;\n        header[index + 2] = (flags >> 16) & 0xff;\n        header[index + 1] = (flags >> 8) & 0xff;\n        header[index] = flags & 0xff;\n        index = index + 4;\n        // Write collection name\n        index = index + header.write(this.ns, index, 'utf8') + 1;\n        header[index - 1] = 0;\n        // Write header information flags numberToSkip\n        header[index + 3] = (this.numberToSkip >> 24) & 0xff;\n        header[index + 2] = (this.numberToSkip >> 16) & 0xff;\n        header[index + 1] = (this.numberToSkip >> 8) & 0xff;\n        header[index] = this.numberToSkip & 0xff;\n        index = index + 4;\n        // Write header information flags numberToReturn\n        header[index + 3] = (this.numberToReturn >> 24) & 0xff;\n        header[index + 2] = (this.numberToReturn >> 16) & 0xff;\n        header[index + 1] = (this.numberToReturn >> 8) & 0xff;\n        header[index] = this.numberToReturn & 0xff;\n        index = index + 4;\n        // Return the buffers\n        return buffers;\n    }\n}\nexports.Query = Query;\n/**************************************************************\n * GETMORE\n **************************************************************/\n/** @internal */\nclass GetMore {\n    constructor(ns, cursorId, opts = {}) {\n        this.numberToReturn = opts.numberToReturn || 0;\n        this.requestId = _requestId++;\n        this.ns = ns;\n        this.cursorId = cursorId;\n    }\n    // Uses a single allocated buffer for the process, avoiding multiple memory allocations\n    toBin() {\n        const length = 4 + Buffer.byteLength(this.ns) + 1 + 4 + 8 + 4 * 4;\n        // Create command buffer\n        let index = 0;\n        // Allocate buffer\n        const _buffer = Buffer.alloc(length);\n        // Write header information\n        // index = write32bit(index, _buffer, length);\n        _buffer[index + 3] = (length >> 24) & 0xff;\n        _buffer[index + 2] = (length >> 16) & 0xff;\n        _buffer[index + 1] = (length >> 8) & 0xff;\n        _buffer[index] = length & 0xff;\n        index = index + 4;\n        // index = write32bit(index, _buffer, requestId);\n        _buffer[index + 3] = (this.requestId >> 24) & 0xff;\n        _buffer[index + 2] = (this.requestId >> 16) & 0xff;\n        _buffer[index + 1] = (this.requestId >> 8) & 0xff;\n        _buffer[index] = this.requestId & 0xff;\n        index = index + 4;\n        // index = write32bit(index, _buffer, 0);\n        _buffer[index + 3] = (0 >> 24) & 0xff;\n        _buffer[index + 2] = (0 >> 16) & 0xff;\n        _buffer[index + 1] = (0 >> 8) & 0xff;\n        _buffer[index] = 0 & 0xff;\n        index = index + 4;\n        // index = write32bit(index, _buffer, OP_GETMORE);\n        _buffer[index + 3] = (constants_1.OP_GETMORE >> 24) & 0xff;\n        _buffer[index + 2] = (constants_1.OP_GETMORE >> 16) & 0xff;\n        _buffer[index + 1] = (constants_1.OP_GETMORE >> 8) & 0xff;\n        _buffer[index] = constants_1.OP_GETMORE & 0xff;\n        index = index + 4;\n        // index = write32bit(index, _buffer, 0);\n        _buffer[index + 3] = (0 >> 24) & 0xff;\n        _buffer[index + 2] = (0 >> 16) & 0xff;\n        _buffer[index + 1] = (0 >> 8) & 0xff;\n        _buffer[index] = 0 & 0xff;\n        index = index + 4;\n        // Write collection name\n        index = index + _buffer.write(this.ns, index, 'utf8') + 1;\n        _buffer[index - 1] = 0;\n        // Write batch size\n        // index = write32bit(index, _buffer, numberToReturn);\n        _buffer[index + 3] = (this.numberToReturn >> 24) & 0xff;\n        _buffer[index + 2] = (this.numberToReturn >> 16) & 0xff;\n        _buffer[index + 1] = (this.numberToReturn >> 8) & 0xff;\n        _buffer[index] = this.numberToReturn & 0xff;\n        index = index + 4;\n        // Write cursor id\n        // index = write32bit(index, _buffer, cursorId.getLowBits());\n        _buffer[index + 3] = (this.cursorId.getLowBits() >> 24) & 0xff;\n        _buffer[index + 2] = (this.cursorId.getLowBits() >> 16) & 0xff;\n        _buffer[index + 1] = (this.cursorId.getLowBits() >> 8) & 0xff;\n        _buffer[index] = this.cursorId.getLowBits() & 0xff;\n        index = index + 4;\n        // index = write32bit(index, _buffer, cursorId.getHighBits());\n        _buffer[index + 3] = (this.cursorId.getHighBits() >> 24) & 0xff;\n        _buffer[index + 2] = (this.cursorId.getHighBits() >> 16) & 0xff;\n        _buffer[index + 1] = (this.cursorId.getHighBits() >> 8) & 0xff;\n        _buffer[index] = this.cursorId.getHighBits() & 0xff;\n        index = index + 4;\n        // Return buffer\n        return [_buffer];\n    }\n}\nexports.GetMore = GetMore;\n/**************************************************************\n * KILLCURSOR\n **************************************************************/\n/** @internal */\nclass KillCursor {\n    constructor(ns, cursorIds) {\n        this.ns = ns;\n        this.requestId = _requestId++;\n        this.cursorIds = cursorIds;\n    }\n    // Uses a single allocated buffer for the process, avoiding multiple memory allocations\n    toBin() {\n        const length = 4 + 4 + 4 * 4 + this.cursorIds.length * 8;\n        // Create command buffer\n        let index = 0;\n        const _buffer = Buffer.alloc(length);\n        // Write header information\n        // index = write32bit(index, _buffer, length);\n        _buffer[index + 3] = (length >> 24) & 0xff;\n        _buffer[index + 2] = (length >> 16) & 0xff;\n        _buffer[index + 1] = (length >> 8) & 0xff;\n        _buffer[index] = length & 0xff;\n        index = index + 4;\n        // index = write32bit(index, _buffer, requestId);\n        _buffer[index + 3] = (this.requestId >> 24) & 0xff;\n        _buffer[index + 2] = (this.requestId >> 16) & 0xff;\n        _buffer[index + 1] = (this.requestId >> 8) & 0xff;\n        _buffer[index] = this.requestId & 0xff;\n        index = index + 4;\n        // index = write32bit(index, _buffer, 0);\n        _buffer[index + 3] = (0 >> 24) & 0xff;\n        _buffer[index + 2] = (0 >> 16) & 0xff;\n        _buffer[index + 1] = (0 >> 8) & 0xff;\n        _buffer[index] = 0 & 0xff;\n        index = index + 4;\n        // index = write32bit(index, _buffer, OP_KILL_CURSORS);\n        _buffer[index + 3] = (constants_1.OP_KILL_CURSORS >> 24) & 0xff;\n        _buffer[index + 2] = (constants_1.OP_KILL_CURSORS >> 16) & 0xff;\n        _buffer[index + 1] = (constants_1.OP_KILL_CURSORS >> 8) & 0xff;\n        _buffer[index] = constants_1.OP_KILL_CURSORS & 0xff;\n        index = index + 4;\n        // index = write32bit(index, _buffer, 0);\n        _buffer[index + 3] = (0 >> 24) & 0xff;\n        _buffer[index + 2] = (0 >> 16) & 0xff;\n        _buffer[index + 1] = (0 >> 8) & 0xff;\n        _buffer[index] = 0 & 0xff;\n        index = index + 4;\n        // Write batch size\n        // index = write32bit(index, _buffer, this.cursorIds.length);\n        _buffer[index + 3] = (this.cursorIds.length >> 24) & 0xff;\n        _buffer[index + 2] = (this.cursorIds.length >> 16) & 0xff;\n        _buffer[index + 1] = (this.cursorIds.length >> 8) & 0xff;\n        _buffer[index] = this.cursorIds.length & 0xff;\n        index = index + 4;\n        // Write all the cursor ids into the array\n        for (let i = 0; i < this.cursorIds.length; i++) {\n            // Write cursor id\n            // index = write32bit(index, _buffer, cursorIds[i].getLowBits());\n            _buffer[index + 3] = (this.cursorIds[i].getLowBits() >> 24) & 0xff;\n            _buffer[index + 2] = (this.cursorIds[i].getLowBits() >> 16) & 0xff;\n            _buffer[index + 1] = (this.cursorIds[i].getLowBits() >> 8) & 0xff;\n            _buffer[index] = this.cursorIds[i].getLowBits() & 0xff;\n            index = index + 4;\n            // index = write32bit(index, _buffer, cursorIds[i].getHighBits());\n            _buffer[index + 3] = (this.cursorIds[i].getHighBits() >> 24) & 0xff;\n            _buffer[index + 2] = (this.cursorIds[i].getHighBits() >> 16) & 0xff;\n            _buffer[index + 1] = (this.cursorIds[i].getHighBits() >> 8) & 0xff;\n            _buffer[index] = this.cursorIds[i].getHighBits() & 0xff;\n            index = index + 4;\n        }\n        // Return buffer\n        return [_buffer];\n    }\n}\nexports.KillCursor = KillCursor;\n/** @internal */\nclass Response {\n    constructor(message, msgHeader, msgBody, opts) {\n        this.parsed = false;\n        this.raw = message;\n        this.data = msgBody;\n        this.opts = opts !== null && opts !== void 0 ? opts : {\n            promoteLongs: true,\n            promoteValues: true,\n            promoteBuffers: false,\n            bsonRegExp: false\n        };\n        // Read the message header\n        this.length = msgHeader.length;\n        this.requestId = msgHeader.requestId;\n        this.responseTo = msgHeader.responseTo;\n        this.opCode = msgHeader.opCode;\n        this.fromCompressed = msgHeader.fromCompressed;\n        // Read the message body\n        this.responseFlags = msgBody.readInt32LE(0);\n        this.cursorId = new BSON.Long(msgBody.readInt32LE(4), msgBody.readInt32LE(8));\n        this.startingFrom = msgBody.readInt32LE(12);\n        this.numberReturned = msgBody.readInt32LE(16);\n        // Preallocate document array\n        this.documents = new Array(this.numberReturned);\n        // Flag values\n        this.cursorNotFound = (this.responseFlags & CURSOR_NOT_FOUND) !== 0;\n        this.queryFailure = (this.responseFlags & QUERY_FAILURE) !== 0;\n        this.shardConfigStale = (this.responseFlags & SHARD_CONFIG_STALE) !== 0;\n        this.awaitCapable = (this.responseFlags & AWAIT_CAPABLE) !== 0;\n        this.promoteLongs = typeof this.opts.promoteLongs === 'boolean' ? this.opts.promoteLongs : true;\n        this.promoteValues =\n            typeof this.opts.promoteValues === 'boolean' ? this.opts.promoteValues : true;\n        this.promoteBuffers =\n            typeof this.opts.promoteBuffers === 'boolean' ? this.opts.promoteBuffers : false;\n        this.bsonRegExp = typeof this.opts.bsonRegExp === 'boolean' ? this.opts.bsonRegExp : false;\n    }\n    isParsed() {\n        return this.parsed;\n    }\n    parse(options) {\n        var _a, _b, _c, _d;\n        // Don't parse again if not needed\n        if (this.parsed)\n            return;\n        options = options !== null && options !== void 0 ? options : {};\n        // Allow the return of raw documents instead of parsing\n        const raw = options.raw || false;\n        const documentsReturnedIn = options.documentsReturnedIn || null;\n        const promoteLongs = (_a = options.promoteLongs) !== null && _a !== void 0 ? _a : this.opts.promoteLongs;\n        const promoteValues = (_b = options.promoteValues) !== null && _b !== void 0 ? _b : this.opts.promoteValues;\n        const promoteBuffers = (_c = options.promoteBuffers) !== null && _c !== void 0 ? _c : this.opts.promoteBuffers;\n        const bsonRegExp = (_d = options.bsonRegExp) !== null && _d !== void 0 ? _d : this.opts.bsonRegExp;\n        let bsonSize;\n        // Set up the options\n        const _options = {\n            promoteLongs,\n            promoteValues,\n            promoteBuffers,\n            bsonRegExp\n        };\n        // Position within OP_REPLY at which documents start\n        // (See https://docs.mongodb.com/manual/reference/mongodb-wire-protocol/#wire-op-reply)\n        this.index = 20;\n        // Parse Body\n        for (let i = 0; i < this.numberReturned; i++) {\n            bsonSize =\n                this.data[this.index] |\n                    (this.data[this.index + 1] << 8) |\n                    (this.data[this.index + 2] << 16) |\n                    (this.data[this.index + 3] << 24);\n            // If we have raw results specified slice the return document\n            if (raw) {\n                this.documents[i] = this.data.slice(this.index, this.index + bsonSize);\n            }\n            else {\n                this.documents[i] = BSON.deserialize(this.data.slice(this.index, this.index + bsonSize), _options);\n            }\n            // Adjust the index\n            this.index = this.index + bsonSize;\n        }\n        if (this.documents.length === 1 && documentsReturnedIn != null && raw) {\n            const fieldsAsRaw = {};\n            fieldsAsRaw[documentsReturnedIn] = true;\n            _options.fieldsAsRaw = fieldsAsRaw;\n            const doc = BSON.deserialize(this.documents[0], _options);\n            this.documents = [doc];\n        }\n        // Set parsed\n        this.parsed = true;\n    }\n}\nexports.Response = Response;\n// Implementation of OP_MSG spec:\n// https://github.com/mongodb/specifications/blob/master/source/message/OP_MSG.rst\n//\n// struct Section {\n//   uint8 payloadType;\n//   union payload {\n//       document  document; // payloadType == 0\n//       struct sequence { // payloadType == 1\n//           int32      size;\n//           cstring    identifier;\n//           document*  documents;\n//       };\n//   };\n// };\n// struct OP_MSG {\n//   struct MsgHeader {\n//       int32  messageLength;\n//       int32  requestID;\n//       int32  responseTo;\n//       int32  opCode = 2013;\n//   };\n//   uint32      flagBits;\n//   Section+    sections;\n//   [uint32     checksum;]\n// };\n// Msg Flags\nconst OPTS_CHECKSUM_PRESENT = 1;\nconst OPTS_MORE_TO_COME = 2;\nconst OPTS_EXHAUST_ALLOWED = 1 << 16;\n/** @internal */\nclass Msg {\n    constructor(ns, command, options) {\n        // Basic options needed to be passed in\n        if (command == null)\n            throw new error_1.MongoInvalidArgumentError('Query document must be specified for query');\n        // Basic options\n        this.ns = ns;\n        this.command = command;\n        this.command.$db = (0, utils_1.databaseNamespace)(ns);\n        if (options.readPreference && options.readPreference.mode !== read_preference_1.ReadPreference.PRIMARY) {\n            this.command.$readPreference = options.readPreference.toJSON();\n        }\n        // Ensure empty options\n        this.options = options !== null && options !== void 0 ? options : {};\n        // Additional options\n        this.requestId = options.requestId ? options.requestId : Msg.getRequestId();\n        // Serialization option\n        this.serializeFunctions =\n            typeof options.serializeFunctions === 'boolean' ? options.serializeFunctions : false;\n        this.ignoreUndefined =\n            typeof options.ignoreUndefined === 'boolean' ? options.ignoreUndefined : false;\n        this.checkKeys = typeof options.checkKeys === 'boolean' ? options.checkKeys : false;\n        this.maxBsonSize = options.maxBsonSize || 1024 * 1024 * 16;\n        // flags\n        this.checksumPresent = false;\n        this.moreToCome = options.moreToCome || false;\n        this.exhaustAllowed =\n            typeof options.exhaustAllowed === 'boolean' ? options.exhaustAllowed : false;\n    }\n    toBin() {\n        const buffers = [];\n        let flags = 0;\n        if (this.checksumPresent) {\n            flags |= OPTS_CHECKSUM_PRESENT;\n        }\n        if (this.moreToCome) {\n            flags |= OPTS_MORE_TO_COME;\n        }\n        if (this.exhaustAllowed) {\n            flags |= OPTS_EXHAUST_ALLOWED;\n        }\n        const header = Buffer.alloc(4 * 4 + // Header\n            4 // Flags\n        );\n        buffers.push(header);\n        let totalLength = header.length;\n        const command = this.command;\n        totalLength += this.makeDocumentSegment(buffers, command);\n        header.writeInt32LE(totalLength, 0); // messageLength\n        header.writeInt32LE(this.requestId, 4); // requestID\n        header.writeInt32LE(0, 8); // responseTo\n        header.writeInt32LE(constants_1.OP_MSG, 12); // opCode\n        header.writeUInt32LE(flags, 16); // flags\n        return buffers;\n    }\n    makeDocumentSegment(buffers, document) {\n        const payloadTypeBuffer = Buffer.alloc(1);\n        payloadTypeBuffer[0] = 0;\n        const documentBuffer = this.serializeBson(document);\n        buffers.push(payloadTypeBuffer);\n        buffers.push(documentBuffer);\n        return payloadTypeBuffer.length + documentBuffer.length;\n    }\n    serializeBson(document) {\n        return BSON.serialize(document, {\n            checkKeys: this.checkKeys,\n            serializeFunctions: this.serializeFunctions,\n            ignoreUndefined: this.ignoreUndefined\n        });\n    }\n    static getRequestId() {\n        _requestId = (_requestId + 1) & 0x7fffffff;\n        return _requestId;\n    }\n}\nexports.Msg = Msg;\n/** @internal */\nclass BinMsg {\n    constructor(message, msgHeader, msgBody, opts) {\n        this.parsed = false;\n        this.raw = message;\n        this.data = msgBody;\n        this.opts = opts !== null && opts !== void 0 ? opts : {\n            promoteLongs: true,\n            promoteValues: true,\n            promoteBuffers: false,\n            bsonRegExp: false\n        };\n        // Read the message header\n        this.length = msgHeader.length;\n        this.requestId = msgHeader.requestId;\n        this.responseTo = msgHeader.responseTo;\n        this.opCode = msgHeader.opCode;\n        this.fromCompressed = msgHeader.fromCompressed;\n        // Read response flags\n        this.responseFlags = msgBody.readInt32LE(0);\n        this.checksumPresent = (this.responseFlags & OPTS_CHECKSUM_PRESENT) !== 0;\n        this.moreToCome = (this.responseFlags & OPTS_MORE_TO_COME) !== 0;\n        this.exhaustAllowed = (this.responseFlags & OPTS_EXHAUST_ALLOWED) !== 0;\n        this.promoteLongs = typeof this.opts.promoteLongs === 'boolean' ? this.opts.promoteLongs : true;\n        this.promoteValues =\n            typeof this.opts.promoteValues === 'boolean' ? this.opts.promoteValues : true;\n        this.promoteBuffers =\n            typeof this.opts.promoteBuffers === 'boolean' ? this.opts.promoteBuffers : false;\n        this.bsonRegExp = typeof this.opts.bsonRegExp === 'boolean' ? this.opts.bsonRegExp : false;\n        this.documents = [];\n    }\n    isParsed() {\n        return this.parsed;\n    }\n    parse(options) {\n        var _a, _b, _c, _d, _e;\n        // Don't parse again if not needed\n        if (this.parsed)\n            return;\n        options = options !== null && options !== void 0 ? options : {};\n        this.index = 4;\n        // Allow the return of raw documents instead of parsing\n        const raw = options.raw || false;\n        const documentsReturnedIn = options.documentsReturnedIn || null;\n        const promoteLongs = (_a = options.promoteLongs) !== null && _a !== void 0 ? _a : this.opts.promoteLongs;\n        const promoteValues = (_b = options.promoteValues) !== null && _b !== void 0 ? _b : this.opts.promoteValues;\n        const promoteBuffers = (_c = options.promoteBuffers) !== null && _c !== void 0 ? _c : this.opts.promoteBuffers;\n        const bsonRegExp = (_d = options.bsonRegExp) !== null && _d !== void 0 ? _d : this.opts.bsonRegExp;\n        const validation = (_e = options.validation) !== null && _e !== void 0 ? _e : { utf8: { writeErrors: false } };\n        // Set up the options\n        const bsonOptions = {\n            promoteLongs,\n            promoteValues,\n            promoteBuffers,\n            bsonRegExp,\n            validation\n            // Due to the strictness of the BSON libraries validation option we need this cast\n        };\n        while (this.index < this.data.length) {\n            const payloadType = this.data.readUInt8(this.index++);\n            if (payloadType === 0) {\n                const bsonSize = this.data.readUInt32LE(this.index);\n                const bin = this.data.slice(this.index, this.index + bsonSize);\n                this.documents.push(raw ? bin : BSON.deserialize(bin, bsonOptions));\n                this.index += bsonSize;\n            }\n            else if (payloadType === 1) {\n                // It was decided that no driver makes use of payload type 1\n                // TODO(NODE-3483): Replace with MongoDeprecationError\n                throw new error_1.MongoRuntimeError('OP_MSG Payload Type 1 detected unsupported protocol');\n            }\n        }\n        if (this.documents.length === 1 && documentsReturnedIn != null && raw) {\n            const fieldsAsRaw = {};\n            fieldsAsRaw[documentsReturnedIn] = true;\n            bsonOptions.fieldsAsRaw = fieldsAsRaw;\n            const doc = BSON.deserialize(this.documents[0], bsonOptions);\n            this.documents = [doc];\n        }\n        this.parsed = true;\n    }\n}\nexports.BinMsg = BinMsg;\n//# sourceMappingURL=commands.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.decompress = exports.compress = exports.uncompressibleCommands = exports.Compressor = void 0;\nconst zlib = require(\"zlib\");\nconst deps_1 = require(\"../../deps\");\nconst error_1 = require(\"../../error\");\n/** @public */\nexports.Compressor = Object.freeze({\n    none: 0,\n    snappy: 1,\n    zlib: 2\n});\nexports.uncompressibleCommands = new Set([\n    'ismaster',\n    'saslStart',\n    'saslContinue',\n    'getnonce',\n    'authenticate',\n    'createUser',\n    'updateUser',\n    'copydbSaslStart',\n    'copydbgetnonce',\n    'copydb'\n]);\n// Facilitate compressing a message using an agreed compressor\nfunction compress(self, dataToBeCompressed, callback) {\n    const zlibOptions = {};\n    switch (self.options.agreedCompressor) {\n        case 'snappy': {\n            if ('kModuleError' in deps_1.Snappy) {\n                return callback(deps_1.Snappy['kModuleError']);\n            }\n            if (deps_1.Snappy[deps_1.PKG_VERSION].major <= 6) {\n                deps_1.Snappy.compress(dataToBeCompressed, callback);\n            }\n            else {\n                deps_1.Snappy.compress(dataToBeCompressed)\n                    .then(buffer => callback(undefined, buffer))\n                    .catch(error => callback(error));\n            }\n            break;\n        }\n        case 'zlib':\n            // Determine zlibCompressionLevel\n            if (self.options.zlibCompressionLevel) {\n                zlibOptions.level = self.options.zlibCompressionLevel;\n            }\n            zlib.deflate(dataToBeCompressed, zlibOptions, callback);\n            break;\n        default:\n            throw new error_1.MongoInvalidArgumentError(`Unknown compressor ${self.options.agreedCompressor} failed to compress`);\n    }\n}\nexports.compress = compress;\n// Decompress a message using the given compressor\nfunction decompress(compressorID, compressedData, callback) {\n    if (compressorID < 0 || compressorID > Math.max(2)) {\n        throw new error_1.MongoDecompressionError(`Server sent message compressed using an unsupported compressor. (Received compressor ID ${compressorID})`);\n    }\n    switch (compressorID) {\n        case exports.Compressor.snappy: {\n            if ('kModuleError' in deps_1.Snappy) {\n                return callback(deps_1.Snappy['kModuleError']);\n            }\n            if (deps_1.Snappy[deps_1.PKG_VERSION].major <= 6) {\n                deps_1.Snappy.uncompress(compressedData, { asBuffer: true }, callback);\n            }\n            else {\n                deps_1.Snappy.uncompress(compressedData, { asBuffer: true })\n                    .then(buffer => callback(undefined, buffer))\n                    .catch(error => callback(error));\n            }\n            break;\n        }\n        case exports.Compressor.zlib:\n            zlib.inflate(compressedData, callback);\n            break;\n        default:\n            callback(undefined, compressedData);\n    }\n}\nexports.decompress = decompress;\n//# sourceMappingURL=compression.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.StreamDescription = void 0;\nconst server_description_1 = require(\"../sdam/server_description\");\nconst common_1 = require(\"../sdam/common\");\nconst RESPONSE_FIELDS = [\n    'minWireVersion',\n    'maxWireVersion',\n    'maxBsonObjectSize',\n    'maxMessageSizeBytes',\n    'maxWriteBatchSize',\n    'logicalSessionTimeoutMinutes'\n];\n/** @public */\nclass StreamDescription {\n    constructor(address, options) {\n        this.address = address;\n        this.type = common_1.ServerType.Unknown;\n        this.minWireVersion = undefined;\n        this.maxWireVersion = undefined;\n        this.maxBsonObjectSize = 16777216;\n        this.maxMessageSizeBytes = 48000000;\n        this.maxWriteBatchSize = 100000;\n        this.logicalSessionTimeoutMinutes = options === null || options === void 0 ? void 0 : options.logicalSessionTimeoutMinutes;\n        this.loadBalanced = !!(options === null || options === void 0 ? void 0 : options.loadBalanced);\n        this.compressors =\n            options && options.compressors && Array.isArray(options.compressors)\n                ? options.compressors\n                : [];\n    }\n    receiveResponse(response) {\n        this.type = (0, server_description_1.parseServerType)(response);\n        for (const field of RESPONSE_FIELDS) {\n            if (response[field] != null) {\n                this[field] = response[field];\n            }\n            // testing case\n            if ('__nodejs_mock_server__' in response) {\n                this.__nodejs_mock_server__ = response['__nodejs_mock_server__'];\n            }\n        }\n        if (response.compression) {\n            this.compressor = this.compressors.filter(c => { var _a; return (_a = response.compression) === null || _a === void 0 ? void 0 : _a.includes(c); })[0];\n        }\n    }\n}\nexports.StreamDescription = StreamDescription;\n//# sourceMappingURL=stream_description.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.CommandFailedEvent = exports.CommandSucceededEvent = exports.CommandStartedEvent = void 0;\nconst commands_1 = require(\"./commands\");\nconst utils_1 = require(\"../utils\");\n/**\n * An event indicating the start of a given\n * @public\n * @category Event\n */\nclass CommandStartedEvent {\n    /**\n     * Create a started event\n     *\n     * @internal\n     * @param pool - the pool that originated the command\n     * @param command - the command\n     */\n    constructor(connection, command) {\n        const cmd = extractCommand(command);\n        const commandName = extractCommandName(cmd);\n        const { address, connectionId, serviceId } = extractConnectionDetails(connection);\n        // TODO: remove in major revision, this is not spec behavior\n        if (SENSITIVE_COMMANDS.has(commandName)) {\n            this.commandObj = {};\n            this.commandObj[commandName] = true;\n        }\n        this.address = address;\n        this.connectionId = connectionId;\n        this.serviceId = serviceId;\n        this.requestId = command.requestId;\n        this.databaseName = databaseName(command);\n        this.commandName = commandName;\n        this.command = maybeRedact(commandName, cmd, cmd);\n    }\n    /* @internal */\n    get hasServiceId() {\n        return !!this.serviceId;\n    }\n}\nexports.CommandStartedEvent = CommandStartedEvent;\n/**\n * An event indicating the success of a given command\n * @public\n * @category Event\n */\nclass CommandSucceededEvent {\n    /**\n     * Create a succeeded event\n     *\n     * @internal\n     * @param pool - the pool that originated the command\n     * @param command - the command\n     * @param reply - the reply for this command from the server\n     * @param started - a high resolution tuple timestamp of when the command was first sent, to calculate duration\n     */\n    constructor(connection, command, reply, started) {\n        const cmd = extractCommand(command);\n        const commandName = extractCommandName(cmd);\n        const { address, connectionId, serviceId } = extractConnectionDetails(connection);\n        this.address = address;\n        this.connectionId = connectionId;\n        this.serviceId = serviceId;\n        this.requestId = command.requestId;\n        this.commandName = commandName;\n        this.duration = (0, utils_1.calculateDurationInMs)(started);\n        this.reply = maybeRedact(commandName, cmd, extractReply(command, reply));\n    }\n    /* @internal */\n    get hasServiceId() {\n        return !!this.serviceId;\n    }\n}\nexports.CommandSucceededEvent = CommandSucceededEvent;\n/**\n * An event indicating the failure of a given command\n * @public\n * @category Event\n */\nclass CommandFailedEvent {\n    /**\n     * Create a failure event\n     *\n     * @internal\n     * @param pool - the pool that originated the command\n     * @param command - the command\n     * @param error - the generated error or a server error response\n     * @param started - a high resolution tuple timestamp of when the command was first sent, to calculate duration\n     */\n    constructor(connection, command, error, started) {\n        const cmd = extractCommand(command);\n        const commandName = extractCommandName(cmd);\n        const { address, connectionId, serviceId } = extractConnectionDetails(connection);\n        this.address = address;\n        this.connectionId = connectionId;\n        this.serviceId = serviceId;\n        this.requestId = command.requestId;\n        this.commandName = commandName;\n        this.duration = (0, utils_1.calculateDurationInMs)(started);\n        this.failure = maybeRedact(commandName, cmd, error);\n    }\n    /* @internal */\n    get hasServiceId() {\n        return !!this.serviceId;\n    }\n}\nexports.CommandFailedEvent = CommandFailedEvent;\n/** Commands that we want to redact because of the sensitive nature of their contents */\nconst SENSITIVE_COMMANDS = new Set([\n    'authenticate',\n    'saslStart',\n    'saslContinue',\n    'getnonce',\n    'createUser',\n    'updateUser',\n    'copydbgetnonce',\n    'copydbsaslstart',\n    'copydb'\n]);\nconst HELLO_COMMANDS = new Set(['hello', 'ismaster', 'isMaster']);\n// helper methods\nconst extractCommandName = (commandDoc) => Object.keys(commandDoc)[0];\nconst namespace = (command) => command.ns;\nconst databaseName = (command) => command.ns.split('.')[0];\nconst collectionName = (command) => command.ns.split('.')[1];\nconst maybeRedact = (commandName, commandDoc, result) => SENSITIVE_COMMANDS.has(commandName) ||\n    (HELLO_COMMANDS.has(commandName) && commandDoc.speculativeAuthenticate)\n    ? {}\n    : result;\nconst LEGACY_FIND_QUERY_MAP = {\n    $query: 'filter',\n    $orderby: 'sort',\n    $hint: 'hint',\n    $comment: 'comment',\n    $maxScan: 'maxScan',\n    $max: 'max',\n    $min: 'min',\n    $returnKey: 'returnKey',\n    $showDiskLoc: 'showRecordId',\n    $maxTimeMS: 'maxTimeMS',\n    $snapshot: 'snapshot'\n};\nconst LEGACY_FIND_OPTIONS_MAP = {\n    numberToSkip: 'skip',\n    numberToReturn: 'batchSize',\n    returnFieldSelector: 'projection'\n};\nconst OP_QUERY_KEYS = [\n    'tailable',\n    'oplogReplay',\n    'noCursorTimeout',\n    'awaitData',\n    'partial',\n    'exhaust'\n];\n/** Extract the actual command from the query, possibly up-converting if it's a legacy format */\nfunction extractCommand(command) {\n    var _a;\n    if (command instanceof commands_1.GetMore) {\n        return {\n            getMore: (0, utils_1.deepCopy)(command.cursorId),\n            collection: collectionName(command),\n            batchSize: command.numberToReturn\n        };\n    }\n    if (command instanceof commands_1.KillCursor) {\n        return {\n            killCursors: collectionName(command),\n            cursors: (0, utils_1.deepCopy)(command.cursorIds)\n        };\n    }\n    if (command instanceof commands_1.Msg) {\n        return (0, utils_1.deepCopy)(command.command);\n    }\n    if ((_a = command.query) === null || _a === void 0 ? void 0 : _a.$query) {\n        let result;\n        if (command.ns === 'admin.$cmd') {\n            // up-convert legacy command\n            result = Object.assign({}, command.query.$query);\n        }\n        else {\n            // up-convert legacy find command\n            result = { find: collectionName(command) };\n            Object.keys(LEGACY_FIND_QUERY_MAP).forEach(key => {\n                if (command.query[key] != null) {\n                    result[LEGACY_FIND_QUERY_MAP[key]] = (0, utils_1.deepCopy)(command.query[key]);\n                }\n            });\n        }\n        Object.keys(LEGACY_FIND_OPTIONS_MAP).forEach(key => {\n            const legacyKey = key;\n            if (command[legacyKey] != null) {\n                result[LEGACY_FIND_OPTIONS_MAP[legacyKey]] = (0, utils_1.deepCopy)(command[legacyKey]);\n            }\n        });\n        OP_QUERY_KEYS.forEach(key => {\n            const opKey = key;\n            if (command[opKey]) {\n                result[opKey] = command[opKey];\n            }\n        });\n        if (command.pre32Limit != null) {\n            result.limit = command.pre32Limit;\n        }\n        if (command.query.$explain) {\n            return { explain: result };\n        }\n        return result;\n    }\n    const clonedQuery = {};\n    const clonedCommand = {};\n    if (command.query) {\n        for (const k in command.query) {\n            clonedQuery[k] = (0, utils_1.deepCopy)(command.query[k]);\n        }\n        clonedCommand.query = clonedQuery;\n    }\n    for (const k in command) {\n        if (k === 'query')\n            continue;\n        clonedCommand[k] = (0, utils_1.deepCopy)(command[k]);\n    }\n    return command.query ? clonedQuery : clonedCommand;\n}\nfunction extractReply(command, reply) {\n    if (command instanceof commands_1.KillCursor) {\n        return {\n            ok: 1,\n            cursorsUnknown: command.cursorIds\n        };\n    }\n    if (!reply) {\n        return reply;\n    }\n    if (command instanceof commands_1.GetMore) {\n        return {\n            ok: 1,\n            cursor: {\n                id: (0, utils_1.deepCopy)(reply.cursorId),\n                ns: namespace(command),\n                nextBatch: (0, utils_1.deepCopy)(reply.documents)\n            }\n        };\n    }\n    if (command instanceof commands_1.Msg) {\n        return (0, utils_1.deepCopy)(reply.result ? reply.result : reply);\n    }\n    // is this a legacy find command?\n    if (command.query && command.query.$query != null) {\n        return {\n            ok: 1,\n            cursor: {\n                id: (0, utils_1.deepCopy)(reply.cursorId),\n                ns: namespace(command),\n                firstBatch: (0, utils_1.deepCopy)(reply.documents)\n            }\n        };\n    }\n    return (0, utils_1.deepCopy)(reply.result ? reply.result : reply);\n}\nfunction extractConnectionDetails(connection) {\n    let connectionId;\n    if ('id' in connection) {\n        connectionId = connection.id;\n    }\n    return {\n        address: connection.address,\n        serviceId: connection.serviceId,\n        connectionId\n    };\n}\n//# sourceMappingURL=command_monitoring_events.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.CancellationToken = exports.TypedEventEmitter = exports.BSONType = void 0;\nconst events_1 = require(\"events\");\n/** @public */\nexports.BSONType = Object.freeze({\n    double: 1,\n    string: 2,\n    object: 3,\n    array: 4,\n    binData: 5,\n    undefined: 6,\n    objectId: 7,\n    bool: 8,\n    date: 9,\n    null: 10,\n    regex: 11,\n    dbPointer: 12,\n    javascript: 13,\n    symbol: 14,\n    javascriptWithScope: 15,\n    int: 16,\n    timestamp: 17,\n    long: 18,\n    decimal: 19,\n    minKey: -1,\n    maxKey: 127\n});\n/**\n * Typescript type safe event emitter\n * @public\n */\n// eslint-disable-next-line @typescript-eslint/no-unused-vars\nclass TypedEventEmitter extends events_1.EventEmitter {\n}\nexports.TypedEventEmitter = TypedEventEmitter;\n/** @public */\nclass CancellationToken extends TypedEventEmitter {\n}\nexports.CancellationToken = CancellationToken;\n//# sourceMappingURL=mongo_types.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.ConnectionPoolMetrics = void 0;\n/** @internal */\nclass ConnectionPoolMetrics {\n    constructor() {\n        this.txnConnections = 0;\n        this.cursorConnections = 0;\n        this.otherConnections = 0;\n    }\n    /**\n     * Mark a connection as pinned for a specific operation.\n     */\n    markPinned(pinType) {\n        if (pinType === ConnectionPoolMetrics.TXN) {\n            this.txnConnections += 1;\n        }\n        else if (pinType === ConnectionPoolMetrics.CURSOR) {\n            this.cursorConnections += 1;\n        }\n        else {\n            this.otherConnections += 1;\n        }\n    }\n    /**\n     * Unmark a connection as pinned for an operation.\n     */\n    markUnpinned(pinType) {\n        if (pinType === ConnectionPoolMetrics.TXN) {\n            this.txnConnections -= 1;\n        }\n        else if (pinType === ConnectionPoolMetrics.CURSOR) {\n            this.cursorConnections -= 1;\n        }\n        else {\n            this.otherConnections -= 1;\n        }\n    }\n    /**\n     * Return information about the cmap metrics as a string.\n     */\n    info(maxPoolSize) {\n        return ('Timed out while checking out a connection from connection pool: ' +\n            `maxPoolSize: ${maxPoolSize}, ` +\n            `connections in use by cursors: ${this.cursorConnections}, ` +\n            `connections in use by transactions: ${this.txnConnections}, ` +\n            `connections in use by other operations: ${this.otherConnections}`);\n    }\n    /**\n     * Reset the metrics to the initial values.\n     */\n    reset() {\n        this.txnConnections = 0;\n        this.cursorConnections = 0;\n        this.otherConnections = 0;\n    }\n}\nexports.ConnectionPoolMetrics = ConnectionPoolMetrics;\nConnectionPoolMetrics.TXN = 'txn';\nConnectionPoolMetrics.CURSOR = 'cursor';\nConnectionPoolMetrics.OTHER = 'other';\n//# sourceMappingURL=metrics.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.GetMoreOperation = void 0;\nconst error_1 = require(\"../error\");\nconst operation_1 = require(\"./operation\");\n/** @internal */\nclass GetMoreOperation extends operation_1.AbstractOperation {\n    constructor(ns, cursorId, server, options = {}) {\n        super(options);\n        this.options = options;\n        this.ns = ns;\n        this.cursorId = cursorId;\n        this.server = server;\n    }\n    /**\n     * Although there is a server already associated with the get more operation, the signature\n     * for execute passes a server so we will just use that one.\n     */\n    execute(server, session, callback) {\n        if (server !== this.server) {\n            return callback(new error_1.MongoRuntimeError('Getmore must run on the same server operation began on'));\n        }\n        server.getMore(this.ns, this.cursorId, this.options, callback);\n    }\n}\nexports.GetMoreOperation = GetMoreOperation;\n(0, operation_1.defineAspects)(GetMoreOperation, [operation_1.Aspect.READ_OPERATION, operation_1.Aspect.CURSOR_ITERATING]);\n//# sourceMappingURL=get_more.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.AggregationCursor = void 0;\nconst aggregate_1 = require(\"../operations/aggregate\");\nconst abstract_cursor_1 = require(\"./abstract_cursor\");\nconst execute_operation_1 = require(\"../operations/execute_operation\");\nconst utils_1 = require(\"../utils\");\n/** @internal */\nconst kPipeline = Symbol('pipeline');\n/** @internal */\nconst kOptions = Symbol('options');\n/**\n * The **AggregationCursor** class is an internal class that embodies an aggregation cursor on MongoDB\n * allowing for iteration over the results returned from the underlying query. It supports\n * one by one document iteration, conversion to an array or can be iterated as a Node 4.X\n * or higher stream\n * @public\n */\nclass AggregationCursor extends abstract_cursor_1.AbstractCursor {\n    /** @internal */\n    constructor(topology, namespace, pipeline = [], options = {}) {\n        super(topology, namespace, options);\n        this[kPipeline] = pipeline;\n        this[kOptions] = options;\n    }\n    get pipeline() {\n        return this[kPipeline];\n    }\n    clone() {\n        const clonedOptions = (0, utils_1.mergeOptions)({}, this[kOptions]);\n        delete clonedOptions.session;\n        return new AggregationCursor(this.topology, this.namespace, this[kPipeline], {\n            ...clonedOptions\n        });\n    }\n    map(transform) {\n        return super.map(transform);\n    }\n    /** @internal */\n    _initialize(session, callback) {\n        const aggregateOperation = new aggregate_1.AggregateOperation(this.namespace, this[kPipeline], {\n            ...this[kOptions],\n            ...this.cursorOptions,\n            session\n        });\n        (0, execute_operation_1.executeOperation)(this.topology, aggregateOperation, (err, response) => {\n            if (err || response == null)\n                return callback(err);\n            // TODO: NODE-2882\n            callback(undefined, { server: aggregateOperation.server, session, response });\n        });\n    }\n    explain(verbosity, callback) {\n        if (typeof verbosity === 'function')\n            (callback = verbosity), (verbosity = true);\n        if (verbosity == null)\n            verbosity = true;\n        return (0, execute_operation_1.executeOperation)(this.topology, new aggregate_1.AggregateOperation(this.namespace, this[kPipeline], {\n            ...this[kOptions],\n            ...this.cursorOptions,\n            explain: verbosity\n        }), callback);\n    }\n    group($group) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        this[kPipeline].push({ $group });\n        return this;\n    }\n    /** Add a limit stage to the aggregation pipeline */\n    limit($limit) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        this[kPipeline].push({ $limit });\n        return this;\n    }\n    /** Add a match stage to the aggregation pipeline */\n    match($match) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        this[kPipeline].push({ $match });\n        return this;\n    }\n    /** Add an out stage to the aggregation pipeline */\n    out($out) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        this[kPipeline].push({ $out });\n        return this;\n    }\n    /**\n     * Add a project stage to the aggregation pipeline\n     *\n     * @remarks\n     * In order to strictly type this function you must provide an interface\n     * that represents the effect of your projection on the result documents.\n     *\n     * By default chaining a projection to your cursor changes the returned type to the generic {@link Document} type.\n     * You should specify a parameterized type to have assertions on your final results.\n     *\n     * @example\n     * ```typescript\n     * // Best way\n     * const docs: AggregationCursor<{ a: number }> = cursor.project<{ a: number }>({ _id: 0, a: true });\n     * // Flexible way\n     * const docs: AggregationCursor<Document> = cursor.project({ _id: 0, a: true });\n     * ```\n     *\n     * @remarks\n     * In order to strictly type this function you must provide an interface\n     * that represents the effect of your projection on the result documents.\n     *\n     * **Note for Typescript Users:** adding a transform changes the return type of the iteration of this cursor,\n     * it **does not** return a new instance of a cursor. This means when calling project,\n     * you should always assign the result to a new variable in order to get a correctly typed cursor variable.\n     * Take note of the following example:\n     *\n     * @example\n     * ```typescript\n     * const cursor: AggregationCursor<{ a: number; b: string }> = coll.aggregate([]);\n     * const projectCursor = cursor.project<{ a: number }>({ _id: 0, a: true });\n     * const aPropOnlyArray: {a: number}[] = await projectCursor.toArray();\n     *\n     * // or always use chaining and save the final cursor\n     *\n     * const cursor = coll.aggregate().project<{ a: string }>({\n     *   _id: 0,\n     *   a: { $convert: { input: '$a', to: 'string' }\n     * }});\n     * ```\n     */\n    project($project) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        this[kPipeline].push({ $project });\n        return this;\n    }\n    /** Add a lookup stage to the aggregation pipeline */\n    lookup($lookup) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        this[kPipeline].push({ $lookup });\n        return this;\n    }\n    /** Add a redact stage to the aggregation pipeline */\n    redact($redact) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        this[kPipeline].push({ $redact });\n        return this;\n    }\n    /** Add a skip stage to the aggregation pipeline */\n    skip($skip) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        this[kPipeline].push({ $skip });\n        return this;\n    }\n    /** Add a sort stage to the aggregation pipeline */\n    sort($sort) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        this[kPipeline].push({ $sort });\n        return this;\n    }\n    /** Add a unwind stage to the aggregation pipeline */\n    unwind($unwind) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        this[kPipeline].push({ $unwind });\n        return this;\n    }\n    // deprecated methods\n    /** @deprecated Add a geoNear stage to the aggregation pipeline */\n    geoNear($geoNear) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        this[kPipeline].push({ $geoNear });\n        return this;\n    }\n}\nexports.AggregationCursor = AggregationCursor;\n//# sourceMappingURL=aggregation_cursor.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.AggregateOperation = exports.DB_AGGREGATE_COLLECTION = void 0;\nconst command_1 = require(\"./command\");\nconst error_1 = require(\"../error\");\nconst utils_1 = require(\"../utils\");\nconst operation_1 = require(\"./operation\");\n/** @internal */\nexports.DB_AGGREGATE_COLLECTION = 1;\nconst MIN_WIRE_VERSION_$OUT_READ_CONCERN_SUPPORT = 8;\n/** @internal */\nclass AggregateOperation extends command_1.CommandOperation {\n    constructor(ns, pipeline, options) {\n        super(undefined, { ...options, dbName: ns.db });\n        this.options = options !== null && options !== void 0 ? options : {};\n        // Covers when ns.collection is null, undefined or the empty string, use DB_AGGREGATE_COLLECTION\n        this.target = ns.collection || exports.DB_AGGREGATE_COLLECTION;\n        this.pipeline = pipeline;\n        // determine if we have a write stage, override read preference if so\n        this.hasWriteStage = false;\n        if (typeof (options === null || options === void 0 ? void 0 : options.out) === 'string') {\n            this.pipeline = this.pipeline.concat({ $out: options.out });\n            this.hasWriteStage = true;\n        }\n        else if (pipeline.length > 0) {\n            const finalStage = pipeline[pipeline.length - 1];\n            if (finalStage.$out || finalStage.$merge) {\n                this.hasWriteStage = true;\n            }\n        }\n        if (this.hasWriteStage) {\n            this.trySecondaryWrite = true;\n        }\n        if (this.explain && this.writeConcern) {\n            throw new error_1.MongoInvalidArgumentError('Option \"explain\" cannot be used on an aggregate call with writeConcern');\n        }\n        if ((options === null || options === void 0 ? void 0 : options.cursor) != null && typeof options.cursor !== 'object') {\n            throw new error_1.MongoInvalidArgumentError('Cursor options must be an object');\n        }\n    }\n    get canRetryRead() {\n        return !this.hasWriteStage;\n    }\n    addToPipeline(stage) {\n        this.pipeline.push(stage);\n    }\n    execute(server, session, callback) {\n        const options = this.options;\n        const serverWireVersion = (0, utils_1.maxWireVersion)(server);\n        const command = { aggregate: this.target, pipeline: this.pipeline };\n        if (this.hasWriteStage && serverWireVersion < MIN_WIRE_VERSION_$OUT_READ_CONCERN_SUPPORT) {\n            this.readConcern = undefined;\n        }\n        if (serverWireVersion >= 5) {\n            if (this.hasWriteStage && this.writeConcern) {\n                Object.assign(command, { writeConcern: this.writeConcern });\n            }\n        }\n        if (options.bypassDocumentValidation === true) {\n            command.bypassDocumentValidation = options.bypassDocumentValidation;\n        }\n        if (typeof options.allowDiskUse === 'boolean') {\n            command.allowDiskUse = options.allowDiskUse;\n        }\n        if (options.hint) {\n            command.hint = options.hint;\n        }\n        if (options.let) {\n            command.let = options.let;\n        }\n        command.cursor = options.cursor || {};\n        if (options.batchSize && !this.hasWriteStage) {\n            command.cursor.batchSize = options.batchSize;\n        }\n        super.executeCommand(server, session, command, callback);\n    }\n}\nexports.AggregateOperation = AggregateOperation;\n(0, operation_1.defineAspects)(AggregateOperation, [\n    operation_1.Aspect.READ_OPERATION,\n    operation_1.Aspect.RETRYABLE,\n    operation_1.Aspect.EXPLAINABLE,\n    operation_1.Aspect.CURSOR_CREATING\n]);\n//# sourceMappingURL=aggregate.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.FindCursor = exports.FLAGS = void 0;\nconst error_1 = require(\"../error\");\nconst count_1 = require(\"../operations/count\");\nconst execute_operation_1 = require(\"../operations/execute_operation\");\nconst find_1 = require(\"../operations/find\");\nconst utils_1 = require(\"../utils\");\nconst sort_1 = require(\"../sort\");\nconst abstract_cursor_1 = require(\"./abstract_cursor\");\n/** @internal */\nconst kFilter = Symbol('filter');\n/** @internal */\nconst kNumReturned = Symbol('numReturned');\n/** @internal */\nconst kBuiltOptions = Symbol('builtOptions');\n/** @public Flags allowed for cursor */\nexports.FLAGS = [\n    'tailable',\n    'oplogReplay',\n    'noCursorTimeout',\n    'awaitData',\n    'exhaust',\n    'partial'\n];\n/** @public */\nclass FindCursor extends abstract_cursor_1.AbstractCursor {\n    /** @internal */\n    constructor(topology, namespace, filter, options = {}) {\n        super(topology, namespace, options);\n        this[kFilter] = filter || {};\n        this[kBuiltOptions] = options;\n        if (options.sort != null) {\n            this[kBuiltOptions].sort = (0, sort_1.formatSort)(options.sort);\n        }\n    }\n    clone() {\n        const clonedOptions = (0, utils_1.mergeOptions)({}, this[kBuiltOptions]);\n        delete clonedOptions.session;\n        return new FindCursor(this.topology, this.namespace, this[kFilter], {\n            ...clonedOptions\n        });\n    }\n    map(transform) {\n        return super.map(transform);\n    }\n    /** @internal */\n    _initialize(session, callback) {\n        const findOperation = new find_1.FindOperation(undefined, this.namespace, this[kFilter], {\n            ...this[kBuiltOptions],\n            ...this.cursorOptions,\n            session\n        });\n        (0, execute_operation_1.executeOperation)(this.topology, findOperation, (err, response) => {\n            if (err || response == null)\n                return callback(err);\n            // TODO: We only need this for legacy queries that do not support `limit`, maybe\n            //       the value should only be saved in those cases.\n            if (response.cursor) {\n                this[kNumReturned] = response.cursor.firstBatch.length;\n            }\n            else {\n                this[kNumReturned] = response.documents ? response.documents.length : 0;\n            }\n            // TODO: NODE-2882\n            callback(undefined, { server: findOperation.server, session, response });\n        });\n    }\n    /** @internal */\n    _getMore(batchSize, callback) {\n        // NOTE: this is to support client provided limits in pre-command servers\n        const numReturned = this[kNumReturned];\n        if (numReturned) {\n            const limit = this[kBuiltOptions].limit;\n            batchSize =\n                limit && limit > 0 && numReturned + batchSize > limit ? limit - numReturned : batchSize;\n            if (batchSize <= 0) {\n                return this.close(callback);\n            }\n        }\n        super._getMore(batchSize, (err, response) => {\n            if (err)\n                return callback(err);\n            // TODO: wrap this in some logic to prevent it from happening if we don't need this support\n            if (response) {\n                this[kNumReturned] = this[kNumReturned] + response.cursor.nextBatch.length;\n            }\n            callback(undefined, response);\n        });\n    }\n    count(options, callback) {\n        if (typeof options === 'boolean') {\n            throw new error_1.MongoInvalidArgumentError('Invalid first parameter to count');\n        }\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = options !== null && options !== void 0 ? options : {};\n        return (0, execute_operation_1.executeOperation)(this.topology, new count_1.CountOperation(this.namespace, this[kFilter], {\n            ...this[kBuiltOptions],\n            ...this.cursorOptions,\n            ...options\n        }), callback);\n    }\n    explain(verbosity, callback) {\n        if (typeof verbosity === 'function')\n            (callback = verbosity), (verbosity = true);\n        if (verbosity == null)\n            verbosity = true;\n        return (0, execute_operation_1.executeOperation)(this.topology, new find_1.FindOperation(undefined, this.namespace, this[kFilter], {\n            ...this[kBuiltOptions],\n            ...this.cursorOptions,\n            explain: verbosity\n        }), callback);\n    }\n    /** Set the cursor query */\n    filter(filter) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        this[kFilter] = filter;\n        return this;\n    }\n    /**\n     * Set the cursor hint\n     *\n     * @param hint - If specified, then the query system will only consider plans using the hinted index.\n     */\n    hint(hint) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        this[kBuiltOptions].hint = hint;\n        return this;\n    }\n    /**\n     * Set the cursor min\n     *\n     * @param min - Specify a $min value to specify the inclusive lower bound for a specific index in order to constrain the results of find(). The $min specifies the lower bound for all keys of a specific index in order.\n     */\n    min(min) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        this[kBuiltOptions].min = min;\n        return this;\n    }\n    /**\n     * Set the cursor max\n     *\n     * @param max - Specify a $max value to specify the exclusive upper bound for a specific index in order to constrain the results of find(). The $max specifies the upper bound for all keys of a specific index in order.\n     */\n    max(max) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        this[kBuiltOptions].max = max;\n        return this;\n    }\n    /**\n     * Set the cursor returnKey.\n     * If set to true, modifies the cursor to only return the index field or fields for the results of the query, rather than documents.\n     * If set to true and the query does not use an index to perform the read operation, the returned documents will not contain any fields.\n     *\n     * @param value - the returnKey value.\n     */\n    returnKey(value) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        this[kBuiltOptions].returnKey = value;\n        return this;\n    }\n    /**\n     * Modifies the output of a query by adding a field $recordId to matching documents. $recordId is the internal key which uniquely identifies a document in a collection.\n     *\n     * @param value - The $showDiskLoc option has now been deprecated and replaced with the showRecordId field. $showDiskLoc will still be accepted for OP_QUERY stye find.\n     */\n    showRecordId(value) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        this[kBuiltOptions].showRecordId = value;\n        return this;\n    }\n    /**\n     * Add a query modifier to the cursor query\n     *\n     * @param name - The query modifier (must start with $, such as $orderby etc)\n     * @param value - The modifier value.\n     */\n    addQueryModifier(name, value) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        if (name[0] !== '$') {\n            throw new error_1.MongoInvalidArgumentError(`${name} is not a valid query modifier`);\n        }\n        // Strip of the $\n        const field = name.substr(1);\n        // NOTE: consider some TS magic for this\n        switch (field) {\n            case 'comment':\n                this[kBuiltOptions].comment = value;\n                break;\n            case 'explain':\n                this[kBuiltOptions].explain = value;\n                break;\n            case 'hint':\n                this[kBuiltOptions].hint = value;\n                break;\n            case 'max':\n                this[kBuiltOptions].max = value;\n                break;\n            case 'maxTimeMS':\n                this[kBuiltOptions].maxTimeMS = value;\n                break;\n            case 'min':\n                this[kBuiltOptions].min = value;\n                break;\n            case 'orderby':\n                this[kBuiltOptions].sort = (0, sort_1.formatSort)(value);\n                break;\n            case 'query':\n                this[kFilter] = value;\n                break;\n            case 'returnKey':\n                this[kBuiltOptions].returnKey = value;\n                break;\n            case 'showDiskLoc':\n                this[kBuiltOptions].showRecordId = value;\n                break;\n            default:\n                throw new error_1.MongoInvalidArgumentError(`Invalid query modifier: ${name}`);\n        }\n        return this;\n    }\n    /**\n     * Add a comment to the cursor query allowing for tracking the comment in the log.\n     *\n     * @param value - The comment attached to this query.\n     */\n    comment(value) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        this[kBuiltOptions].comment = value;\n        return this;\n    }\n    /**\n     * Set a maxAwaitTimeMS on a tailing cursor query to allow to customize the timeout value for the option awaitData (Only supported on MongoDB 3.2 or higher, ignored otherwise)\n     *\n     * @param value - Number of milliseconds to wait before aborting the tailed query.\n     */\n    maxAwaitTimeMS(value) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        if (typeof value !== 'number') {\n            throw new error_1.MongoInvalidArgumentError('Argument for maxAwaitTimeMS must be a number');\n        }\n        this[kBuiltOptions].maxAwaitTimeMS = value;\n        return this;\n    }\n    /**\n     * Set a maxTimeMS on the cursor query, allowing for hard timeout limits on queries (Only supported on MongoDB 2.6 or higher)\n     *\n     * @param value - Number of milliseconds to wait before aborting the query.\n     */\n    maxTimeMS(value) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        if (typeof value !== 'number') {\n            throw new error_1.MongoInvalidArgumentError('Argument for maxTimeMS must be a number');\n        }\n        this[kBuiltOptions].maxTimeMS = value;\n        return this;\n    }\n    /**\n     * Add a project stage to the aggregation pipeline\n     *\n     * @remarks\n     * In order to strictly type this function you must provide an interface\n     * that represents the effect of your projection on the result documents.\n     *\n     * By default chaining a projection to your cursor changes the returned type to the generic\n     * {@link Document} type.\n     * You should specify a parameterized type to have assertions on your final results.\n     *\n     * @example\n     * ```typescript\n     * // Best way\n     * const docs: FindCursor<{ a: number }> = cursor.project<{ a: number }>({ _id: 0, a: true });\n     * // Flexible way\n     * const docs: FindCursor<Document> = cursor.project({ _id: 0, a: true });\n     * ```\n     *\n     * @remarks\n     *\n     * **Note for Typescript Users:** adding a transform changes the return type of the iteration of this cursor,\n     * it **does not** return a new instance of a cursor. This means when calling project,\n     * you should always assign the result to a new variable in order to get a correctly typed cursor variable.\n     * Take note of the following example:\n     *\n     * @example\n     * ```typescript\n     * const cursor: FindCursor<{ a: number; b: string }> = coll.find();\n     * const projectCursor = cursor.project<{ a: number }>({ _id: 0, a: true });\n     * const aPropOnlyArray: {a: number}[] = await projectCursor.toArray();\n     *\n     * // or always use chaining and save the final cursor\n     *\n     * const cursor = coll.find().project<{ a: string }>({\n     *   _id: 0,\n     *   a: { $convert: { input: '$a', to: 'string' }\n     * }});\n     * ```\n     */\n    project(value) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        this[kBuiltOptions].projection = value;\n        return this;\n    }\n    /**\n     * Sets the sort order of the cursor query.\n     *\n     * @param sort - The key or keys set for the sort.\n     * @param direction - The direction of the sorting (1 or -1).\n     */\n    sort(sort, direction) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        if (this[kBuiltOptions].tailable) {\n            throw new error_1.MongoTailableCursorError('Tailable cursor does not support sorting');\n        }\n        this[kBuiltOptions].sort = (0, sort_1.formatSort)(sort, direction);\n        return this;\n    }\n    /**\n     * Allows disk use for blocking sort operations exceeding 100MB memory. (MongoDB 3.2 or higher)\n     *\n     * @remarks\n     * {@link https://docs.mongodb.com/manual/reference/command/find/#find-cmd-allowdiskuse | find command allowDiskUse documentation}\n     */\n    allowDiskUse() {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        if (!this[kBuiltOptions].sort) {\n            throw new error_1.MongoInvalidArgumentError('Option \"allowDiskUse\" requires a sort specification');\n        }\n        this[kBuiltOptions].allowDiskUse = true;\n        return this;\n    }\n    /**\n     * Set the collation options for the cursor.\n     *\n     * @param value - The cursor collation options (MongoDB 3.4 or higher) settings for update operation (see 3.4 documentation for available fields).\n     */\n    collation(value) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        this[kBuiltOptions].collation = value;\n        return this;\n    }\n    /**\n     * Set the limit for the cursor.\n     *\n     * @param value - The limit for the cursor query.\n     */\n    limit(value) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        if (this[kBuiltOptions].tailable) {\n            throw new error_1.MongoTailableCursorError('Tailable cursor does not support limit');\n        }\n        if (typeof value !== 'number') {\n            throw new error_1.MongoInvalidArgumentError('Operation \"limit\" requires an integer');\n        }\n        this[kBuiltOptions].limit = value;\n        return this;\n    }\n    /**\n     * Set the skip for the cursor.\n     *\n     * @param value - The skip for the cursor query.\n     */\n    skip(value) {\n        (0, abstract_cursor_1.assertUninitialized)(this);\n        if (this[kBuiltOptions].tailable) {\n            throw new error_1.MongoTailableCursorError('Tailable cursor does not support skip');\n        }\n        if (typeof value !== 'number') {\n            throw new error_1.MongoInvalidArgumentError('Operation \"skip\" requires an integer');\n        }\n        this[kBuiltOptions].skip = value;\n        return this;\n    }\n}\nexports.FindCursor = FindCursor;\n//# sourceMappingURL=find_cursor.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.CountOperation = void 0;\nconst operation_1 = require(\"./operation\");\nconst command_1 = require(\"./command\");\n/** @internal */\nclass CountOperation extends command_1.CommandOperation {\n    constructor(namespace, filter, options) {\n        super({ s: { namespace: namespace } }, options);\n        this.options = options;\n        this.collectionName = namespace.collection;\n        this.query = filter;\n    }\n    execute(server, session, callback) {\n        const options = this.options;\n        const cmd = {\n            count: this.collectionName,\n            query: this.query\n        };\n        if (typeof options.limit === 'number') {\n            cmd.limit = options.limit;\n        }\n        if (typeof options.skip === 'number') {\n            cmd.skip = options.skip;\n        }\n        if (options.hint != null) {\n            cmd.hint = options.hint;\n        }\n        if (typeof options.maxTimeMS === 'number') {\n            cmd.maxTimeMS = options.maxTimeMS;\n        }\n        super.executeCommand(server, session, cmd, (err, result) => {\n            callback(err, result ? result.n : 0);\n        });\n    }\n}\nexports.CountOperation = CountOperation;\n(0, operation_1.defineAspects)(CountOperation, [operation_1.Aspect.READ_OPERATION, operation_1.Aspect.RETRYABLE]);\n//# sourceMappingURL=count.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.FindOperation = void 0;\nconst operation_1 = require(\"./operation\");\nconst utils_1 = require(\"../utils\");\nconst error_1 = require(\"../error\");\nconst command_1 = require(\"./command\");\nconst sort_1 = require(\"../sort\");\nconst shared_1 = require(\"../cmap/wire_protocol/shared\");\nconst read_concern_1 = require(\"../read_concern\");\nconst SUPPORTS_WRITE_CONCERN_AND_COLLATION = 5;\n/** @internal */\nclass FindOperation extends command_1.CommandOperation {\n    constructor(collection, ns, filter = {}, options = {}) {\n        super(collection, options);\n        this.options = options;\n        this.ns = ns;\n        if (typeof filter !== 'object' || Array.isArray(filter)) {\n            throw new error_1.MongoInvalidArgumentError('Query filter must be a plain object or ObjectId');\n        }\n        // If the filter is a buffer, validate that is a valid BSON document\n        if (Buffer.isBuffer(filter)) {\n            const objectSize = filter[0] | (filter[1] << 8) | (filter[2] << 16) | (filter[3] << 24);\n            if (objectSize !== filter.length) {\n                throw new error_1.MongoInvalidArgumentError(`Query filter raw message size does not match message header size [${filter.length}] != [${objectSize}]`);\n            }\n        }\n        // special case passing in an ObjectId as a filter\n        this.filter = filter != null && filter._bsontype === 'ObjectID' ? { _id: filter } : filter;\n    }\n    execute(server, session, callback) {\n        this.server = server;\n        const serverWireVersion = (0, utils_1.maxWireVersion)(server);\n        const options = this.options;\n        if (options.allowDiskUse != null && serverWireVersion < 4) {\n            callback(new error_1.MongoCompatibilityError('Option \"allowDiskUse\" is not supported on MongoDB < 3.2'));\n            return;\n        }\n        if (options.collation && serverWireVersion < SUPPORTS_WRITE_CONCERN_AND_COLLATION) {\n            callback(new error_1.MongoCompatibilityError(`Server ${server.name}, which reports wire version ${serverWireVersion}, does not support collation`));\n            return;\n        }\n        if (serverWireVersion < 4) {\n            if (this.readConcern && this.readConcern.level !== 'local') {\n                callback(new error_1.MongoCompatibilityError(`Server find command does not support a readConcern level of ${this.readConcern.level}`));\n                return;\n            }\n            const findCommand = makeLegacyFindCommand(this.ns, this.filter, options);\n            if ((0, shared_1.isSharded)(server) && this.readPreference) {\n                findCommand.$readPreference = this.readPreference.toJSON();\n            }\n            server.query(this.ns, findCommand, {\n                ...this.options,\n                ...this.bsonOptions,\n                documentsReturnedIn: 'firstBatch',\n                readPreference: this.readPreference\n            }, callback);\n            return;\n        }\n        let findCommand = makeFindCommand(this.ns, this.filter, options);\n        if (this.explain) {\n            findCommand = (0, utils_1.decorateWithExplain)(findCommand, this.explain);\n        }\n        server.command(this.ns, findCommand, {\n            ...this.options,\n            ...this.bsonOptions,\n            documentsReturnedIn: 'firstBatch',\n            session\n        }, callback);\n    }\n}\nexports.FindOperation = FindOperation;\nfunction makeFindCommand(ns, filter, options) {\n    const findCommand = {\n        find: ns.collection,\n        filter\n    };\n    if (options.sort) {\n        findCommand.sort = (0, sort_1.formatSort)(options.sort);\n    }\n    if (options.projection) {\n        let projection = options.projection;\n        if (projection && Array.isArray(projection)) {\n            projection = projection.length\n                ? projection.reduce((result, field) => {\n                    result[field] = 1;\n                    return result;\n                }, {})\n                : { _id: 1 };\n        }\n        findCommand.projection = projection;\n    }\n    if (options.hint) {\n        findCommand.hint = (0, utils_1.normalizeHintField)(options.hint);\n    }\n    if (typeof options.skip === 'number') {\n        findCommand.skip = options.skip;\n    }\n    if (typeof options.limit === 'number') {\n        if (options.limit < 0) {\n            findCommand.limit = -options.limit;\n            findCommand.singleBatch = true;\n        }\n        else {\n            findCommand.limit = options.limit;\n        }\n    }\n    if (typeof options.batchSize === 'number') {\n        if (options.batchSize < 0) {\n            if (options.limit &&\n                options.limit !== 0 &&\n                Math.abs(options.batchSize) < Math.abs(options.limit)) {\n                findCommand.limit = -options.batchSize;\n            }\n            findCommand.singleBatch = true;\n        }\n        else {\n            findCommand.batchSize = options.batchSize;\n        }\n    }\n    if (typeof options.singleBatch === 'boolean') {\n        findCommand.singleBatch = options.singleBatch;\n    }\n    if (options.comment) {\n        findCommand.comment = options.comment;\n    }\n    if (typeof options.maxTimeMS === 'number') {\n        findCommand.maxTimeMS = options.maxTimeMS;\n    }\n    const readConcern = read_concern_1.ReadConcern.fromOptions(options);\n    if (readConcern) {\n        findCommand.readConcern = readConcern.toJSON();\n    }\n    if (options.max) {\n        findCommand.max = options.max;\n    }\n    if (options.min) {\n        findCommand.min = options.min;\n    }\n    if (typeof options.returnKey === 'boolean') {\n        findCommand.returnKey = options.returnKey;\n    }\n    if (typeof options.showRecordId === 'boolean') {\n        findCommand.showRecordId = options.showRecordId;\n    }\n    if (typeof options.tailable === 'boolean') {\n        findCommand.tailable = options.tailable;\n    }\n    if (typeof options.timeout === 'boolean') {\n        findCommand.noCursorTimeout = !options.timeout;\n    }\n    else if (typeof options.noCursorTimeout === 'boolean') {\n        findCommand.noCursorTimeout = options.noCursorTimeout;\n    }\n    if (typeof options.awaitData === 'boolean') {\n        findCommand.awaitData = options.awaitData;\n    }\n    if (typeof options.allowPartialResults === 'boolean') {\n        findCommand.allowPartialResults = options.allowPartialResults;\n    }\n    if (options.collation) {\n        findCommand.collation = options.collation;\n    }\n    if (typeof options.allowDiskUse === 'boolean') {\n        findCommand.allowDiskUse = options.allowDiskUse;\n    }\n    if (options.let) {\n        findCommand.let = options.let;\n    }\n    return findCommand;\n}\nfunction makeLegacyFindCommand(ns, filter, options) {\n    const findCommand = {\n        $query: filter\n    };\n    if (options.sort) {\n        findCommand.$orderby = (0, sort_1.formatSort)(options.sort);\n    }\n    if (options.hint) {\n        findCommand.$hint = (0, utils_1.normalizeHintField)(options.hint);\n    }\n    if (typeof options.returnKey === 'boolean') {\n        findCommand.$returnKey = options.returnKey;\n    }\n    if (options.max) {\n        findCommand.$max = options.max;\n    }\n    if (options.min) {\n        findCommand.$min = options.min;\n    }\n    if (typeof options.showRecordId === 'boolean') {\n        findCommand.$showDiskLoc = options.showRecordId;\n    }\n    if (options.comment) {\n        findCommand.$comment = options.comment;\n    }\n    if (typeof options.maxTimeMS === 'number') {\n        findCommand.$maxTimeMS = options.maxTimeMS;\n    }\n    if (options.explain != null) {\n        findCommand.$explain = true;\n    }\n    return findCommand;\n}\n(0, operation_1.defineAspects)(FindOperation, [\n    operation_1.Aspect.READ_OPERATION,\n    operation_1.Aspect.RETRYABLE,\n    operation_1.Aspect.EXPLAINABLE,\n    operation_1.Aspect.CURSOR_CREATING\n]);\n//# sourceMappingURL=find.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.formatSort = void 0;\nconst error_1 = require(\"./error\");\n/** @internal */\nfunction prepareDirection(direction = 1) {\n    const value = `${direction}`.toLowerCase();\n    if (isMeta(direction))\n        return direction;\n    switch (value) {\n        case 'ascending':\n        case 'asc':\n        case '1':\n            return 1;\n        case 'descending':\n        case 'desc':\n        case '-1':\n            return -1;\n        default:\n            throw new error_1.MongoInvalidArgumentError(`Invalid sort direction: ${JSON.stringify(direction)}`);\n    }\n}\n/** @internal */\nfunction isMeta(t) {\n    return typeof t === 'object' && t != null && '$meta' in t && typeof t.$meta === 'string';\n}\n/** @internal */\nfunction isPair(t) {\n    if (Array.isArray(t) && t.length === 2) {\n        try {\n            prepareDirection(t[1]);\n            return true;\n        }\n        catch (e) {\n            return false;\n        }\n    }\n    return false;\n}\nfunction isDeep(t) {\n    return Array.isArray(t) && Array.isArray(t[0]);\n}\nfunction isMap(t) {\n    return t instanceof Map && t.size > 0;\n}\n/** @internal */\nfunction pairToMap(v) {\n    return new Map([[`${v[0]}`, prepareDirection([v[1]])]]);\n}\n/** @internal */\nfunction deepToMap(t) {\n    const sortEntries = t.map(([k, v]) => [`${k}`, prepareDirection(v)]);\n    return new Map(sortEntries);\n}\n/** @internal */\nfunction stringsToMap(t) {\n    const sortEntries = t.map(key => [`${key}`, 1]);\n    return new Map(sortEntries);\n}\n/** @internal */\nfunction objectToMap(t) {\n    const sortEntries = Object.entries(t).map(([k, v]) => [\n        `${k}`,\n        prepareDirection(v)\n    ]);\n    return new Map(sortEntries);\n}\n/** @internal */\nfunction mapToMap(t) {\n    const sortEntries = Array.from(t).map(([k, v]) => [\n        `${k}`,\n        prepareDirection(v)\n    ]);\n    return new Map(sortEntries);\n}\n/** converts a Sort type into a type that is valid for the server (SortForCmd) */\nfunction formatSort(sort, direction) {\n    if (sort == null)\n        return undefined;\n    if (typeof sort === 'string')\n        return new Map([[sort, prepareDirection(direction)]]);\n    if (typeof sort !== 'object') {\n        throw new error_1.MongoInvalidArgumentError(`Invalid sort format: ${JSON.stringify(sort)} Sort must be a valid object`);\n    }\n    if (!Array.isArray(sort)) {\n        return isMap(sort) ? mapToMap(sort) : Object.keys(sort).length ? objectToMap(sort) : undefined;\n    }\n    if (!sort.length)\n        return undefined;\n    if (isDeep(sort))\n        return deepToMap(sort);\n    if (isPair(sort))\n        return pairToMap(sort);\n    return stringsToMap(sort);\n}\nexports.formatSort = formatSort;\n//# sourceMappingURL=sort.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.IndexInformationOperation = exports.IndexExistsOperation = exports.ListIndexesCursor = exports.ListIndexesOperation = exports.DropIndexesOperation = exports.DropIndexOperation = exports.EnsureIndexOperation = exports.CreateIndexOperation = exports.CreateIndexesOperation = exports.IndexesOperation = void 0;\nconst common_functions_1 = require(\"./common_functions\");\nconst operation_1 = require(\"./operation\");\nconst error_1 = require(\"../error\");\nconst utils_1 = require(\"../utils\");\nconst command_1 = require(\"./command\");\nconst read_preference_1 = require(\"../read_preference\");\nconst abstract_cursor_1 = require(\"../cursor/abstract_cursor\");\nconst execute_operation_1 = require(\"./execute_operation\");\nconst LIST_INDEXES_WIRE_VERSION = 3;\nconst VALID_INDEX_OPTIONS = new Set([\n    'background',\n    'unique',\n    'name',\n    'partialFilterExpression',\n    'sparse',\n    'hidden',\n    'expireAfterSeconds',\n    'storageEngine',\n    'collation',\n    'version',\n    // text indexes\n    'weights',\n    'default_language',\n    'language_override',\n    'textIndexVersion',\n    // 2d-sphere indexes\n    '2dsphereIndexVersion',\n    // 2d indexes\n    'bits',\n    'min',\n    'max',\n    // geoHaystack Indexes\n    'bucketSize',\n    // wildcard indexes\n    'wildcardProjection'\n]);\nfunction makeIndexSpec(indexSpec, options) {\n    const indexParameters = (0, utils_1.parseIndexOptions)(indexSpec);\n    // Generate the index name\n    const name = typeof options.name === 'string' ? options.name : indexParameters.name;\n    // Set up the index\n    const finalIndexSpec = { name, key: indexParameters.fieldHash };\n    // merge valid index options into the index spec\n    for (const optionName in options) {\n        if (VALID_INDEX_OPTIONS.has(optionName)) {\n            finalIndexSpec[optionName] = options[optionName];\n        }\n    }\n    return finalIndexSpec;\n}\n/** @internal */\nclass IndexesOperation extends operation_1.AbstractOperation {\n    constructor(collection, options) {\n        super(options);\n        this.options = options;\n        this.collection = collection;\n    }\n    execute(server, session, callback) {\n        const coll = this.collection;\n        const options = this.options;\n        (0, common_functions_1.indexInformation)(coll.s.db, coll.collectionName, { full: true, ...options, readPreference: this.readPreference, session }, callback);\n    }\n}\nexports.IndexesOperation = IndexesOperation;\n/** @internal */\nclass CreateIndexesOperation extends command_1.CommandOperation {\n    constructor(parent, collectionName, indexes, options) {\n        super(parent, options);\n        this.options = options !== null && options !== void 0 ? options : {};\n        this.collectionName = collectionName;\n        this.indexes = indexes;\n    }\n    execute(server, session, callback) {\n        const options = this.options;\n        const indexes = this.indexes;\n        const serverWireVersion = (0, utils_1.maxWireVersion)(server);\n        // Ensure we generate the correct name if the parameter is not set\n        for (let i = 0; i < indexes.length; i++) {\n            // Did the user pass in a collation, check if our write server supports it\n            if (indexes[i].collation && serverWireVersion < 5) {\n                callback(new error_1.MongoCompatibilityError(`Server ${server.name}, which reports wire version ${serverWireVersion}, ` +\n                    'does not support collation'));\n                return;\n            }\n            if (indexes[i].name == null) {\n                const keys = [];\n                for (const name in indexes[i].key) {\n                    keys.push(`${name}_${indexes[i].key[name]}`);\n                }\n                // Set the name\n                indexes[i].name = keys.join('_');\n            }\n        }\n        const cmd = { createIndexes: this.collectionName, indexes };\n        if (options.commitQuorum != null) {\n            if (serverWireVersion < 9) {\n                callback(new error_1.MongoCompatibilityError('Option `commitQuorum` for `createIndexes` not supported on servers < 4.4'));\n                return;\n            }\n            cmd.commitQuorum = options.commitQuorum;\n        }\n        // collation is set on each index, it should not be defined at the root\n        this.options.collation = undefined;\n        super.executeCommand(server, session, cmd, err => {\n            if (err) {\n                callback(err);\n                return;\n            }\n            const indexNames = indexes.map(index => index.name || '');\n            callback(undefined, indexNames);\n        });\n    }\n}\nexports.CreateIndexesOperation = CreateIndexesOperation;\n/** @internal */\nclass CreateIndexOperation extends CreateIndexesOperation {\n    constructor(parent, collectionName, indexSpec, options) {\n        // createIndex can be called with a variety of styles:\n        //   coll.createIndex('a');\n        //   coll.createIndex({ a: 1 });\n        //   coll.createIndex([['a', 1]]);\n        // createIndexes is always called with an array of index spec objects\n        super(parent, collectionName, [makeIndexSpec(indexSpec, options)], options);\n    }\n    execute(server, session, callback) {\n        super.execute(server, session, (err, indexNames) => {\n            if (err || !indexNames)\n                return callback(err);\n            return callback(undefined, indexNames[0]);\n        });\n    }\n}\nexports.CreateIndexOperation = CreateIndexOperation;\n/** @internal */\nclass EnsureIndexOperation extends CreateIndexOperation {\n    constructor(db, collectionName, indexSpec, options) {\n        super(db, collectionName, indexSpec, options);\n        this.readPreference = read_preference_1.ReadPreference.primary;\n        this.db = db;\n        this.collectionName = collectionName;\n    }\n    execute(server, session, callback) {\n        const indexName = this.indexes[0].name;\n        const cursor = this.db.collection(this.collectionName).listIndexes({ session });\n        cursor.toArray((err, indexes) => {\n            /// ignore \"NamespaceNotFound\" errors\n            if (err && err.code !== error_1.MONGODB_ERROR_CODES.NamespaceNotFound) {\n                return callback(err);\n            }\n            if (indexes) {\n                indexes = Array.isArray(indexes) ? indexes : [indexes];\n                if (indexes.some(index => index.name === indexName)) {\n                    callback(undefined, indexName);\n                    return;\n                }\n            }\n            super.execute(server, session, callback);\n        });\n    }\n}\nexports.EnsureIndexOperation = EnsureIndexOperation;\n/** @internal */\nclass DropIndexOperation extends command_1.CommandOperation {\n    constructor(collection, indexName, options) {\n        super(collection, options);\n        this.options = options !== null && options !== void 0 ? options : {};\n        this.collection = collection;\n        this.indexName = indexName;\n    }\n    execute(server, session, callback) {\n        const cmd = { dropIndexes: this.collection.collectionName, index: this.indexName };\n        super.executeCommand(server, session, cmd, callback);\n    }\n}\nexports.DropIndexOperation = DropIndexOperation;\n/** @internal */\nclass DropIndexesOperation extends DropIndexOperation {\n    constructor(collection, options) {\n        super(collection, '*', options);\n    }\n    execute(server, session, callback) {\n        super.execute(server, session, err => {\n            if (err)\n                return callback(err, false);\n            callback(undefined, true);\n        });\n    }\n}\nexports.DropIndexesOperation = DropIndexesOperation;\n/** @internal */\nclass ListIndexesOperation extends command_1.CommandOperation {\n    constructor(collection, options) {\n        super(collection, options);\n        this.options = options !== null && options !== void 0 ? options : {};\n        this.collectionNamespace = collection.s.namespace;\n    }\n    execute(server, session, callback) {\n        const serverWireVersion = (0, utils_1.maxWireVersion)(server);\n        if (serverWireVersion < LIST_INDEXES_WIRE_VERSION) {\n            const systemIndexesNS = this.collectionNamespace.withCollection('system.indexes');\n            const collectionNS = this.collectionNamespace.toString();\n            server.query(systemIndexesNS, { query: { ns: collectionNS } }, { ...this.options, readPreference: this.readPreference }, callback);\n            return;\n        }\n        const cursor = this.options.batchSize ? { batchSize: this.options.batchSize } : {};\n        super.executeCommand(server, session, { listIndexes: this.collectionNamespace.collection, cursor }, callback);\n    }\n}\nexports.ListIndexesOperation = ListIndexesOperation;\n/** @public */\nclass ListIndexesCursor extends abstract_cursor_1.AbstractCursor {\n    constructor(collection, options) {\n        super((0, utils_1.getTopology)(collection), collection.s.namespace, options);\n        this.parent = collection;\n        this.options = options;\n    }\n    clone() {\n        return new ListIndexesCursor(this.parent, {\n            ...this.options,\n            ...this.cursorOptions\n        });\n    }\n    /** @internal */\n    _initialize(session, callback) {\n        const operation = new ListIndexesOperation(this.parent, {\n            ...this.cursorOptions,\n            ...this.options,\n            session\n        });\n        (0, execute_operation_1.executeOperation)((0, utils_1.getTopology)(this.parent), operation, (err, response) => {\n            if (err || response == null)\n                return callback(err);\n            // TODO: NODE-2882\n            callback(undefined, { server: operation.server, session, response });\n        });\n    }\n}\nexports.ListIndexesCursor = ListIndexesCursor;\n/** @internal */\nclass IndexExistsOperation extends operation_1.AbstractOperation {\n    constructor(collection, indexes, options) {\n        super(options);\n        this.options = options;\n        this.collection = collection;\n        this.indexes = indexes;\n    }\n    execute(server, session, callback) {\n        const coll = this.collection;\n        const indexes = this.indexes;\n        (0, common_functions_1.indexInformation)(coll.s.db, coll.collectionName, { ...this.options, readPreference: this.readPreference, session }, (err, indexInformation) => {\n            // If we have an error return\n            if (err != null)\n                return callback(err);\n            // Let's check for the index names\n            if (!Array.isArray(indexes))\n                return callback(undefined, indexInformation[indexes] != null);\n            // Check in list of indexes\n            for (let i = 0; i < indexes.length; i++) {\n                if (indexInformation[indexes[i]] == null) {\n                    return callback(undefined, false);\n                }\n            }\n            // All keys found return true\n            return callback(undefined, true);\n        });\n    }\n}\nexports.IndexExistsOperation = IndexExistsOperation;\n/** @internal */\nclass IndexInformationOperation extends operation_1.AbstractOperation {\n    constructor(db, name, options) {\n        super(options);\n        this.options = options !== null && options !== void 0 ? options : {};\n        this.db = db;\n        this.name = name;\n    }\n    execute(server, session, callback) {\n        const db = this.db;\n        const name = this.name;\n        (0, common_functions_1.indexInformation)(db, name, { ...this.options, readPreference: this.readPreference, session }, callback);\n    }\n}\nexports.IndexInformationOperation = IndexInformationOperation;\n(0, operation_1.defineAspects)(ListIndexesOperation, [\n    operation_1.Aspect.READ_OPERATION,\n    operation_1.Aspect.RETRYABLE,\n    operation_1.Aspect.CURSOR_CREATING\n]);\n(0, operation_1.defineAspects)(CreateIndexesOperation, [operation_1.Aspect.WRITE_OPERATION]);\n(0, operation_1.defineAspects)(CreateIndexOperation, [operation_1.Aspect.WRITE_OPERATION]);\n(0, operation_1.defineAspects)(EnsureIndexOperation, [operation_1.Aspect.WRITE_OPERATION]);\n(0, operation_1.defineAspects)(DropIndexOperation, [operation_1.Aspect.WRITE_OPERATION]);\n(0, operation_1.defineAspects)(DropIndexesOperation, [operation_1.Aspect.WRITE_OPERATION]);\n//# sourceMappingURL=indexes.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.prepareDocs = exports.indexInformation = void 0;\nconst error_1 = require(\"../error\");\nconst utils_1 = require(\"../utils\");\nfunction indexInformation(db, name, _optionsOrCallback, _callback) {\n    let options = _optionsOrCallback;\n    let callback = _callback;\n    if ('function' === typeof _optionsOrCallback) {\n        callback = _optionsOrCallback;\n        options = {};\n    }\n    // If we specified full information\n    const full = options.full == null ? false : options.full;\n    // Did the user destroy the topology\n    if ((0, utils_1.getTopology)(db).isDestroyed())\n        return callback(new error_1.MongoTopologyClosedError());\n    // Process all the results from the index command and collection\n    function processResults(indexes) {\n        // Contains all the information\n        const info = {};\n        // Process all the indexes\n        for (let i = 0; i < indexes.length; i++) {\n            const index = indexes[i];\n            // Let's unpack the object\n            info[index.name] = [];\n            for (const name in index.key) {\n                info[index.name].push([name, index.key[name]]);\n            }\n        }\n        return info;\n    }\n    // Get the list of indexes of the specified collection\n    db.collection(name)\n        .listIndexes(options)\n        .toArray((err, indexes) => {\n        if (err)\n            return callback(err);\n        if (!Array.isArray(indexes))\n            return callback(undefined, []);\n        if (full)\n            return callback(undefined, indexes);\n        callback(undefined, processResults(indexes));\n    });\n}\nexports.indexInformation = indexInformation;\nfunction prepareDocs(coll, docs, options) {\n    var _a;\n    const forceServerObjectId = typeof options.forceServerObjectId === 'boolean'\n        ? options.forceServerObjectId\n        : (_a = coll.s.db.options) === null || _a === void 0 ? void 0 : _a.forceServerObjectId;\n    // no need to modify the docs if server sets the ObjectId\n    if (forceServerObjectId === true) {\n        return docs;\n    }\n    return docs.map(doc => {\n        if (doc._id == null) {\n            doc._id = coll.s.pkFactory.createPk();\n        }\n        return doc;\n    });\n}\nexports.prepareDocs = prepareDocs;\n//# sourceMappingURL=common_functions.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.ListCollectionsCursor = exports.ListCollectionsOperation = void 0;\nconst command_1 = require(\"./command\");\nconst operation_1 = require(\"./operation\");\nconst utils_1 = require(\"../utils\");\nconst CONSTANTS = require(\"../constants\");\nconst abstract_cursor_1 = require(\"../cursor/abstract_cursor\");\nconst execute_operation_1 = require(\"./execute_operation\");\nconst LIST_COLLECTIONS_WIRE_VERSION = 3;\n/** @internal */\nclass ListCollectionsOperation extends command_1.CommandOperation {\n    constructor(db, filter, options) {\n        super(db, options);\n        this.options = options !== null && options !== void 0 ? options : {};\n        this.db = db;\n        this.filter = filter;\n        this.nameOnly = !!this.options.nameOnly;\n        this.authorizedCollections = !!this.options.authorizedCollections;\n        if (typeof this.options.batchSize === 'number') {\n            this.batchSize = this.options.batchSize;\n        }\n    }\n    execute(server, session, callback) {\n        if ((0, utils_1.maxWireVersion)(server) < LIST_COLLECTIONS_WIRE_VERSION) {\n            let filter = this.filter;\n            const databaseName = this.db.s.namespace.db;\n            // If we have legacy mode and have not provided a full db name filter it\n            if (typeof filter.name === 'string' && !new RegExp(`^${databaseName}\\\\.`).test(filter.name)) {\n                filter = Object.assign({}, filter);\n                filter.name = this.db.s.namespace.withCollection(filter.name).toString();\n            }\n            // No filter, filter by current database\n            if (filter == null) {\n                filter = { name: `/${databaseName}/` };\n            }\n            // Rewrite the filter to use $and to filter out indexes\n            if (filter.name) {\n                filter = { $and: [{ name: filter.name }, { name: /^((?!\\$).)*$/ }] };\n            }\n            else {\n                filter = { name: /^((?!\\$).)*$/ };\n            }\n            const documentTransform = (doc) => {\n                const matching = `${databaseName}.`;\n                const index = doc.name.indexOf(matching);\n                // Remove database name if available\n                if (doc.name && index === 0) {\n                    doc.name = doc.name.substr(index + matching.length);\n                }\n                return doc;\n            };\n            server.query(new utils_1.MongoDBNamespace(databaseName, CONSTANTS.SYSTEM_NAMESPACE_COLLECTION), { query: filter }, { batchSize: this.batchSize || 1000, readPreference: this.readPreference }, (err, result) => {\n                if (result && result.documents && Array.isArray(result.documents)) {\n                    result.documents = result.documents.map(documentTransform);\n                }\n                callback(err, result);\n            });\n            return;\n        }\n        return super.executeCommand(server, session, this.generateCommand(), callback);\n    }\n    /* This is here for the purpose of unit testing the final command that gets sent. */\n    generateCommand() {\n        return {\n            listCollections: 1,\n            filter: this.filter,\n            cursor: this.batchSize ? { batchSize: this.batchSize } : {},\n            nameOnly: this.nameOnly,\n            authorizedCollections: this.authorizedCollections\n        };\n    }\n}\nexports.ListCollectionsOperation = ListCollectionsOperation;\n/** @public */\nclass ListCollectionsCursor extends abstract_cursor_1.AbstractCursor {\n    constructor(db, filter, options) {\n        super((0, utils_1.getTopology)(db), db.s.namespace, options);\n        this.parent = db;\n        this.filter = filter;\n        this.options = options;\n    }\n    clone() {\n        return new ListCollectionsCursor(this.parent, this.filter, {\n            ...this.options,\n            ...this.cursorOptions\n        });\n    }\n    /** @internal */\n    _initialize(session, callback) {\n        const operation = new ListCollectionsOperation(this.parent, this.filter, {\n            ...this.cursorOptions,\n            ...this.options,\n            session\n        });\n        (0, execute_operation_1.executeOperation)((0, utils_1.getTopology)(this.parent), operation, (err, response) => {\n            if (err || response == null)\n                return callback(err);\n            // TODO: NODE-2882\n            callback(undefined, { server: operation.server, session, response });\n        });\n    }\n}\nexports.ListCollectionsCursor = ListCollectionsCursor;\n(0, operation_1.defineAspects)(ListCollectionsOperation, [\n    operation_1.Aspect.READ_OPERATION,\n    operation_1.Aspect.RETRYABLE,\n    operation_1.Aspect.CURSOR_CREATING\n]);\n//# sourceMappingURL=list_collections.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.SYSTEM_JS_COLLECTION = exports.SYSTEM_COMMAND_COLLECTION = exports.SYSTEM_USER_COLLECTION = exports.SYSTEM_PROFILE_COLLECTION = exports.SYSTEM_INDEX_COLLECTION = exports.SYSTEM_NAMESPACE_COLLECTION = void 0;\nexports.SYSTEM_NAMESPACE_COLLECTION = 'system.namespaces';\nexports.SYSTEM_INDEX_COLLECTION = 'system.indexes';\nexports.SYSTEM_PROFILE_COLLECTION = 'system.profile';\nexports.SYSTEM_USER_COLLECTION = 'system.users';\nexports.SYSTEM_COMMAND_COLLECTION = '$cmd';\nexports.SYSTEM_JS_COLLECTION = 'system.js';\n//# sourceMappingURL=constants.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.Admin = void 0;\nconst add_user_1 = require(\"./operations/add_user\");\nconst remove_user_1 = require(\"./operations/remove_user\");\nconst validate_collection_1 = require(\"./operations/validate_collection\");\nconst list_databases_1 = require(\"./operations/list_databases\");\nconst execute_operation_1 = require(\"./operations/execute_operation\");\nconst run_command_1 = require(\"./operations/run_command\");\nconst utils_1 = require(\"./utils\");\n/**\n * The **Admin** class is an internal class that allows convenient access to\n * the admin functionality and commands for MongoDB.\n *\n * **ADMIN Cannot directly be instantiated**\n * @public\n *\n * @example\n * ```js\n * const MongoClient = require('mongodb').MongoClient;\n * const test = require('assert');\n * // Connection url\n * const url = 'mongodb://localhost:27017';\n * // Database Name\n * const dbName = 'test';\n *\n * // Connect using MongoClient\n * MongoClient.connect(url, function(err, client) {\n *   // Use the admin database for the operation\n *   const adminDb = client.db(dbName).admin();\n *\n *   // List all the available databases\n *   adminDb.listDatabases(function(err, dbs) {\n *     expect(err).to.not.exist;\n *     test.ok(dbs.databases.length > 0);\n *     client.close();\n *   });\n * });\n * ```\n */\nclass Admin {\n    /**\n     * Create a new Admin instance\n     * @internal\n     */\n    constructor(db) {\n        this.s = { db };\n    }\n    command(command, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = Object.assign({ dbName: 'admin' }, options);\n        return (0, execute_operation_1.executeOperation)((0, utils_1.getTopology)(this.s.db), new run_command_1.RunCommandOperation(this.s.db, command, options), callback);\n    }\n    buildInfo(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = options !== null && options !== void 0 ? options : {};\n        return this.command({ buildinfo: 1 }, options, callback);\n    }\n    serverInfo(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = options !== null && options !== void 0 ? options : {};\n        return this.command({ buildinfo: 1 }, options, callback);\n    }\n    serverStatus(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = options !== null && options !== void 0 ? options : {};\n        return this.command({ serverStatus: 1 }, options, callback);\n    }\n    ping(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = options !== null && options !== void 0 ? options : {};\n        return this.command({ ping: 1 }, options, callback);\n    }\n    addUser(username, password, options, callback) {\n        if (typeof password === 'function') {\n            (callback = password), (password = undefined), (options = {});\n        }\n        else if (typeof password !== 'string') {\n            if (typeof options === 'function') {\n                (callback = options), (options = password), (password = undefined);\n            }\n            else {\n                (options = password), (callback = undefined), (password = undefined);\n            }\n        }\n        else {\n            if (typeof options === 'function')\n                (callback = options), (options = {});\n        }\n        options = Object.assign({ dbName: 'admin' }, options);\n        return (0, execute_operation_1.executeOperation)((0, utils_1.getTopology)(this.s.db), new add_user_1.AddUserOperation(this.s.db, username, password, options), callback);\n    }\n    removeUser(username, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = Object.assign({ dbName: 'admin' }, options);\n        return (0, execute_operation_1.executeOperation)((0, utils_1.getTopology)(this.s.db), new remove_user_1.RemoveUserOperation(this.s.db, username, options), callback);\n    }\n    validateCollection(collectionName, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = options !== null && options !== void 0 ? options : {};\n        return (0, execute_operation_1.executeOperation)((0, utils_1.getTopology)(this.s.db), new validate_collection_1.ValidateCollectionOperation(this, collectionName, options), callback);\n    }\n    listDatabases(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = options !== null && options !== void 0 ? options : {};\n        return (0, execute_operation_1.executeOperation)((0, utils_1.getTopology)(this.s.db), new list_databases_1.ListDatabasesOperation(this.s.db, options), callback);\n    }\n    replSetGetStatus(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = options !== null && options !== void 0 ? options : {};\n        return this.command({ replSetGetStatus: 1 }, options, callback);\n    }\n}\nexports.Admin = Admin;\n//# sourceMappingURL=admin.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.AddUserOperation = void 0;\nconst crypto = require(\"crypto\");\nconst operation_1 = require(\"./operation\");\nconst command_1 = require(\"./command\");\nconst error_1 = require(\"../error\");\nconst utils_1 = require(\"../utils\");\n/** @internal */\nclass AddUserOperation extends command_1.CommandOperation {\n    constructor(db, username, password, options) {\n        super(db, options);\n        this.db = db;\n        this.username = username;\n        this.password = password;\n        this.options = options !== null && options !== void 0 ? options : {};\n    }\n    execute(server, session, callback) {\n        const db = this.db;\n        const username = this.username;\n        const password = this.password;\n        const options = this.options;\n        // Error out if digestPassword set\n        if (options.digestPassword != null) {\n            return callback(new error_1.MongoInvalidArgumentError('Option \"digestPassword\" not supported via addUser, use db.command(...) instead'));\n        }\n        let roles;\n        if (!options.roles || (Array.isArray(options.roles) && options.roles.length === 0)) {\n            (0, utils_1.emitWarningOnce)('Creating a user without roles is deprecated. Defaults to \"root\" if db is \"admin\" or \"dbOwner\" otherwise');\n            if (db.databaseName.toLowerCase() === 'admin') {\n                roles = ['root'];\n            }\n            else {\n                roles = ['dbOwner'];\n            }\n        }\n        else {\n            roles = Array.isArray(options.roles) ? options.roles : [options.roles];\n        }\n        const digestPassword = (0, utils_1.getTopology)(db).lastIsMaster().maxWireVersion >= 7;\n        let userPassword = password;\n        if (!digestPassword) {\n            // Use node md5 generator\n            const md5 = crypto.createHash('md5');\n            // Generate keys used for authentication\n            md5.update(`${username}:mongo:${password}`);\n            userPassword = md5.digest('hex');\n        }\n        // Build the command to execute\n        const command = {\n            createUser: username,\n            customData: options.customData || {},\n            roles: roles,\n            digestPassword\n        };\n        // No password\n        if (typeof password === 'string') {\n            command.pwd = userPassword;\n        }\n        super.executeCommand(server, session, command, callback);\n    }\n}\nexports.AddUserOperation = AddUserOperation;\n(0, operation_1.defineAspects)(AddUserOperation, [operation_1.Aspect.WRITE_OPERATION]);\n//# sourceMappingURL=add_user.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.RemoveUserOperation = void 0;\nconst operation_1 = require(\"./operation\");\nconst command_1 = require(\"./command\");\n/** @internal */\nclass RemoveUserOperation extends command_1.CommandOperation {\n    constructor(db, username, options) {\n        super(db, options);\n        this.options = options;\n        this.username = username;\n    }\n    execute(server, session, callback) {\n        super.executeCommand(server, session, { dropUser: this.username }, err => {\n            callback(err, err ? false : true);\n        });\n    }\n}\nexports.RemoveUserOperation = RemoveUserOperation;\n(0, operation_1.defineAspects)(RemoveUserOperation, [operation_1.Aspect.WRITE_OPERATION]);\n//# sourceMappingURL=remove_user.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.ValidateCollectionOperation = void 0;\nconst command_1 = require(\"./command\");\nconst error_1 = require(\"../error\");\n/** @internal */\nclass ValidateCollectionOperation extends command_1.CommandOperation {\n    constructor(admin, collectionName, options) {\n        // Decorate command with extra options\n        const command = { validate: collectionName };\n        const keys = Object.keys(options);\n        for (let i = 0; i < keys.length; i++) {\n            if (Object.prototype.hasOwnProperty.call(options, keys[i]) && keys[i] !== 'session') {\n                command[keys[i]] = options[keys[i]];\n            }\n        }\n        super(admin.s.db, options);\n        this.options = options;\n        this.command = command;\n        this.collectionName = collectionName;\n    }\n    execute(server, session, callback) {\n        const collectionName = this.collectionName;\n        super.executeCommand(server, session, this.command, (err, doc) => {\n            if (err != null)\n                return callback(err);\n            // TODO(NODE-3483): Replace these with MongoUnexpectedServerResponseError\n            if (doc.ok === 0)\n                return callback(new error_1.MongoRuntimeError('Error with validate command'));\n            if (doc.result != null && typeof doc.result !== 'string')\n                return callback(new error_1.MongoRuntimeError('Error with validation data'));\n            if (doc.result != null && doc.result.match(/exception|corrupt/) != null)\n                return callback(new error_1.MongoRuntimeError(`Invalid collection ${collectionName}`));\n            if (doc.valid != null && !doc.valid)\n                return callback(new error_1.MongoRuntimeError(`Invalid collection ${collectionName}`));\n            return callback(undefined, doc);\n        });\n    }\n}\nexports.ValidateCollectionOperation = ValidateCollectionOperation;\n//# sourceMappingURL=validate_collection.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.ListDatabasesOperation = void 0;\nconst command_1 = require(\"./command\");\nconst operation_1 = require(\"./operation\");\nconst utils_1 = require(\"../utils\");\n/** @internal */\nclass ListDatabasesOperation extends command_1.CommandOperation {\n    constructor(db, options) {\n        super(db, options);\n        this.options = options !== null && options !== void 0 ? options : {};\n        this.ns = new utils_1.MongoDBNamespace('admin', '$cmd');\n    }\n    execute(server, session, callback) {\n        const cmd = { listDatabases: 1 };\n        if (this.options.nameOnly) {\n            cmd.nameOnly = Number(cmd.nameOnly);\n        }\n        if (this.options.filter) {\n            cmd.filter = this.options.filter;\n        }\n        if (typeof this.options.authorizedDatabases === 'boolean') {\n            cmd.authorizedDatabases = this.options.authorizedDatabases;\n        }\n        super.executeCommand(server, session, cmd, callback);\n    }\n}\nexports.ListDatabasesOperation = ListDatabasesOperation;\n(0, operation_1.defineAspects)(ListDatabasesOperation, [operation_1.Aspect.READ_OPERATION, operation_1.Aspect.RETRYABLE]);\n//# sourceMappingURL=list_databases.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.MongoClient = exports.ServerApiVersion = void 0;\nconst db_1 = require(\"./db\");\nconst change_stream_1 = require(\"./change_stream\");\nconst error_1 = require(\"./error\");\nconst utils_1 = require(\"./utils\");\nconst connect_1 = require(\"./operations/connect\");\nconst promise_provider_1 = require(\"./promise_provider\");\nconst bson_1 = require(\"./bson\");\nconst connection_string_1 = require(\"./connection_string\");\nconst mongo_types_1 = require(\"./mongo_types\");\n/** @public */\nexports.ServerApiVersion = Object.freeze({\n    v1: '1'\n});\n/** @internal */\nconst kOptions = Symbol('options');\n/**\n * The **MongoClient** class is a class that allows for making Connections to MongoDB.\n * @public\n *\n * @remarks\n * The programmatically provided options take precedent over the URI options.\n *\n * @example\n * ```js\n * // Connect using a MongoClient instance\n * const MongoClient = require('mongodb').MongoClient;\n * const test = require('assert');\n * // Connection url\n * const url = 'mongodb://localhost:27017';\n * // Database Name\n * const dbName = 'test';\n * // Connect using MongoClient\n * const mongoClient = new MongoClient(url);\n * mongoClient.connect(function(err, client) {\n *   const db = client.db(dbName);\n *   client.close();\n * });\n * ```\n *\n * @example\n * ```js\n * // Connect using the MongoClient.connect static method\n * const MongoClient = require('mongodb').MongoClient;\n * const test = require('assert');\n * // Connection url\n * const url = 'mongodb://localhost:27017';\n * // Database Name\n * const dbName = 'test';\n * // Connect using MongoClient\n * MongoClient.connect(url, function(err, client) {\n *   const db = client.db(dbName);\n *   client.close();\n * });\n * ```\n */\nclass MongoClient extends mongo_types_1.TypedEventEmitter {\n    constructor(url, options) {\n        super();\n        this[kOptions] = (0, connection_string_1.parseOptions)(url, this, options);\n        // eslint-disable-next-line @typescript-eslint/no-this-alias\n        const client = this;\n        // The internal state\n        this.s = {\n            url,\n            sessions: new Set(),\n            bsonOptions: (0, bson_1.resolveBSONOptions)(this[kOptions]),\n            namespace: (0, utils_1.ns)('admin'),\n            get options() {\n                return client[kOptions];\n            },\n            get readConcern() {\n                return client[kOptions].readConcern;\n            },\n            get writeConcern() {\n                return client[kOptions].writeConcern;\n            },\n            get readPreference() {\n                return client[kOptions].readPreference;\n            },\n            get logger() {\n                return client[kOptions].logger;\n            }\n        };\n    }\n    get options() {\n        return Object.freeze({ ...this[kOptions] });\n    }\n    get serverApi() {\n        return this[kOptions].serverApi && Object.freeze({ ...this[kOptions].serverApi });\n    }\n    /**\n     * Intended for APM use only\n     * @internal\n     */\n    get monitorCommands() {\n        return this[kOptions].monitorCommands;\n    }\n    set monitorCommands(value) {\n        this[kOptions].monitorCommands = value;\n    }\n    get autoEncrypter() {\n        return this[kOptions].autoEncrypter;\n    }\n    get readConcern() {\n        return this.s.readConcern;\n    }\n    get writeConcern() {\n        return this.s.writeConcern;\n    }\n    get readPreference() {\n        return this.s.readPreference;\n    }\n    get bsonOptions() {\n        return this.s.bsonOptions;\n    }\n    get logger() {\n        return this.s.logger;\n    }\n    connect(callback) {\n        if (callback && typeof callback !== 'function') {\n            throw new error_1.MongoInvalidArgumentError('Method `connect` only accepts a callback');\n        }\n        return (0, utils_1.maybePromise)(callback, cb => {\n            (0, connect_1.connect)(this, this[kOptions], err => {\n                if (err)\n                    return cb(err);\n                cb(undefined, this);\n            });\n        });\n    }\n    close(forceOrCallback, callback) {\n        if (typeof forceOrCallback === 'function') {\n            callback = forceOrCallback;\n        }\n        const force = typeof forceOrCallback === 'boolean' ? forceOrCallback : false;\n        return (0, utils_1.maybePromise)(callback, callback => {\n            if (this.topology == null) {\n                return callback();\n            }\n            // clear out references to old topology\n            const topology = this.topology;\n            this.topology = undefined;\n            topology.close({ force }, error => {\n                if (error)\n                    return callback(error);\n                const { encrypter } = this[kOptions];\n                if (encrypter) {\n                    return encrypter.close(this, force, error => {\n                        callback(error);\n                    });\n                }\n                callback();\n            });\n        });\n    }\n    /**\n     * Create a new Db instance sharing the current socket connections.\n     *\n     * @param dbName - The name of the database we want to use. If not provided, use database name from connection string.\n     * @param options - Optional settings for Db construction\n     */\n    db(dbName, options) {\n        options = options !== null && options !== void 0 ? options : {};\n        // Default to db from connection string if not provided\n        if (!dbName) {\n            dbName = this.options.dbName;\n        }\n        // Copy the options and add out internal override of the not shared flag\n        const finalOptions = Object.assign({}, this[kOptions], options);\n        // Return the db object\n        const db = new db_1.Db(this, dbName, finalOptions);\n        // Return the database\n        return db;\n    }\n    static connect(url, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = options !== null && options !== void 0 ? options : {};\n        try {\n            // Create client\n            const mongoClient = new MongoClient(url, options);\n            // Execute the connect method\n            if (callback) {\n                return mongoClient.connect(callback);\n            }\n            else {\n                return mongoClient.connect();\n            }\n        }\n        catch (error) {\n            if (callback)\n                return callback(error);\n            else\n                return promise_provider_1.PromiseProvider.get().reject(error);\n        }\n    }\n    startSession(options) {\n        options = Object.assign({ explicit: true }, options);\n        if (!this.topology) {\n            throw new error_1.MongoNotConnectedError('MongoClient must be connected to start a session');\n        }\n        return this.topology.startSession(options, this.s.options);\n    }\n    withSession(optionsOrOperation, callback) {\n        let options = optionsOrOperation;\n        if (typeof optionsOrOperation === 'function') {\n            callback = optionsOrOperation;\n            options = { owner: Symbol() };\n        }\n        if (callback == null) {\n            throw new error_1.MongoInvalidArgumentError('Missing required callback parameter');\n        }\n        const session = this.startSession(options);\n        const Promise = promise_provider_1.PromiseProvider.get();\n        let cleanupHandler = ((err, result, opts) => {\n            // prevent multiple calls to cleanupHandler\n            cleanupHandler = () => {\n                // TODO(NODE-3483)\n                throw new error_1.MongoRuntimeError('cleanupHandler was called too many times');\n            };\n            opts = Object.assign({ throw: true }, opts);\n            session.endSession();\n            if (err) {\n                if (opts.throw)\n                    throw err;\n                return Promise.reject(err);\n            }\n        });\n        try {\n            const result = callback(session);\n            return Promise.resolve(result).then(result => cleanupHandler(undefined, result, undefined), err => cleanupHandler(err, null, { throw: true }));\n        }\n        catch (err) {\n            return cleanupHandler(err, null, { throw: false });\n        }\n    }\n    /**\n     * Create a new Change Stream, watching for new changes (insertions, updates,\n     * replacements, deletions, and invalidations) in this cluster. Will ignore all\n     * changes to system collections, as well as the local, admin, and config databases.\n     *\n     * @param pipeline - An array of {@link https://docs.mongodb.com/manual/reference/operator/aggregation-pipeline/|aggregation pipeline stages} through which to pass change stream documents. This allows for filtering (using $match) and manipulating the change stream documents.\n     * @param options - Optional settings for the command\n     */\n    watch(pipeline = [], options = {}) {\n        // Allow optionally not specifying a pipeline\n        if (!Array.isArray(pipeline)) {\n            options = pipeline;\n            pipeline = [];\n        }\n        return new change_stream_1.ChangeStream(this, pipeline, (0, utils_1.resolveOptions)(this, options));\n    }\n    /** Return the mongo client logger */\n    getLogger() {\n        return this.s.logger;\n    }\n}\nexports.MongoClient = MongoClient;\n//# sourceMappingURL=mongo_client.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.Db = void 0;\nconst utils_1 = require(\"./utils\");\nconst aggregation_cursor_1 = require(\"./cursor/aggregation_cursor\");\nconst bson_1 = require(\"./bson\");\nconst read_preference_1 = require(\"./read_preference\");\nconst error_1 = require(\"./error\");\nconst collection_1 = require(\"./collection\");\nconst change_stream_1 = require(\"./change_stream\");\nconst CONSTANTS = require(\"./constants\");\nconst write_concern_1 = require(\"./write_concern\");\nconst read_concern_1 = require(\"./read_concern\");\nconst logger_1 = require(\"./logger\");\nconst add_user_1 = require(\"./operations/add_user\");\nconst collections_1 = require(\"./operations/collections\");\nconst stats_1 = require(\"./operations/stats\");\nconst run_command_1 = require(\"./operations/run_command\");\nconst create_collection_1 = require(\"./operations/create_collection\");\nconst indexes_1 = require(\"./operations/indexes\");\nconst drop_1 = require(\"./operations/drop\");\nconst list_collections_1 = require(\"./operations/list_collections\");\nconst profiling_level_1 = require(\"./operations/profiling_level\");\nconst remove_user_1 = require(\"./operations/remove_user\");\nconst rename_1 = require(\"./operations/rename\");\nconst set_profiling_level_1 = require(\"./operations/set_profiling_level\");\nconst execute_operation_1 = require(\"./operations/execute_operation\");\nconst admin_1 = require(\"./admin\");\n// Allowed parameters\nconst DB_OPTIONS_ALLOW_LIST = [\n    'writeConcern',\n    'readPreference',\n    'readPreferenceTags',\n    'native_parser',\n    'forceServerObjectId',\n    'pkFactory',\n    'serializeFunctions',\n    'raw',\n    'authSource',\n    'ignoreUndefined',\n    'readConcern',\n    'retryMiliSeconds',\n    'numberOfRetries',\n    'loggerLevel',\n    'logger',\n    'promoteBuffers',\n    'promoteLongs',\n    'bsonRegExp',\n    'promoteValues',\n    'compression',\n    'retryWrites'\n];\n/**\n * The **Db** class is a class that represents a MongoDB Database.\n * @public\n *\n * @example\n * ```js\n * const { MongoClient } = require('mongodb');\n * // Connection url\n * const url = 'mongodb://localhost:27017';\n * // Database Name\n * const dbName = 'test';\n * // Connect using MongoClient\n * MongoClient.connect(url, function(err, client) {\n *   // Select the database by name\n *   const testDb = client.db(dbName);\n *   client.close();\n * });\n * ```\n */\nclass Db {\n    /**\n     * Creates a new Db instance\n     *\n     * @param client - The MongoClient for the database.\n     * @param databaseName - The name of the database this instance represents.\n     * @param options - Optional settings for Db construction\n     */\n    constructor(client, databaseName, options) {\n        var _a;\n        options = options !== null && options !== void 0 ? options : {};\n        // Filter the options\n        options = (0, utils_1.filterOptions)(options, DB_OPTIONS_ALLOW_LIST);\n        // Ensure we have a valid db name\n        validateDatabaseName(databaseName);\n        // Internal state of the db object\n        this.s = {\n            // Client\n            client,\n            // Options\n            options,\n            // Logger instance\n            logger: new logger_1.Logger('Db', options),\n            // Unpack read preference\n            readPreference: read_preference_1.ReadPreference.fromOptions(options),\n            // Merge bson options\n            bsonOptions: (0, bson_1.resolveBSONOptions)(options, client),\n            // Set up the primary key factory or fallback to ObjectId\n            pkFactory: (_a = options === null || options === void 0 ? void 0 : options.pkFactory) !== null && _a !== void 0 ? _a : utils_1.DEFAULT_PK_FACTORY,\n            // ReadConcern\n            readConcern: read_concern_1.ReadConcern.fromOptions(options),\n            writeConcern: write_concern_1.WriteConcern.fromOptions(options),\n            // Namespace\n            namespace: new utils_1.MongoDBNamespace(databaseName)\n        };\n    }\n    get databaseName() {\n        return this.s.namespace.db;\n    }\n    // Options\n    get options() {\n        return this.s.options;\n    }\n    // slaveOk specified\n    get slaveOk() {\n        var _a;\n        return ((_a = this.s.readPreference) === null || _a === void 0 ? void 0 : _a.preference) !== 'primary' || false;\n    }\n    get readConcern() {\n        return this.s.readConcern;\n    }\n    /**\n     * The current readPreference of the Db. If not explicitly defined for\n     * this Db, will be inherited from the parent MongoClient\n     */\n    get readPreference() {\n        if (this.s.readPreference == null) {\n            return this.s.client.readPreference;\n        }\n        return this.s.readPreference;\n    }\n    get bsonOptions() {\n        return this.s.bsonOptions;\n    }\n    // get the write Concern\n    get writeConcern() {\n        return this.s.writeConcern;\n    }\n    get namespace() {\n        return this.s.namespace.toString();\n    }\n    createCollection(name, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_1.getTopology)(this), new create_collection_1.CreateCollectionOperation(this, name, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    command(command, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        // Intentionally, we do not inherit options from parent for this operation.\n        return (0, execute_operation_1.executeOperation)((0, utils_1.getTopology)(this), new run_command_1.RunCommandOperation(this, command, options !== null && options !== void 0 ? options : {}), callback);\n    }\n    /**\n     * Execute an aggregation framework pipeline against the database, needs MongoDB \\>= 3.6\n     *\n     * @param pipeline - An array of aggregation stages to be executed\n     * @param options - Optional settings for the command\n     */\n    aggregate(pipeline = [], options) {\n        if (arguments.length > 2) {\n            throw new error_1.MongoInvalidArgumentError('Method \"db.aggregate()\" accepts at most two arguments');\n        }\n        if (typeof pipeline === 'function') {\n            throw new error_1.MongoInvalidArgumentError('Argument \"pipeline\" must not be function');\n        }\n        if (typeof options === 'function') {\n            throw new error_1.MongoInvalidArgumentError('Argument \"options\" must not be function');\n        }\n        return new aggregation_cursor_1.AggregationCursor((0, utils_1.getTopology)(this), this.s.namespace, pipeline, (0, utils_1.resolveOptions)(this, options));\n    }\n    /** Return the Admin db instance */\n    admin() {\n        return new admin_1.Admin(this);\n    }\n    /**\n     * Returns a reference to a MongoDB Collection. If it does not exist it will be created implicitly.\n     *\n     * @param name - the collection name we wish to access.\n     * @returns return the new Collection instance\n     */\n    collection(name, options = {}) {\n        if (typeof options === 'function') {\n            throw new error_1.MongoInvalidArgumentError('The callback form of this helper has been removed.');\n        }\n        const finalOptions = (0, utils_1.resolveOptions)(this, options);\n        return new collection_1.Collection(this, name, finalOptions);\n    }\n    stats(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_1.getTopology)(this), new stats_1.DbStatsOperation(this, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    listCollections(filter = {}, options = {}) {\n        return new list_collections_1.ListCollectionsCursor(this, filter, (0, utils_1.resolveOptions)(this, options));\n    }\n    renameCollection(fromCollection, toCollection, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        // Intentionally, we do not inherit options from parent for this operation.\n        options = { ...options, readPreference: read_preference_1.ReadPreference.PRIMARY };\n        // Add return new collection\n        options.new_collection = true;\n        return (0, execute_operation_1.executeOperation)((0, utils_1.getTopology)(this), new rename_1.RenameOperation(this.collection(fromCollection), toCollection, options), callback);\n    }\n    dropCollection(name, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_1.getTopology)(this), new drop_1.DropCollectionOperation(this, name, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    dropDatabase(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_1.getTopology)(this), new drop_1.DropDatabaseOperation(this, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    collections(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_1.getTopology)(this), new collections_1.CollectionsOperation(this, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    createIndex(name, indexSpec, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_1.getTopology)(this), new indexes_1.CreateIndexOperation(this, name, indexSpec, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    addUser(username, password, options, callback) {\n        if (typeof password === 'function') {\n            (callback = password), (password = undefined), (options = {});\n        }\n        else if (typeof password !== 'string') {\n            if (typeof options === 'function') {\n                (callback = options), (options = password), (password = undefined);\n            }\n            else {\n                (options = password), (callback = undefined), (password = undefined);\n            }\n        }\n        else {\n            if (typeof options === 'function')\n                (callback = options), (options = {});\n        }\n        return (0, execute_operation_1.executeOperation)((0, utils_1.getTopology)(this), new add_user_1.AddUserOperation(this, username, password, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    removeUser(username, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_1.getTopology)(this), new remove_user_1.RemoveUserOperation(this, username, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    setProfilingLevel(level, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_1.getTopology)(this), new set_profiling_level_1.SetProfilingLevelOperation(this, level, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    profilingLevel(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_1.getTopology)(this), new profiling_level_1.ProfilingLevelOperation(this, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    indexInformation(name, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_1.getTopology)(this), new indexes_1.IndexInformationOperation(this, name, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    /**\n     * Unref all sockets\n     * @deprecated This function is deprecated and will be removed in the next major version.\n     */\n    unref() {\n        (0, utils_1.getTopology)(this).unref();\n    }\n    /**\n     * Create a new Change Stream, watching for new changes (insertions, updates,\n     * replacements, deletions, and invalidations) in this database. Will ignore all\n     * changes to system collections.\n     *\n     * @param pipeline - An array of {@link https://docs.mongodb.com/manual/reference/operator/aggregation-pipeline/|aggregation pipeline stages} through which to pass change stream documents. This allows for filtering (using $match) and manipulating the change stream documents.\n     * @param options - Optional settings for the command\n     */\n    watch(pipeline = [], options = {}) {\n        // Allow optionally not specifying a pipeline\n        if (!Array.isArray(pipeline)) {\n            options = pipeline;\n            pipeline = [];\n        }\n        return new change_stream_1.ChangeStream(this, pipeline, (0, utils_1.resolveOptions)(this, options));\n    }\n    /** Return the db logger */\n    getLogger() {\n        return this.s.logger;\n    }\n    get logger() {\n        return this.s.logger;\n    }\n}\nexports.Db = Db;\nDb.SYSTEM_NAMESPACE_COLLECTION = CONSTANTS.SYSTEM_NAMESPACE_COLLECTION;\nDb.SYSTEM_INDEX_COLLECTION = CONSTANTS.SYSTEM_INDEX_COLLECTION;\nDb.SYSTEM_PROFILE_COLLECTION = CONSTANTS.SYSTEM_PROFILE_COLLECTION;\nDb.SYSTEM_USER_COLLECTION = CONSTANTS.SYSTEM_USER_COLLECTION;\nDb.SYSTEM_COMMAND_COLLECTION = CONSTANTS.SYSTEM_COMMAND_COLLECTION;\nDb.SYSTEM_JS_COLLECTION = CONSTANTS.SYSTEM_JS_COLLECTION;\n// TODO(NODE-3484): Refactor into MongoDBNamespace\n// Validate the database name\nfunction validateDatabaseName(databaseName) {\n    if (typeof databaseName !== 'string')\n        throw new error_1.MongoInvalidArgumentError('Database name must be a string');\n    if (databaseName.length === 0)\n        throw new error_1.MongoInvalidArgumentError('Database name cannot be the empty string');\n    if (databaseName === '$external')\n        return;\n    const invalidChars = [' ', '.', '$', '/', '\\\\'];\n    for (let i = 0; i < invalidChars.length; i++) {\n        if (databaseName.indexOf(invalidChars[i]) !== -1)\n            throw new error_1.MongoAPIError(`database names cannot contain the character '${invalidChars[i]}'`);\n    }\n}\n//# sourceMappingURL=db.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.Collection = void 0;\nconst utils_1 = require(\"./utils\");\nconst read_preference_1 = require(\"./read_preference\");\nconst utils_2 = require(\"./utils\");\nconst bson_1 = require(\"./bson\");\nconst error_1 = require(\"./error\");\nconst unordered_1 = require(\"./bulk/unordered\");\nconst ordered_1 = require(\"./bulk/ordered\");\nconst change_stream_1 = require(\"./change_stream\");\nconst write_concern_1 = require(\"./write_concern\");\nconst read_concern_1 = require(\"./read_concern\");\nconst aggregation_cursor_1 = require(\"./cursor/aggregation_cursor\");\nconst bulk_write_1 = require(\"./operations/bulk_write\");\nconst count_documents_1 = require(\"./operations/count_documents\");\nconst indexes_1 = require(\"./operations/indexes\");\nconst distinct_1 = require(\"./operations/distinct\");\nconst drop_1 = require(\"./operations/drop\");\nconst estimated_document_count_1 = require(\"./operations/estimated_document_count\");\nconst find_and_modify_1 = require(\"./operations/find_and_modify\");\nconst insert_1 = require(\"./operations/insert\");\nconst update_1 = require(\"./operations/update\");\nconst delete_1 = require(\"./operations/delete\");\nconst is_capped_1 = require(\"./operations/is_capped\");\nconst map_reduce_1 = require(\"./operations/map_reduce\");\nconst options_operation_1 = require(\"./operations/options_operation\");\nconst rename_1 = require(\"./operations/rename\");\nconst stats_1 = require(\"./operations/stats\");\nconst execute_operation_1 = require(\"./operations/execute_operation\");\nconst find_cursor_1 = require(\"./cursor/find_cursor\");\n/**\n * The **Collection** class is an internal class that embodies a MongoDB collection\n * allowing for insert/update/remove/find and other command operation on that MongoDB collection.\n *\n * **COLLECTION Cannot directly be instantiated**\n * @public\n *\n * @example\n * ```js\n * const MongoClient = require('mongodb').MongoClient;\n * const test = require('assert');\n * // Connection url\n * const url = 'mongodb://localhost:27017';\n * // Database Name\n * const dbName = 'test';\n * // Connect using MongoClient\n * MongoClient.connect(url, function(err, client) {\n *   // Create a collection we want to drop later\n *   const col = client.db(dbName).collection('createIndexExample1');\n *   // Show that duplicate records got dropped\n *   col.find({}).toArray(function(err, items) {\n *     expect(err).to.not.exist;\n *     test.equal(4, items.length);\n *     client.close();\n *   });\n * });\n * ```\n */\nclass Collection {\n    /**\n     * Create a new Collection instance\n     * @internal\n     */\n    constructor(db, name, options) {\n        var _a, _b;\n        (0, utils_2.checkCollectionName)(name);\n        // Internal state\n        this.s = {\n            db,\n            options,\n            namespace: new utils_2.MongoDBNamespace(db.databaseName, name),\n            pkFactory: (_b = (_a = db.options) === null || _a === void 0 ? void 0 : _a.pkFactory) !== null && _b !== void 0 ? _b : utils_1.DEFAULT_PK_FACTORY,\n            readPreference: read_preference_1.ReadPreference.fromOptions(options),\n            bsonOptions: (0, bson_1.resolveBSONOptions)(options, db),\n            readConcern: read_concern_1.ReadConcern.fromOptions(options),\n            writeConcern: write_concern_1.WriteConcern.fromOptions(options),\n            slaveOk: options == null || options.slaveOk == null ? db.slaveOk : options.slaveOk\n        };\n    }\n    /**\n     * The name of the database this collection belongs to\n     */\n    get dbName() {\n        return this.s.namespace.db;\n    }\n    /**\n     * The name of this collection\n     */\n    get collectionName() {\n        // eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n        return this.s.namespace.collection;\n    }\n    /**\n     * The namespace of this collection, in the format `${this.dbName}.${this.collectionName}`\n     */\n    get namespace() {\n        return this.s.namespace.toString();\n    }\n    /**\n     * The current readConcern of the collection. If not explicitly defined for\n     * this collection, will be inherited from the parent DB\n     */\n    get readConcern() {\n        if (this.s.readConcern == null) {\n            return this.s.db.readConcern;\n        }\n        return this.s.readConcern;\n    }\n    /**\n     * The current readPreference of the collection. If not explicitly defined for\n     * this collection, will be inherited from the parent DB\n     */\n    get readPreference() {\n        if (this.s.readPreference == null) {\n            return this.s.db.readPreference;\n        }\n        return this.s.readPreference;\n    }\n    get bsonOptions() {\n        return this.s.bsonOptions;\n    }\n    /**\n     * The current writeConcern of the collection. If not explicitly defined for\n     * this collection, will be inherited from the parent DB\n     */\n    get writeConcern() {\n        if (this.s.writeConcern == null) {\n            return this.s.db.writeConcern;\n        }\n        return this.s.writeConcern;\n    }\n    /** The current index hint for the collection */\n    get hint() {\n        return this.s.collectionHint;\n    }\n    set hint(v) {\n        this.s.collectionHint = (0, utils_2.normalizeHintField)(v);\n    }\n    insertOne(doc, options, callback) {\n        if (typeof options === 'function') {\n            callback = options;\n            options = {};\n        }\n        // CSFLE passes in { w: 'majority' } to ensure the lib works in both 3.x and 4.x\n        // we support that option style here only\n        if (options && Reflect.get(options, 'w')) {\n            options.writeConcern = write_concern_1.WriteConcern.fromOptions(Reflect.get(options, 'w'));\n        }\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new insert_1.InsertOneOperation(this, doc, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    insertMany(docs, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = options ? Object.assign({}, options) : { ordered: true };\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new insert_1.InsertManyOperation(this, docs, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    bulkWrite(operations, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = options || { ordered: true };\n        if (!Array.isArray(operations)) {\n            throw new error_1.MongoInvalidArgumentError('Argument \"operations\" must be an array of documents');\n        }\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new bulk_write_1.BulkWriteOperation(this, operations, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    updateOne(filter, update, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new update_1.UpdateOneOperation(this, filter, update, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    replaceOne(filter, replacement, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new update_1.ReplaceOneOperation(this, filter, replacement, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    updateMany(filter, update, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new update_1.UpdateManyOperation(this, filter, update, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    deleteOne(filter, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new delete_1.DeleteOneOperation(this, filter, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    deleteMany(filter, options, callback) {\n        if (filter == null) {\n            filter = {};\n            options = {};\n            callback = undefined;\n        }\n        else if (typeof filter === 'function') {\n            callback = filter;\n            filter = {};\n            options = {};\n        }\n        else if (typeof options === 'function') {\n            callback = options;\n            options = {};\n        }\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new delete_1.DeleteManyOperation(this, filter, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    rename(newName, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        // Intentionally, we do not inherit options from parent for this operation.\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new rename_1.RenameOperation(this, newName, {\n            ...options,\n            readPreference: read_preference_1.ReadPreference.PRIMARY\n        }), callback);\n    }\n    drop(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = options !== null && options !== void 0 ? options : {};\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new drop_1.DropCollectionOperation(this.s.db, this.collectionName, options), callback);\n    }\n    findOne(filter, options, callback) {\n        if (callback != null && typeof callback !== 'function') {\n            throw new error_1.MongoInvalidArgumentError('Third parameter to `findOne()` must be a callback or undefined');\n        }\n        if (typeof filter === 'function') {\n            callback = filter;\n            filter = {};\n            options = {};\n        }\n        if (typeof options === 'function') {\n            callback = options;\n            options = {};\n        }\n        const finalFilter = filter !== null && filter !== void 0 ? filter : {};\n        const finalOptions = options !== null && options !== void 0 ? options : {};\n        return this.find(finalFilter, finalOptions).limit(-1).batchSize(1).next(callback);\n    }\n    find(filter, options) {\n        if (arguments.length > 2) {\n            throw new error_1.MongoInvalidArgumentError('Method \"collection.find()\" accepts at most two arguments');\n        }\n        if (typeof options === 'function') {\n            throw new error_1.MongoInvalidArgumentError('Argument \"options\" must not be function');\n        }\n        return new find_cursor_1.FindCursor((0, utils_2.getTopology)(this), this.s.namespace, filter, (0, utils_1.resolveOptions)(this, options));\n    }\n    options(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new options_operation_1.OptionsOperation(this, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    isCapped(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new is_capped_1.IsCappedOperation(this, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    createIndex(indexSpec, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new indexes_1.CreateIndexOperation(this, this.collectionName, indexSpec, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    createIndexes(indexSpecs, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = options ? Object.assign({}, options) : {};\n        if (typeof options.maxTimeMS !== 'number')\n            delete options.maxTimeMS;\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new indexes_1.CreateIndexesOperation(this, this.collectionName, indexSpecs, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    dropIndex(indexName, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = (0, utils_1.resolveOptions)(this, options);\n        // Run only against primary\n        options.readPreference = read_preference_1.ReadPreference.primary;\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new indexes_1.DropIndexOperation(this, indexName, options), callback);\n    }\n    dropIndexes(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new indexes_1.DropIndexesOperation(this, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    /**\n     * Get the list of all indexes information for the collection.\n     *\n     * @param options - Optional settings for the command\n     */\n    listIndexes(options) {\n        return new indexes_1.ListIndexesCursor(this, (0, utils_1.resolveOptions)(this, options));\n    }\n    indexExists(indexes, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new indexes_1.IndexExistsOperation(this, indexes, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    indexInformation(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new indexes_1.IndexInformationOperation(this.s.db, this.collectionName, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    estimatedDocumentCount(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new estimated_document_count_1.EstimatedDocumentCountOperation(this, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    countDocuments(filter, options, callback) {\n        if (filter == null) {\n            (filter = {}), (options = {}), (callback = undefined);\n        }\n        else if (typeof filter === 'function') {\n            (callback = filter), (filter = {}), (options = {});\n        }\n        else {\n            if (arguments.length === 2) {\n                if (typeof options === 'function')\n                    (callback = options), (options = {});\n            }\n        }\n        filter !== null && filter !== void 0 ? filter : (filter = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new count_documents_1.CountDocumentsOperation(this, filter, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    // Implementation\n    distinct(key, filter, options, callback) {\n        if (typeof filter === 'function') {\n            (callback = filter), (filter = {}), (options = {});\n        }\n        else {\n            if (arguments.length === 3 && typeof options === 'function') {\n                (callback = options), (options = {});\n            }\n        }\n        filter !== null && filter !== void 0 ? filter : (filter = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new distinct_1.DistinctOperation(this, key, filter, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    indexes(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new indexes_1.IndexesOperation(this, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    stats(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = options !== null && options !== void 0 ? options : {};\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new stats_1.CollStatsOperation(this, options), callback);\n    }\n    findOneAndDelete(filter, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new find_and_modify_1.FindOneAndDeleteOperation(this, filter, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    findOneAndReplace(filter, replacement, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new find_and_modify_1.FindOneAndReplaceOperation(this, filter, replacement, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    findOneAndUpdate(filter, update, options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new find_and_modify_1.FindOneAndUpdateOperation(this, filter, update, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    /**\n     * Execute an aggregation framework pipeline against the collection, needs MongoDB \\>= 2.2\n     *\n     * @param pipeline - An array of aggregation pipelines to execute\n     * @param options - Optional settings for the command\n     */\n    aggregate(pipeline = [], options) {\n        if (arguments.length > 2) {\n            throw new error_1.MongoInvalidArgumentError('Method \"collection.aggregate()\" accepts at most two arguments');\n        }\n        if (!Array.isArray(pipeline)) {\n            throw new error_1.MongoInvalidArgumentError('Argument \"pipeline\" must be an array of aggregation stages');\n        }\n        if (typeof options === 'function') {\n            throw new error_1.MongoInvalidArgumentError('Argument \"options\" must not be function');\n        }\n        return new aggregation_cursor_1.AggregationCursor((0, utils_2.getTopology)(this), this.s.namespace, pipeline, (0, utils_1.resolveOptions)(this, options));\n    }\n    /**\n     * Create a new Change Stream, watching for new changes (insertions, updates, replacements, deletions, and invalidations) in this collection.\n     *\n     * @since 3.0.0\n     * @param pipeline - An array of {@link https://docs.mongodb.com/manual/reference/operator/aggregation-pipeline/|aggregation pipeline stages} through which to pass change stream documents. This allows for filtering (using $match) and manipulating the change stream documents.\n     * @param options - Optional settings for the command\n     */\n    watch(pipeline = [], options = {}) {\n        // Allow optionally not specifying a pipeline\n        if (!Array.isArray(pipeline)) {\n            options = pipeline;\n            pipeline = [];\n        }\n        return new change_stream_1.ChangeStream(this, pipeline, (0, utils_1.resolveOptions)(this, options));\n    }\n    mapReduce(map, reduce, options, callback) {\n        (0, utils_1.emitWarningOnce)('collection.mapReduce is deprecated. Use the aggregation pipeline instead. Visit https://docs.mongodb.com/manual/reference/map-reduce-to-aggregation-pipeline for more information on how to translate map-reduce operations to the aggregation pipeline.');\n        if ('function' === typeof options)\n            (callback = options), (options = {});\n        // Out must always be defined (make sure we don't break weirdly on pre 1.8+ servers)\n        // TODO NODE-3339: Figure out if this is still necessary given we no longer officially support pre-1.8\n        if ((options === null || options === void 0 ? void 0 : options.out) == null) {\n            throw new error_1.MongoInvalidArgumentError('Option \"out\" must be defined, see mongodb docs for possible values');\n        }\n        if ('function' === typeof map) {\n            map = map.toString();\n        }\n        if ('function' === typeof reduce) {\n            reduce = reduce.toString();\n        }\n        if ('function' === typeof options.finalize) {\n            options.finalize = options.finalize.toString();\n        }\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new map_reduce_1.MapReduceOperation(this, map, reduce, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n    /** Initiate an Out of order batch write operation. All operations will be buffered into insert/update/remove commands executed out of order. */\n    initializeUnorderedBulkOp(options) {\n        return new unordered_1.UnorderedBulkOperation(this, (0, utils_1.resolveOptions)(this, options));\n    }\n    /** Initiate an In order bulk write operation. Operations will be serially executed in the order they are added, creating a new operation for each switch in types. */\n    initializeOrderedBulkOp(options) {\n        return new ordered_1.OrderedBulkOperation(this, (0, utils_1.resolveOptions)(this, options));\n    }\n    /** Get the db scoped logger */\n    getLogger() {\n        return this.s.db.s.logger;\n    }\n    get logger() {\n        return this.s.db.s.logger;\n    }\n    /**\n     * Inserts a single document or a an array of documents into MongoDB. If documents passed in do not contain the **_id** field,\n     * one will be added to each of the documents missing it by the driver, mutating the document. This behavior\n     * can be overridden by setting the **forceServerObjectId** flag.\n     *\n     * @deprecated Use insertOne, insertMany or bulkWrite instead.\n     * @param docs - The documents to insert\n     * @param options - Optional settings for the command\n     * @param callback - An optional callback, a Promise will be returned if none is provided\n     */\n    insert(docs, options, callback) {\n        (0, utils_1.emitWarningOnce)('collection.insert is deprecated. Use insertOne, insertMany or bulkWrite instead.');\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = options || { ordered: false };\n        docs = !Array.isArray(docs) ? [docs] : docs;\n        if (options.keepGoing === true) {\n            options.ordered = false;\n        }\n        return this.insertMany(docs, options, callback);\n    }\n    /**\n     * Updates documents.\n     *\n     * @deprecated use updateOne, updateMany or bulkWrite\n     * @param selector - The selector for the update operation.\n     * @param update - The update operations to be applied to the documents\n     * @param options - Optional settings for the command\n     * @param callback - An optional callback, a Promise will be returned if none is provided\n     */\n    update(selector, update, options, callback) {\n        (0, utils_1.emitWarningOnce)('collection.update is deprecated. Use updateOne, updateMany, or bulkWrite instead.');\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = options !== null && options !== void 0 ? options : {};\n        return this.updateMany(selector, update, options, callback);\n    }\n    /**\n     * Remove documents.\n     *\n     * @deprecated use deleteOne, deleteMany or bulkWrite\n     * @param selector - The selector for the update operation.\n     * @param options - Optional settings for the command\n     * @param callback - An optional callback, a Promise will be returned if none is provided\n     */\n    remove(selector, options, callback) {\n        (0, utils_1.emitWarningOnce)('collection.remove is deprecated. Use deleteOne, deleteMany, or bulkWrite instead.');\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = options !== null && options !== void 0 ? options : {};\n        return this.deleteMany(selector, options, callback);\n    }\n    count(filter, options, callback) {\n        if (typeof filter === 'function') {\n            (callback = filter), (filter = {}), (options = {});\n        }\n        else {\n            if (typeof options === 'function')\n                (callback = options), (options = {});\n        }\n        filter !== null && filter !== void 0 ? filter : (filter = {});\n        return (0, execute_operation_1.executeOperation)((0, utils_2.getTopology)(this), new count_documents_1.CountDocumentsOperation(this, filter, (0, utils_1.resolveOptions)(this, options)), callback);\n    }\n}\nexports.Collection = Collection;\n//# sourceMappingURL=collection.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.UnorderedBulkOperation = void 0;\nconst BSON = require(\"../bson\");\nconst common_1 = require(\"./common\");\nconst error_1 = require(\"../error\");\n/** @public */\nclass UnorderedBulkOperation extends common_1.BulkOperationBase {\n    constructor(collection, options) {\n        super(collection, options, false);\n    }\n    handleWriteError(callback, writeResult) {\n        if (this.s.batches.length) {\n            return false;\n        }\n        return super.handleWriteError(callback, writeResult);\n    }\n    addToOperationsList(batchType, document) {\n        // Get the bsonSize\n        const bsonSize = BSON.calculateObjectSize(document, {\n            checkKeys: false,\n            // Since we don't know what the user selected for BSON options here,\n            // err on the safe side, and check the size with ignoreUndefined: false.\n            ignoreUndefined: false\n        });\n        // Throw error if the doc is bigger than the max BSON size\n        if (bsonSize >= this.s.maxBsonObjectSize) {\n            // TODO(NODE-3483): Change this to MongoBSONError\n            throw new error_1.MongoInvalidArgumentError(`Document is larger than the maximum size ${this.s.maxBsonObjectSize}`);\n        }\n        // Holds the current batch\n        this.s.currentBatch = undefined;\n        // Get the right type of batch\n        if (batchType === common_1.BatchType.INSERT) {\n            this.s.currentBatch = this.s.currentInsertBatch;\n        }\n        else if (batchType === common_1.BatchType.UPDATE) {\n            this.s.currentBatch = this.s.currentUpdateBatch;\n        }\n        else if (batchType === common_1.BatchType.DELETE) {\n            this.s.currentBatch = this.s.currentRemoveBatch;\n        }\n        const maxKeySize = this.s.maxKeySize;\n        // Create a new batch object if we don't have a current one\n        if (this.s.currentBatch == null) {\n            this.s.currentBatch = new common_1.Batch(batchType, this.s.currentIndex);\n        }\n        // Check if we need to create a new batch\n        if (\n        // New batch if we exceed the max batch op size\n        this.s.currentBatch.size + 1 >= this.s.maxWriteBatchSize ||\n            // New batch if we exceed the maxBatchSizeBytes. Only matters if batch already has a doc,\n            // since we can't sent an empty batch\n            (this.s.currentBatch.size > 0 &&\n                this.s.currentBatch.sizeBytes + maxKeySize + bsonSize >= this.s.maxBatchSizeBytes) ||\n            // New batch if the new op does not have the same op type as the current batch\n            this.s.currentBatch.batchType !== batchType) {\n            // Save the batch to the execution stack\n            this.s.batches.push(this.s.currentBatch);\n            // Create a new batch\n            this.s.currentBatch = new common_1.Batch(batchType, this.s.currentIndex);\n        }\n        // We have an array of documents\n        if (Array.isArray(document)) {\n            throw new error_1.MongoInvalidArgumentError('Operation passed in cannot be an Array');\n        }\n        this.s.currentBatch.operations.push(document);\n        this.s.currentBatch.originalIndexes.push(this.s.currentIndex);\n        this.s.currentIndex = this.s.currentIndex + 1;\n        // Save back the current Batch to the right type\n        if (batchType === common_1.BatchType.INSERT) {\n            this.s.currentInsertBatch = this.s.currentBatch;\n            this.s.bulkResult.insertedIds.push({\n                index: this.s.bulkResult.insertedIds.length,\n                _id: document._id\n            });\n        }\n        else if (batchType === common_1.BatchType.UPDATE) {\n            this.s.currentUpdateBatch = this.s.currentBatch;\n        }\n        else if (batchType === common_1.BatchType.DELETE) {\n            this.s.currentRemoveBatch = this.s.currentBatch;\n        }\n        // Update current batch size\n        this.s.currentBatch.size += 1;\n        this.s.currentBatch.sizeBytes += maxKeySize + bsonSize;\n        return this;\n    }\n}\nexports.UnorderedBulkOperation = UnorderedBulkOperation;\n//# sourceMappingURL=unordered.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.BulkOperationBase = exports.FindOperators = exports.MongoBulkWriteError = exports.mergeBatchResults = exports.WriteError = exports.WriteConcernError = exports.BulkWriteResult = exports.Batch = exports.BatchType = void 0;\nconst promise_provider_1 = require(\"../promise_provider\");\nconst bson_1 = require(\"../bson\");\nconst error_1 = require(\"../error\");\nconst utils_1 = require(\"../utils\");\nconst execute_operation_1 = require(\"../operations/execute_operation\");\nconst insert_1 = require(\"../operations/insert\");\nconst update_1 = require(\"../operations/update\");\nconst delete_1 = require(\"../operations/delete\");\nconst write_concern_1 = require(\"../write_concern\");\n/** @internal */\nconst kServerError = Symbol('serverError');\n/** @public */\nexports.BatchType = Object.freeze({\n    INSERT: 1,\n    UPDATE: 2,\n    DELETE: 3\n});\n/**\n * Keeps the state of a unordered batch so we can rewrite the results\n * correctly after command execution\n *\n * @public\n */\nclass Batch {\n    constructor(batchType, originalZeroIndex) {\n        this.originalZeroIndex = originalZeroIndex;\n        this.currentIndex = 0;\n        this.originalIndexes = [];\n        this.batchType = batchType;\n        this.operations = [];\n        this.size = 0;\n        this.sizeBytes = 0;\n    }\n}\nexports.Batch = Batch;\n/**\n * @public\n * The result of a bulk write.\n */\nclass BulkWriteResult {\n    /**\n     * Create a new BulkWriteResult instance\n     * @internal\n     */\n    constructor(bulkResult) {\n        this.result = bulkResult;\n    }\n    /** Number of documents inserted. */\n    get insertedCount() {\n        var _a;\n        return (_a = this.result.nInserted) !== null && _a !== void 0 ? _a : 0;\n    }\n    /** Number of documents matched for update. */\n    get matchedCount() {\n        var _a;\n        return (_a = this.result.nMatched) !== null && _a !== void 0 ? _a : 0;\n    }\n    /** Number of documents modified. */\n    get modifiedCount() {\n        var _a;\n        return (_a = this.result.nModified) !== null && _a !== void 0 ? _a : 0;\n    }\n    /** Number of documents deleted. */\n    get deletedCount() {\n        var _a;\n        return (_a = this.result.nRemoved) !== null && _a !== void 0 ? _a : 0;\n    }\n    /** Number of documents upserted. */\n    get upsertedCount() {\n        var _a;\n        return (_a = this.result.upserted.length) !== null && _a !== void 0 ? _a : 0;\n    }\n    /** Upserted document generated Id's, hash key is the index of the originating operation */\n    get upsertedIds() {\n        var _a;\n        const upserted = {};\n        for (const doc of (_a = this.result.upserted) !== null && _a !== void 0 ? _a : []) {\n            upserted[doc.index] = doc._id;\n        }\n        return upserted;\n    }\n    /** Inserted document generated Id's, hash key is the index of the originating operation */\n    get insertedIds() {\n        var _a;\n        const inserted = {};\n        for (const doc of (_a = this.result.insertedIds) !== null && _a !== void 0 ? _a : []) {\n            inserted[doc.index] = doc._id;\n        }\n        return inserted;\n    }\n    /** Evaluates to true if the bulk operation correctly executes */\n    get ok() {\n        return this.result.ok;\n    }\n    /** The number of inserted documents */\n    get nInserted() {\n        return this.result.nInserted;\n    }\n    /** Number of upserted documents */\n    get nUpserted() {\n        return this.result.nUpserted;\n    }\n    /** Number of matched documents */\n    get nMatched() {\n        return this.result.nMatched;\n    }\n    /** Number of documents updated physically on disk */\n    get nModified() {\n        return this.result.nModified;\n    }\n    /** Number of removed documents */\n    get nRemoved() {\n        return this.result.nRemoved;\n    }\n    /** Returns an array of all inserted ids */\n    getInsertedIds() {\n        return this.result.insertedIds;\n    }\n    /** Returns an array of all upserted ids */\n    getUpsertedIds() {\n        return this.result.upserted;\n    }\n    /** Returns the upserted id at the given index */\n    getUpsertedIdAt(index) {\n        return this.result.upserted[index];\n    }\n    /** Returns raw internal result */\n    getRawResponse() {\n        return this.result;\n    }\n    /** Returns true if the bulk operation contains a write error */\n    hasWriteErrors() {\n        return this.result.writeErrors.length > 0;\n    }\n    /** Returns the number of write errors off the bulk operation */\n    getWriteErrorCount() {\n        return this.result.writeErrors.length;\n    }\n    /** Returns a specific write error object */\n    getWriteErrorAt(index) {\n        if (index < this.result.writeErrors.length) {\n            return this.result.writeErrors[index];\n        }\n    }\n    /** Retrieve all write errors */\n    getWriteErrors() {\n        return this.result.writeErrors;\n    }\n    /** Retrieve lastOp if available */\n    getLastOp() {\n        return this.result.opTime;\n    }\n    /** Retrieve the write concern error if one exists */\n    getWriteConcernError() {\n        if (this.result.writeConcernErrors.length === 0) {\n            return;\n        }\n        else if (this.result.writeConcernErrors.length === 1) {\n            // Return the error\n            return this.result.writeConcernErrors[0];\n        }\n        else {\n            // Combine the errors\n            let errmsg = '';\n            for (let i = 0; i < this.result.writeConcernErrors.length; i++) {\n                const err = this.result.writeConcernErrors[i];\n                errmsg = errmsg + err.errmsg;\n                // TODO: Something better\n                if (i === 0)\n                    errmsg = errmsg + ' and ';\n            }\n            return new WriteConcernError({ errmsg, code: error_1.MONGODB_ERROR_CODES.WriteConcernFailed });\n        }\n    }\n    toJSON() {\n        return this.result;\n    }\n    toString() {\n        return `BulkWriteResult(${this.toJSON()})`;\n    }\n    isOk() {\n        return this.result.ok === 1;\n    }\n}\nexports.BulkWriteResult = BulkWriteResult;\n/**\n * An error representing a failure by the server to apply the requested write concern to the bulk operation.\n * @public\n * @category Error\n */\nclass WriteConcernError {\n    constructor(error) {\n        this[kServerError] = error;\n    }\n    /** Write concern error code. */\n    get code() {\n        return this[kServerError].code;\n    }\n    /** Write concern error message. */\n    get errmsg() {\n        return this[kServerError].errmsg;\n    }\n    /** Write concern error info. */\n    get errInfo() {\n        return this[kServerError].errInfo;\n    }\n    /** @deprecated The `err` prop that contained a MongoServerError has been deprecated. */\n    get err() {\n        return this[kServerError];\n    }\n    toJSON() {\n        return this[kServerError];\n    }\n    toString() {\n        return `WriteConcernError(${this.errmsg})`;\n    }\n}\nexports.WriteConcernError = WriteConcernError;\n/**\n * An error that occurred during a BulkWrite on the server.\n * @public\n * @category Error\n */\nclass WriteError {\n    constructor(err) {\n        this.err = err;\n    }\n    /** WriteError code. */\n    get code() {\n        return this.err.code;\n    }\n    /** WriteError original bulk operation index. */\n    get index() {\n        return this.err.index;\n    }\n    /** WriteError message. */\n    get errmsg() {\n        return this.err.errmsg;\n    }\n    /** WriteError details. */\n    get errInfo() {\n        return this.err.errInfo;\n    }\n    /** Returns the underlying operation that caused the error */\n    getOperation() {\n        return this.err.op;\n    }\n    toJSON() {\n        return { code: this.err.code, index: this.err.index, errmsg: this.err.errmsg, op: this.err.op };\n    }\n    toString() {\n        return `WriteError(${JSON.stringify(this.toJSON())})`;\n    }\n}\nexports.WriteError = WriteError;\n/** Converts the number to a Long or returns it. */\nfunction longOrConvert(value) {\n    return typeof value === 'number' ? bson_1.Long.fromNumber(value) : value;\n}\n/** Merges results into shared data structure */\nfunction mergeBatchResults(batch, bulkResult, err, result) {\n    // If we have an error set the result to be the err object\n    if (err) {\n        result = err;\n    }\n    else if (result && result.result) {\n        result = result.result;\n    }\n    if (result == null) {\n        return;\n    }\n    // Do we have a top level error stop processing and return\n    if (result.ok === 0 && bulkResult.ok === 1) {\n        bulkResult.ok = 0;\n        const writeError = {\n            index: 0,\n            code: result.code || 0,\n            errmsg: result.message,\n            errInfo: result.errInfo,\n            op: batch.operations[0]\n        };\n        bulkResult.writeErrors.push(new WriteError(writeError));\n        return;\n    }\n    else if (result.ok === 0 && bulkResult.ok === 0) {\n        return;\n    }\n    // The server write command specification states that lastOp is an optional\n    // mongod only field that has a type of timestamp. Across various scarce specs\n    // where opTime is mentioned, it is an \"opaque\" object that can have a \"ts\" and\n    // \"t\" field with Timestamp and Long as their types respectively.\n    // The \"lastOp\" field of the bulk write result is never mentioned in the driver\n    // specifications or the bulk write spec, so we should probably just keep its\n    // value consistent since it seems to vary.\n    // See: https://github.com/mongodb/specifications/blob/master/source/driver-bulk-update.rst#results-object\n    if (result.opTime || result.lastOp) {\n        let opTime = result.lastOp || result.opTime;\n        // If the opTime is a Timestamp, convert it to a consistent format to be\n        // able to compare easily. Converting to the object from a timestamp is\n        // much more straightforward than the other direction.\n        if (opTime._bsontype === 'Timestamp') {\n            opTime = { ts: opTime, t: bson_1.Long.ZERO };\n        }\n        // If there's no lastOp, just set it.\n        if (!bulkResult.opTime) {\n            bulkResult.opTime = opTime;\n        }\n        else {\n            // First compare the ts values and set if the opTimeTS value is greater.\n            const lastOpTS = longOrConvert(bulkResult.opTime.ts);\n            const opTimeTS = longOrConvert(opTime.ts);\n            if (opTimeTS.greaterThan(lastOpTS)) {\n                bulkResult.opTime = opTime;\n            }\n            else if (opTimeTS.equals(lastOpTS)) {\n                // If the ts values are equal, then compare using the t values.\n                const lastOpT = longOrConvert(bulkResult.opTime.t);\n                const opTimeT = longOrConvert(opTime.t);\n                if (opTimeT.greaterThan(lastOpT)) {\n                    bulkResult.opTime = opTime;\n                }\n            }\n        }\n    }\n    // If we have an insert Batch type\n    if (isInsertBatch(batch) && result.n) {\n        bulkResult.nInserted = bulkResult.nInserted + result.n;\n    }\n    // If we have an insert Batch type\n    if (isDeleteBatch(batch) && result.n) {\n        bulkResult.nRemoved = bulkResult.nRemoved + result.n;\n    }\n    let nUpserted = 0;\n    // We have an array of upserted values, we need to rewrite the indexes\n    if (Array.isArray(result.upserted)) {\n        nUpserted = result.upserted.length;\n        for (let i = 0; i < result.upserted.length; i++) {\n            bulkResult.upserted.push({\n                index: result.upserted[i].index + batch.originalZeroIndex,\n                _id: result.upserted[i]._id\n            });\n        }\n    }\n    else if (result.upserted) {\n        nUpserted = 1;\n        bulkResult.upserted.push({\n            index: batch.originalZeroIndex,\n            _id: result.upserted\n        });\n    }\n    // If we have an update Batch type\n    if (isUpdateBatch(batch) && result.n) {\n        const nModified = result.nModified;\n        bulkResult.nUpserted = bulkResult.nUpserted + nUpserted;\n        bulkResult.nMatched = bulkResult.nMatched + (result.n - nUpserted);\n        if (typeof nModified === 'number') {\n            bulkResult.nModified = bulkResult.nModified + nModified;\n        }\n        else {\n            bulkResult.nModified = 0;\n        }\n    }\n    if (Array.isArray(result.writeErrors)) {\n        for (let i = 0; i < result.writeErrors.length; i++) {\n            const writeError = {\n                index: batch.originalIndexes[result.writeErrors[i].index],\n                code: result.writeErrors[i].code,\n                errmsg: result.writeErrors[i].errmsg,\n                errInfo: result.writeErrors[i].errInfo,\n                op: batch.operations[result.writeErrors[i].index]\n            };\n            bulkResult.writeErrors.push(new WriteError(writeError));\n        }\n    }\n    if (result.writeConcernError) {\n        bulkResult.writeConcernErrors.push(new WriteConcernError(result.writeConcernError));\n    }\n}\nexports.mergeBatchResults = mergeBatchResults;\nfunction executeCommands(bulkOperation, options, callback) {\n    if (bulkOperation.s.batches.length === 0) {\n        return callback(undefined, new BulkWriteResult(bulkOperation.s.bulkResult));\n    }\n    const batch = bulkOperation.s.batches.shift();\n    function resultHandler(err, result) {\n        // Error is a driver related error not a bulk op error, return early\n        if (err && 'message' in err && !(err instanceof error_1.MongoWriteConcernError)) {\n            return callback(new MongoBulkWriteError(err, new BulkWriteResult(bulkOperation.s.bulkResult)));\n        }\n        if (err instanceof error_1.MongoWriteConcernError) {\n            return handleMongoWriteConcernError(batch, bulkOperation.s.bulkResult, err, callback);\n        }\n        // Merge the results together\n        const writeResult = new BulkWriteResult(bulkOperation.s.bulkResult);\n        const mergeResult = mergeBatchResults(batch, bulkOperation.s.bulkResult, err, result);\n        if (mergeResult != null) {\n            return callback(undefined, writeResult);\n        }\n        if (bulkOperation.handleWriteError(callback, writeResult))\n            return;\n        // Execute the next command in line\n        executeCommands(bulkOperation, options, callback);\n    }\n    const finalOptions = (0, utils_1.resolveOptions)(bulkOperation, {\n        ...options,\n        ordered: bulkOperation.isOrdered\n    });\n    if (finalOptions.bypassDocumentValidation !== true) {\n        delete finalOptions.bypassDocumentValidation;\n    }\n    // Set an operationIf if provided\n    if (bulkOperation.operationId) {\n        resultHandler.operationId = bulkOperation.operationId;\n    }\n    // Is the bypassDocumentValidation options specific\n    if (bulkOperation.s.bypassDocumentValidation === true) {\n        finalOptions.bypassDocumentValidation = true;\n    }\n    // Is the checkKeys option disabled\n    if (bulkOperation.s.checkKeys === false) {\n        finalOptions.checkKeys = false;\n    }\n    if (finalOptions.retryWrites) {\n        if (isUpdateBatch(batch)) {\n            finalOptions.retryWrites = finalOptions.retryWrites && !batch.operations.some(op => op.multi);\n        }\n        if (isDeleteBatch(batch)) {\n            finalOptions.retryWrites =\n                finalOptions.retryWrites && !batch.operations.some(op => op.limit === 0);\n        }\n    }\n    try {\n        if (isInsertBatch(batch)) {\n            (0, execute_operation_1.executeOperation)(bulkOperation.s.topology, new insert_1.InsertOperation(bulkOperation.s.namespace, batch.operations, finalOptions), resultHandler);\n        }\n        else if (isUpdateBatch(batch)) {\n            (0, execute_operation_1.executeOperation)(bulkOperation.s.topology, new update_1.UpdateOperation(bulkOperation.s.namespace, batch.operations, finalOptions), resultHandler);\n        }\n        else if (isDeleteBatch(batch)) {\n            (0, execute_operation_1.executeOperation)(bulkOperation.s.topology, new delete_1.DeleteOperation(bulkOperation.s.namespace, batch.operations, finalOptions), resultHandler);\n        }\n    }\n    catch (err) {\n        // Force top level error\n        err.ok = 0;\n        // Merge top level error and return\n        mergeBatchResults(batch, bulkOperation.s.bulkResult, err, undefined);\n        callback();\n    }\n}\nfunction handleMongoWriteConcernError(batch, bulkResult, err, callback) {\n    var _a, _b;\n    mergeBatchResults(batch, bulkResult, undefined, err.result);\n    callback(new MongoBulkWriteError({\n        message: (_a = err.result) === null || _a === void 0 ? void 0 : _a.writeConcernError.errmsg,\n        code: (_b = err.result) === null || _b === void 0 ? void 0 : _b.writeConcernError.result\n    }, new BulkWriteResult(bulkResult)));\n}\n/**\n * An error indicating an unsuccessful Bulk Write\n * @public\n * @category Error\n */\nclass MongoBulkWriteError extends error_1.MongoServerError {\n    /** Creates a new MongoBulkWriteError */\n    constructor(error, result) {\n        var _a;\n        super(error);\n        this.writeErrors = [];\n        if (error instanceof WriteConcernError)\n            this.err = error;\n        else if (!(error instanceof Error)) {\n            this.message = error.message;\n            this.code = error.code;\n            this.writeErrors = (_a = error.writeErrors) !== null && _a !== void 0 ? _a : [];\n        }\n        this.result = result;\n        Object.assign(this, error);\n    }\n    get name() {\n        return 'MongoBulkWriteError';\n    }\n    /** Number of documents inserted. */\n    get insertedCount() {\n        return this.result.insertedCount;\n    }\n    /** Number of documents matched for update. */\n    get matchedCount() {\n        return this.result.matchedCount;\n    }\n    /** Number of documents modified. */\n    get modifiedCount() {\n        return this.result.modifiedCount;\n    }\n    /** Number of documents deleted. */\n    get deletedCount() {\n        return this.result.deletedCount;\n    }\n    /** Number of documents upserted. */\n    get upsertedCount() {\n        return this.result.upsertedCount;\n    }\n    /** Inserted document generated Id's, hash key is the index of the originating operation */\n    get insertedIds() {\n        return this.result.insertedIds;\n    }\n    /** Upserted document generated Id's, hash key is the index of the originating operation */\n    get upsertedIds() {\n        return this.result.upsertedIds;\n    }\n}\nexports.MongoBulkWriteError = MongoBulkWriteError;\n/**\n * A builder object that is returned from {@link BulkOperationBase#find}.\n * Is used to build a write operation that involves a query filter.\n *\n * @public\n */\nclass FindOperators {\n    /**\n     * Creates a new FindOperators object.\n     * @internal\n     */\n    constructor(bulkOperation) {\n        this.bulkOperation = bulkOperation;\n    }\n    /** Add a multiple update operation to the bulk operation */\n    update(updateDocument) {\n        const currentOp = buildCurrentOp(this.bulkOperation);\n        return this.bulkOperation.addToOperationsList(exports.BatchType.UPDATE, (0, update_1.makeUpdateStatement)(currentOp.selector, updateDocument, {\n            ...currentOp,\n            multi: true\n        }));\n    }\n    /** Add a single update operation to the bulk operation */\n    updateOne(updateDocument) {\n        if (!(0, utils_1.hasAtomicOperators)(updateDocument)) {\n            throw new error_1.MongoInvalidArgumentError('Update document requires atomic operators');\n        }\n        const currentOp = buildCurrentOp(this.bulkOperation);\n        return this.bulkOperation.addToOperationsList(exports.BatchType.UPDATE, (0, update_1.makeUpdateStatement)(currentOp.selector, updateDocument, { ...currentOp, multi: false }));\n    }\n    /** Add a replace one operation to the bulk operation */\n    replaceOne(replacement) {\n        if ((0, utils_1.hasAtomicOperators)(replacement)) {\n            throw new error_1.MongoInvalidArgumentError('Replacement document must not use atomic operators');\n        }\n        const currentOp = buildCurrentOp(this.bulkOperation);\n        return this.bulkOperation.addToOperationsList(exports.BatchType.UPDATE, (0, update_1.makeUpdateStatement)(currentOp.selector, replacement, { ...currentOp, multi: false }));\n    }\n    /** Add a delete one operation to the bulk operation */\n    deleteOne() {\n        const currentOp = buildCurrentOp(this.bulkOperation);\n        return this.bulkOperation.addToOperationsList(exports.BatchType.DELETE, (0, delete_1.makeDeleteStatement)(currentOp.selector, { ...currentOp, limit: 1 }));\n    }\n    /** Add a delete many operation to the bulk operation */\n    delete() {\n        const currentOp = buildCurrentOp(this.bulkOperation);\n        return this.bulkOperation.addToOperationsList(exports.BatchType.DELETE, (0, delete_1.makeDeleteStatement)(currentOp.selector, { ...currentOp, limit: 0 }));\n    }\n    /** Upsert modifier for update bulk operation, noting that this operation is an upsert. */\n    upsert() {\n        if (!this.bulkOperation.s.currentOp) {\n            this.bulkOperation.s.currentOp = {};\n        }\n        this.bulkOperation.s.currentOp.upsert = true;\n        return this;\n    }\n    /** Specifies the collation for the query condition. */\n    collation(collation) {\n        if (!this.bulkOperation.s.currentOp) {\n            this.bulkOperation.s.currentOp = {};\n        }\n        this.bulkOperation.s.currentOp.collation = collation;\n        return this;\n    }\n    /** Specifies arrayFilters for UpdateOne or UpdateMany bulk operations. */\n    arrayFilters(arrayFilters) {\n        if (!this.bulkOperation.s.currentOp) {\n            this.bulkOperation.s.currentOp = {};\n        }\n        this.bulkOperation.s.currentOp.arrayFilters = arrayFilters;\n        return this;\n    }\n}\nexports.FindOperators = FindOperators;\n/** @public */\nclass BulkOperationBase {\n    /**\n     * Create a new OrderedBulkOperation or UnorderedBulkOperation instance\n     * @internal\n     */\n    constructor(collection, options, isOrdered) {\n        // determine whether bulkOperation is ordered or unordered\n        this.isOrdered = isOrdered;\n        const topology = (0, utils_1.getTopology)(collection);\n        options = options == null ? {} : options;\n        // TODO Bring from driver information in isMaster\n        // Get the namespace for the write operations\n        const namespace = collection.s.namespace;\n        // Used to mark operation as executed\n        const executed = false;\n        // Current item\n        const currentOp = undefined;\n        // Set max byte size\n        const isMaster = topology.lastIsMaster();\n        // If we have autoEncryption on, batch-splitting must be done on 2mb chunks, but single documents\n        // over 2mb are still allowed\n        const usingAutoEncryption = !!(topology.s.options && topology.s.options.autoEncrypter);\n        const maxBsonObjectSize = isMaster && isMaster.maxBsonObjectSize ? isMaster.maxBsonObjectSize : 1024 * 1024 * 16;\n        const maxBatchSizeBytes = usingAutoEncryption ? 1024 * 1024 * 2 : maxBsonObjectSize;\n        const maxWriteBatchSize = isMaster && isMaster.maxWriteBatchSize ? isMaster.maxWriteBatchSize : 1000;\n        // Calculates the largest possible size of an Array key, represented as a BSON string\n        // element. This calculation:\n        //     1 byte for BSON type\n        //     # of bytes = length of (string representation of (maxWriteBatchSize - 1))\n        //   + 1 bytes for null terminator\n        const maxKeySize = (maxWriteBatchSize - 1).toString(10).length + 2;\n        // Final options for retryable writes\n        let finalOptions = Object.assign({}, options);\n        finalOptions = (0, utils_1.applyRetryableWrites)(finalOptions, collection.s.db);\n        // Final results\n        const bulkResult = {\n            ok: 1,\n            writeErrors: [],\n            writeConcernErrors: [],\n            insertedIds: [],\n            nInserted: 0,\n            nUpserted: 0,\n            nMatched: 0,\n            nModified: 0,\n            nRemoved: 0,\n            upserted: []\n        };\n        // Internal state\n        this.s = {\n            // Final result\n            bulkResult,\n            // Current batch state\n            currentBatch: undefined,\n            currentIndex: 0,\n            // ordered specific\n            currentBatchSize: 0,\n            currentBatchSizeBytes: 0,\n            // unordered specific\n            currentInsertBatch: undefined,\n            currentUpdateBatch: undefined,\n            currentRemoveBatch: undefined,\n            batches: [],\n            // Write concern\n            writeConcern: write_concern_1.WriteConcern.fromOptions(options),\n            // Max batch size options\n            maxBsonObjectSize,\n            maxBatchSizeBytes,\n            maxWriteBatchSize,\n            maxKeySize,\n            // Namespace\n            namespace,\n            // Topology\n            topology,\n            // Options\n            options: finalOptions,\n            // BSON options\n            bsonOptions: (0, bson_1.resolveBSONOptions)(options),\n            // Current operation\n            currentOp,\n            // Executed\n            executed,\n            // Collection\n            collection,\n            // Fundamental error\n            err: undefined,\n            // check keys\n            checkKeys: typeof options.checkKeys === 'boolean' ? options.checkKeys : false\n        };\n        // bypass Validation\n        if (options.bypassDocumentValidation === true) {\n            this.s.bypassDocumentValidation = true;\n        }\n    }\n    /**\n     * Add a single insert document to the bulk operation\n     *\n     * @example\n     * ```js\n     * const bulkOp = collection.initializeOrderedBulkOp();\n     *\n     * // Adds three inserts to the bulkOp.\n     * bulkOp\n     *   .insert({ a: 1 })\n     *   .insert({ b: 2 })\n     *   .insert({ c: 3 });\n     * await bulkOp.execute();\n     * ```\n     */\n    insert(document) {\n        if (document._id == null && !shouldForceServerObjectId(this)) {\n            document._id = new bson_1.ObjectId();\n        }\n        return this.addToOperationsList(exports.BatchType.INSERT, document);\n    }\n    /**\n     * Builds a find operation for an update/updateOne/delete/deleteOne/replaceOne.\n     * Returns a builder object used to complete the definition of the operation.\n     *\n     * @example\n     * ```js\n     * const bulkOp = collection.initializeOrderedBulkOp();\n     *\n     * // Add an updateOne to the bulkOp\n     * bulkOp.find({ a: 1 }).updateOne({ $set: { b: 2 } });\n     *\n     * // Add an updateMany to the bulkOp\n     * bulkOp.find({ c: 3 }).update({ $set: { d: 4 } });\n     *\n     * // Add an upsert\n     * bulkOp.find({ e: 5 }).upsert().updateOne({ $set: { f: 6 } });\n     *\n     * // Add a deletion\n     * bulkOp.find({ g: 7 }).deleteOne();\n     *\n     * // Add a multi deletion\n     * bulkOp.find({ h: 8 }).delete();\n     *\n     * // Add a replaceOne\n     * bulkOp.find({ i: 9 }).replaceOne({writeConcern: { j: 10 }});\n     *\n     * // Update using a pipeline (requires Mongodb 4.2 or higher)\n     * bulk.find({ k: 11, y: { $exists: true }, z: { $exists: true } }).updateOne([\n     *   { $set: { total: { $sum: [ '$y', '$z' ] } } }\n     * ]);\n     *\n     * // All of the ops will now be executed\n     * await bulkOp.execute();\n     * ```\n     */\n    find(selector) {\n        if (!selector) {\n            throw new error_1.MongoInvalidArgumentError('Bulk find operation must specify a selector');\n        }\n        // Save a current selector\n        this.s.currentOp = {\n            selector: selector\n        };\n        return new FindOperators(this);\n    }\n    /** Specifies a raw operation to perform in the bulk write. */\n    raw(op) {\n        if ('insertOne' in op) {\n            const forceServerObjectId = shouldForceServerObjectId(this);\n            if (op.insertOne && op.insertOne.document == null) {\n                // NOTE: provided for legacy support, but this is a malformed operation\n                if (forceServerObjectId !== true && op.insertOne._id == null) {\n                    op.insertOne._id = new bson_1.ObjectId();\n                }\n                return this.addToOperationsList(exports.BatchType.INSERT, op.insertOne);\n            }\n            if (forceServerObjectId !== true && op.insertOne.document._id == null) {\n                op.insertOne.document._id = new bson_1.ObjectId();\n            }\n            return this.addToOperationsList(exports.BatchType.INSERT, op.insertOne.document);\n        }\n        if ('replaceOne' in op || 'updateOne' in op || 'updateMany' in op) {\n            if ('replaceOne' in op) {\n                if ('q' in op.replaceOne) {\n                    throw new error_1.MongoInvalidArgumentError('Raw operations are not allowed');\n                }\n                const updateStatement = (0, update_1.makeUpdateStatement)(op.replaceOne.filter, op.replaceOne.replacement, { ...op.replaceOne, multi: false });\n                if ((0, utils_1.hasAtomicOperators)(updateStatement.u)) {\n                    throw new error_1.MongoInvalidArgumentError('Replacement document must not use atomic operators');\n                }\n                return this.addToOperationsList(exports.BatchType.UPDATE, updateStatement);\n            }\n            if ('updateOne' in op) {\n                if ('q' in op.updateOne) {\n                    throw new error_1.MongoInvalidArgumentError('Raw operations are not allowed');\n                }\n                const updateStatement = (0, update_1.makeUpdateStatement)(op.updateOne.filter, op.updateOne.update, {\n                    ...op.updateOne,\n                    multi: false\n                });\n                if (!(0, utils_1.hasAtomicOperators)(updateStatement.u)) {\n                    throw new error_1.MongoInvalidArgumentError('Update document requires atomic operators');\n                }\n                return this.addToOperationsList(exports.BatchType.UPDATE, updateStatement);\n            }\n            if ('updateMany' in op) {\n                if ('q' in op.updateMany) {\n                    throw new error_1.MongoInvalidArgumentError('Raw operations are not allowed');\n                }\n                const updateStatement = (0, update_1.makeUpdateStatement)(op.updateMany.filter, op.updateMany.update, {\n                    ...op.updateMany,\n                    multi: true\n                });\n                if (!(0, utils_1.hasAtomicOperators)(updateStatement.u)) {\n                    throw new error_1.MongoInvalidArgumentError('Update document requires atomic operators');\n                }\n                return this.addToOperationsList(exports.BatchType.UPDATE, updateStatement);\n            }\n        }\n        if ('deleteOne' in op) {\n            if ('q' in op.deleteOne) {\n                throw new error_1.MongoInvalidArgumentError('Raw operations are not allowed');\n            }\n            return this.addToOperationsList(exports.BatchType.DELETE, (0, delete_1.makeDeleteStatement)(op.deleteOne.filter, { ...op.deleteOne, limit: 1 }));\n        }\n        if ('deleteMany' in op) {\n            if ('q' in op.deleteMany) {\n                throw new error_1.MongoInvalidArgumentError('Raw operations are not allowed');\n            }\n            return this.addToOperationsList(exports.BatchType.DELETE, (0, delete_1.makeDeleteStatement)(op.deleteMany.filter, { ...op.deleteMany, limit: 0 }));\n        }\n        // otherwise an unknown operation was provided\n        throw new error_1.MongoInvalidArgumentError('bulkWrite only supports insertOne, updateOne, updateMany, deleteOne, deleteMany');\n    }\n    get bsonOptions() {\n        return this.s.bsonOptions;\n    }\n    get writeConcern() {\n        return this.s.writeConcern;\n    }\n    get batches() {\n        const batches = [...this.s.batches];\n        if (this.isOrdered) {\n            if (this.s.currentBatch)\n                batches.push(this.s.currentBatch);\n        }\n        else {\n            if (this.s.currentInsertBatch)\n                batches.push(this.s.currentInsertBatch);\n            if (this.s.currentUpdateBatch)\n                batches.push(this.s.currentUpdateBatch);\n            if (this.s.currentRemoveBatch)\n                batches.push(this.s.currentRemoveBatch);\n        }\n        return batches;\n    }\n    execute(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = options !== null && options !== void 0 ? options : {};\n        if (this.s.executed) {\n            return handleEarlyError(new error_1.MongoBatchReExecutionError(), callback);\n        }\n        const writeConcern = write_concern_1.WriteConcern.fromOptions(options);\n        if (writeConcern) {\n            this.s.writeConcern = writeConcern;\n        }\n        // If we have current batch\n        if (this.isOrdered) {\n            if (this.s.currentBatch)\n                this.s.batches.push(this.s.currentBatch);\n        }\n        else {\n            if (this.s.currentInsertBatch)\n                this.s.batches.push(this.s.currentInsertBatch);\n            if (this.s.currentUpdateBatch)\n                this.s.batches.push(this.s.currentUpdateBatch);\n            if (this.s.currentRemoveBatch)\n                this.s.batches.push(this.s.currentRemoveBatch);\n        }\n        // If we have no operations in the bulk raise an error\n        if (this.s.batches.length === 0) {\n            const emptyBatchError = new error_1.MongoInvalidArgumentError('Invalid BulkOperation, Batch cannot be empty');\n            return handleEarlyError(emptyBatchError, callback);\n        }\n        this.s.executed = true;\n        const finalOptions = { ...this.s.options, ...options };\n        return (0, utils_1.executeLegacyOperation)(this.s.topology, executeCommands, [this, finalOptions, callback]);\n    }\n    /**\n     * Handles the write error before executing commands\n     * @internal\n     */\n    handleWriteError(callback, writeResult) {\n        if (this.s.bulkResult.writeErrors.length > 0) {\n            const msg = this.s.bulkResult.writeErrors[0].errmsg\n                ? this.s.bulkResult.writeErrors[0].errmsg\n                : 'write operation failed';\n            callback(new MongoBulkWriteError({\n                message: msg,\n                code: this.s.bulkResult.writeErrors[0].code,\n                writeErrors: this.s.bulkResult.writeErrors\n            }, writeResult));\n            return true;\n        }\n        const writeConcernError = writeResult.getWriteConcernError();\n        if (writeConcernError) {\n            callback(new MongoBulkWriteError(writeConcernError, writeResult));\n            return true;\n        }\n    }\n}\nexports.BulkOperationBase = BulkOperationBase;\nObject.defineProperty(BulkOperationBase.prototype, 'length', {\n    enumerable: true,\n    get() {\n        return this.s.currentIndex;\n    }\n});\n/** helper function to assist with promiseOrCallback behavior */\nfunction handleEarlyError(err, callback) {\n    const Promise = promise_provider_1.PromiseProvider.get();\n    if (typeof callback === 'function') {\n        callback(err);\n        return;\n    }\n    return Promise.reject(err);\n}\nfunction shouldForceServerObjectId(bulkOperation) {\n    var _a, _b;\n    if (typeof bulkOperation.s.options.forceServerObjectId === 'boolean') {\n        return bulkOperation.s.options.forceServerObjectId;\n    }\n    if (typeof ((_a = bulkOperation.s.collection.s.db.options) === null || _a === void 0 ? void 0 : _a.forceServerObjectId) === 'boolean') {\n        return (_b = bulkOperation.s.collection.s.db.options) === null || _b === void 0 ? void 0 : _b.forceServerObjectId;\n    }\n    return false;\n}\nfunction isInsertBatch(batch) {\n    return batch.batchType === exports.BatchType.INSERT;\n}\nfunction isUpdateBatch(batch) {\n    return batch.batchType === exports.BatchType.UPDATE;\n}\nfunction isDeleteBatch(batch) {\n    return batch.batchType === exports.BatchType.DELETE;\n}\nfunction buildCurrentOp(bulkOp) {\n    let { currentOp } = bulkOp.s;\n    bulkOp.s.currentOp = undefined;\n    if (!currentOp)\n        currentOp = {};\n    return currentOp;\n}\n//# sourceMappingURL=common.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.InsertManyOperation = exports.InsertOneOperation = exports.InsertOperation = void 0;\nconst error_1 = require(\"../error\");\nconst operation_1 = require(\"./operation\");\nconst command_1 = require(\"./command\");\nconst common_functions_1 = require(\"./common_functions\");\nconst write_concern_1 = require(\"../write_concern\");\nconst bulk_write_1 = require(\"./bulk_write\");\n/** @internal */\nclass InsertOperation extends command_1.CommandOperation {\n    constructor(ns, documents, options) {\n        var _a;\n        super(undefined, options);\n        this.options = { ...options, checkKeys: (_a = options.checkKeys) !== null && _a !== void 0 ? _a : false };\n        this.ns = ns;\n        this.documents = documents;\n    }\n    execute(server, session, callback) {\n        var _a;\n        const options = (_a = this.options) !== null && _a !== void 0 ? _a : {};\n        const ordered = typeof options.ordered === 'boolean' ? options.ordered : true;\n        const command = {\n            insert: this.ns.collection,\n            documents: this.documents,\n            ordered\n        };\n        if (typeof options.bypassDocumentValidation === 'boolean') {\n            command.bypassDocumentValidation = options.bypassDocumentValidation;\n        }\n        if (options.comment != null) {\n            command.comment = options.comment;\n        }\n        super.executeCommand(server, session, command, callback);\n    }\n}\nexports.InsertOperation = InsertOperation;\nclass InsertOneOperation extends InsertOperation {\n    constructor(collection, doc, options) {\n        super(collection.s.namespace, (0, common_functions_1.prepareDocs)(collection, [doc], options), options);\n    }\n    execute(server, session, callback) {\n        super.execute(server, session, (err, res) => {\n            var _a, _b;\n            if (err || res == null)\n                return callback(err);\n            if (res.code)\n                return callback(new error_1.MongoServerError(res));\n            if (res.writeErrors) {\n                // This should be a WriteError but we can't change it now because of error hierarchy\n                return callback(new error_1.MongoServerError(res.writeErrors[0]));\n            }\n            callback(undefined, {\n                acknowledged: (_b = ((_a = this.writeConcern) === null || _a === void 0 ? void 0 : _a.w) !== 0) !== null && _b !== void 0 ? _b : true,\n                insertedId: this.documents[0]._id\n            });\n        });\n    }\n}\nexports.InsertOneOperation = InsertOneOperation;\n/** @internal */\nclass InsertManyOperation extends operation_1.AbstractOperation {\n    constructor(collection, docs, options) {\n        super(options);\n        if (!Array.isArray(docs)) {\n            throw new error_1.MongoInvalidArgumentError('Argument \"docs\" must be an array of documents');\n        }\n        this.options = options;\n        this.collection = collection;\n        this.docs = docs;\n    }\n    execute(server, session, callback) {\n        const coll = this.collection;\n        const options = { ...this.options, ...this.bsonOptions, readPreference: this.readPreference };\n        const writeConcern = write_concern_1.WriteConcern.fromOptions(options);\n        const bulkWriteOperation = new bulk_write_1.BulkWriteOperation(coll, (0, common_functions_1.prepareDocs)(coll, this.docs, options).map(document => ({ insertOne: { document } })), options);\n        bulkWriteOperation.execute(server, session, (err, res) => {\n            var _a;\n            if (err || res == null)\n                return callback(err);\n            callback(undefined, {\n                acknowledged: (_a = (writeConcern === null || writeConcern === void 0 ? void 0 : writeConcern.w) !== 0) !== null && _a !== void 0 ? _a : true,\n                insertedCount: res.insertedCount,\n                insertedIds: res.insertedIds\n            });\n        });\n    }\n}\nexports.InsertManyOperation = InsertManyOperation;\n(0, operation_1.defineAspects)(InsertOperation, [operation_1.Aspect.RETRYABLE, operation_1.Aspect.WRITE_OPERATION]);\n(0, operation_1.defineAspects)(InsertOneOperation, [operation_1.Aspect.RETRYABLE, operation_1.Aspect.WRITE_OPERATION]);\n(0, operation_1.defineAspects)(InsertManyOperation, [operation_1.Aspect.WRITE_OPERATION]);\n//# sourceMappingURL=insert.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.BulkWriteOperation = void 0;\nconst operation_1 = require(\"./operation\");\n/** @internal */\nclass BulkWriteOperation extends operation_1.AbstractOperation {\n    constructor(collection, operations, options) {\n        super(options);\n        this.options = options;\n        this.collection = collection;\n        this.operations = operations;\n    }\n    execute(server, session, callback) {\n        const coll = this.collection;\n        const operations = this.operations;\n        const options = { ...this.options, ...this.bsonOptions, readPreference: this.readPreference };\n        // Create the bulk operation\n        const bulk = options.ordered === false\n            ? coll.initializeUnorderedBulkOp(options)\n            : coll.initializeOrderedBulkOp(options);\n        // for each op go through and add to the bulk\n        try {\n            for (let i = 0; i < operations.length; i++) {\n                bulk.raw(operations[i]);\n            }\n        }\n        catch (err) {\n            return callback(err);\n        }\n        // Execute the bulk\n        bulk.execute({ ...options, session }, (err, r) => {\n            // We have connection level error\n            if (!r && err) {\n                return callback(err);\n            }\n            // Return the results\n            callback(undefined, r);\n        });\n    }\n}\nexports.BulkWriteOperation = BulkWriteOperation;\n(0, operation_1.defineAspects)(BulkWriteOperation, [operation_1.Aspect.WRITE_OPERATION]);\n//# sourceMappingURL=bulk_write.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.makeUpdateStatement = exports.ReplaceOneOperation = exports.UpdateManyOperation = exports.UpdateOneOperation = exports.UpdateOperation = void 0;\nconst operation_1 = require(\"./operation\");\nconst utils_1 = require(\"../utils\");\nconst command_1 = require(\"./command\");\nconst error_1 = require(\"../error\");\n/** @internal */\nclass UpdateOperation extends command_1.CommandOperation {\n    constructor(ns, statements, options) {\n        super(undefined, options);\n        this.options = options;\n        this.ns = ns;\n        this.statements = statements;\n    }\n    get canRetryWrite() {\n        if (super.canRetryWrite === false) {\n            return false;\n        }\n        return this.statements.every(op => op.multi == null || op.multi === false);\n    }\n    execute(server, session, callback) {\n        var _a;\n        const options = (_a = this.options) !== null && _a !== void 0 ? _a : {};\n        const ordered = typeof options.ordered === 'boolean' ? options.ordered : true;\n        const command = {\n            update: this.ns.collection,\n            updates: this.statements,\n            ordered\n        };\n        if (typeof options.bypassDocumentValidation === 'boolean') {\n            command.bypassDocumentValidation = options.bypassDocumentValidation;\n        }\n        if (options.let) {\n            command.let = options.let;\n        }\n        const statementWithCollation = this.statements.find(statement => !!statement.collation);\n        if ((0, utils_1.collationNotSupported)(server, options) ||\n            (statementWithCollation && (0, utils_1.collationNotSupported)(server, statementWithCollation))) {\n            callback(new error_1.MongoCompatibilityError(`Server ${server.name} does not support collation`));\n            return;\n        }\n        const unacknowledgedWrite = this.writeConcern && this.writeConcern.w === 0;\n        if (unacknowledgedWrite || (0, utils_1.maxWireVersion)(server) < 5) {\n            if (this.statements.find((o) => o.hint)) {\n                callback(new error_1.MongoCompatibilityError(`Servers < 3.4 do not support hint on update`));\n                return;\n            }\n        }\n        if (this.explain && (0, utils_1.maxWireVersion)(server) < 3) {\n            callback(new error_1.MongoCompatibilityError(`Server ${server.name} does not support explain on update`));\n            return;\n        }\n        if (this.statements.some(statement => !!statement.arrayFilters) && (0, utils_1.maxWireVersion)(server) < 6) {\n            callback(new error_1.MongoCompatibilityError('Option \"arrayFilters\" is only supported on MongoDB 3.6+'));\n            return;\n        }\n        super.executeCommand(server, session, command, callback);\n    }\n}\nexports.UpdateOperation = UpdateOperation;\n/** @internal */\nclass UpdateOneOperation extends UpdateOperation {\n    constructor(collection, filter, update, options) {\n        super(collection.s.namespace, [makeUpdateStatement(filter, update, { ...options, multi: false })], options);\n        if (!(0, utils_1.hasAtomicOperators)(update)) {\n            throw new error_1.MongoInvalidArgumentError('Update document requires atomic operators');\n        }\n    }\n    execute(server, session, callback) {\n        super.execute(server, session, (err, res) => {\n            var _a, _b;\n            if (err || !res)\n                return callback(err);\n            if (this.explain != null)\n                return callback(undefined, res);\n            if (res.code)\n                return callback(new error_1.MongoServerError(res));\n            if (res.writeErrors)\n                return callback(new error_1.MongoServerError(res.writeErrors[0]));\n            callback(undefined, {\n                acknowledged: (_b = ((_a = this.writeConcern) === null || _a === void 0 ? void 0 : _a.w) !== 0) !== null && _b !== void 0 ? _b : true,\n                modifiedCount: res.nModified != null ? res.nModified : res.n,\n                upsertedId: Array.isArray(res.upserted) && res.upserted.length > 0 ? res.upserted[0]._id : null,\n                upsertedCount: Array.isArray(res.upserted) && res.upserted.length ? res.upserted.length : 0,\n                matchedCount: Array.isArray(res.upserted) && res.upserted.length > 0 ? 0 : res.n\n            });\n        });\n    }\n}\nexports.UpdateOneOperation = UpdateOneOperation;\n/** @internal */\nclass UpdateManyOperation extends UpdateOperation {\n    constructor(collection, filter, update, options) {\n        super(collection.s.namespace, [makeUpdateStatement(filter, update, { ...options, multi: true })], options);\n        if (!(0, utils_1.hasAtomicOperators)(update)) {\n            throw new error_1.MongoInvalidArgumentError('Update document requires atomic operators');\n        }\n    }\n    execute(server, session, callback) {\n        super.execute(server, session, (err, res) => {\n            var _a, _b;\n            if (err || !res)\n                return callback(err);\n            if (this.explain != null)\n                return callback(undefined, res);\n            if (res.code)\n                return callback(new error_1.MongoServerError(res));\n            if (res.writeErrors)\n                return callback(new error_1.MongoServerError(res.writeErrors[0]));\n            callback(undefined, {\n                acknowledged: (_b = ((_a = this.writeConcern) === null || _a === void 0 ? void 0 : _a.w) !== 0) !== null && _b !== void 0 ? _b : true,\n                modifiedCount: res.nModified != null ? res.nModified : res.n,\n                upsertedId: Array.isArray(res.upserted) && res.upserted.length > 0 ? res.upserted[0]._id : null,\n                upsertedCount: Array.isArray(res.upserted) && res.upserted.length ? res.upserted.length : 0,\n                matchedCount: Array.isArray(res.upserted) && res.upserted.length > 0 ? 0 : res.n\n            });\n        });\n    }\n}\nexports.UpdateManyOperation = UpdateManyOperation;\n/** @internal */\nclass ReplaceOneOperation extends UpdateOperation {\n    constructor(collection, filter, replacement, options) {\n        super(collection.s.namespace, [makeUpdateStatement(filter, replacement, { ...options, multi: false })], options);\n        if ((0, utils_1.hasAtomicOperators)(replacement)) {\n            throw new error_1.MongoInvalidArgumentError('Replacement document must not contain atomic operators');\n        }\n    }\n    execute(server, session, callback) {\n        super.execute(server, session, (err, res) => {\n            var _a, _b;\n            if (err || !res)\n                return callback(err);\n            if (this.explain != null)\n                return callback(undefined, res);\n            if (res.code)\n                return callback(new error_1.MongoServerError(res));\n            if (res.writeErrors)\n                return callback(new error_1.MongoServerError(res.writeErrors[0]));\n            callback(undefined, {\n                acknowledged: (_b = ((_a = this.writeConcern) === null || _a === void 0 ? void 0 : _a.w) !== 0) !== null && _b !== void 0 ? _b : true,\n                modifiedCount: res.nModified != null ? res.nModified : res.n,\n                upsertedId: Array.isArray(res.upserted) && res.upserted.length > 0 ? res.upserted[0]._id : null,\n                upsertedCount: Array.isArray(res.upserted) && res.upserted.length ? res.upserted.length : 0,\n                matchedCount: Array.isArray(res.upserted) && res.upserted.length > 0 ? 0 : res.n\n            });\n        });\n    }\n}\nexports.ReplaceOneOperation = ReplaceOneOperation;\nfunction makeUpdateStatement(filter, update, options) {\n    if (filter == null || typeof filter !== 'object') {\n        throw new error_1.MongoInvalidArgumentError('Selector must be a valid JavaScript object');\n    }\n    if (update == null || typeof update !== 'object') {\n        throw new error_1.MongoInvalidArgumentError('Document must be a valid JavaScript object');\n    }\n    const op = { q: filter, u: update };\n    if (typeof options.upsert === 'boolean') {\n        op.upsert = options.upsert;\n    }\n    if (options.multi) {\n        op.multi = options.multi;\n    }\n    if (options.hint) {\n        op.hint = options.hint;\n    }\n    if (options.arrayFilters) {\n        op.arrayFilters = options.arrayFilters;\n    }\n    if (options.collation) {\n        op.collation = options.collation;\n    }\n    return op;\n}\nexports.makeUpdateStatement = makeUpdateStatement;\n(0, operation_1.defineAspects)(UpdateOperation, [operation_1.Aspect.RETRYABLE, operation_1.Aspect.WRITE_OPERATION, operation_1.Aspect.SKIP_COLLATION]);\n(0, operation_1.defineAspects)(UpdateOneOperation, [\n    operation_1.Aspect.RETRYABLE,\n    operation_1.Aspect.WRITE_OPERATION,\n    operation_1.Aspect.EXPLAINABLE,\n    operation_1.Aspect.SKIP_COLLATION\n]);\n(0, operation_1.defineAspects)(UpdateManyOperation, [\n    operation_1.Aspect.WRITE_OPERATION,\n    operation_1.Aspect.EXPLAINABLE,\n    operation_1.Aspect.SKIP_COLLATION\n]);\n(0, operation_1.defineAspects)(ReplaceOneOperation, [\n    operation_1.Aspect.RETRYABLE,\n    operation_1.Aspect.WRITE_OPERATION,\n    operation_1.Aspect.SKIP_COLLATION\n]);\n//# sourceMappingURL=update.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.makeDeleteStatement = exports.DeleteManyOperation = exports.DeleteOneOperation = exports.DeleteOperation = void 0;\nconst operation_1 = require(\"./operation\");\nconst command_1 = require(\"./command\");\nconst utils_1 = require(\"../utils\");\nconst error_1 = require(\"../error\");\n/** @internal */\nclass DeleteOperation extends command_1.CommandOperation {\n    constructor(ns, statements, options) {\n        super(undefined, options);\n        this.options = options;\n        this.ns = ns;\n        this.statements = statements;\n    }\n    get canRetryWrite() {\n        if (super.canRetryWrite === false) {\n            return false;\n        }\n        return this.statements.every(op => (op.limit != null ? op.limit > 0 : true));\n    }\n    execute(server, session, callback) {\n        var _a;\n        const options = (_a = this.options) !== null && _a !== void 0 ? _a : {};\n        const ordered = typeof options.ordered === 'boolean' ? options.ordered : true;\n        const command = {\n            delete: this.ns.collection,\n            deletes: this.statements,\n            ordered\n        };\n        if (options.let) {\n            command.let = options.let;\n        }\n        if (options.explain != null && (0, utils_1.maxWireVersion)(server) < 3) {\n            return callback\n                ? callback(new error_1.MongoCompatibilityError(`Server ${server.name} does not support explain on delete`))\n                : undefined;\n        }\n        const unacknowledgedWrite = this.writeConcern && this.writeConcern.w === 0;\n        if (unacknowledgedWrite || (0, utils_1.maxWireVersion)(server) < 5) {\n            if (this.statements.find((o) => o.hint)) {\n                callback(new error_1.MongoCompatibilityError(`Servers < 3.4 do not support hint on delete`));\n                return;\n            }\n        }\n        const statementWithCollation = this.statements.find(statement => !!statement.collation);\n        if (statementWithCollation && (0, utils_1.collationNotSupported)(server, statementWithCollation)) {\n            callback(new error_1.MongoCompatibilityError(`Server ${server.name} does not support collation`));\n            return;\n        }\n        super.executeCommand(server, session, command, callback);\n    }\n}\nexports.DeleteOperation = DeleteOperation;\nclass DeleteOneOperation extends DeleteOperation {\n    constructor(collection, filter, options) {\n        super(collection.s.namespace, [makeDeleteStatement(filter, { ...options, limit: 1 })], options);\n    }\n    execute(server, session, callback) {\n        super.execute(server, session, (err, res) => {\n            var _a, _b;\n            if (err || res == null)\n                return callback(err);\n            if (res.code)\n                return callback(new error_1.MongoServerError(res));\n            if (res.writeErrors)\n                return callback(new error_1.MongoServerError(res.writeErrors[0]));\n            if (this.explain)\n                return callback(undefined, res);\n            callback(undefined, {\n                acknowledged: (_b = ((_a = this.writeConcern) === null || _a === void 0 ? void 0 : _a.w) !== 0) !== null && _b !== void 0 ? _b : true,\n                deletedCount: res.n\n            });\n        });\n    }\n}\nexports.DeleteOneOperation = DeleteOneOperation;\nclass DeleteManyOperation extends DeleteOperation {\n    constructor(collection, filter, options) {\n        super(collection.s.namespace, [makeDeleteStatement(filter, options)], options);\n    }\n    execute(server, session, callback) {\n        super.execute(server, session, (err, res) => {\n            var _a, _b;\n            if (err || res == null)\n                return callback(err);\n            if (res.code)\n                return callback(new error_1.MongoServerError(res));\n            if (res.writeErrors)\n                return callback(new error_1.MongoServerError(res.writeErrors[0]));\n            if (this.explain)\n                return callback(undefined, res);\n            callback(undefined, {\n                acknowledged: (_b = ((_a = this.writeConcern) === null || _a === void 0 ? void 0 : _a.w) !== 0) !== null && _b !== void 0 ? _b : true,\n                deletedCount: res.n\n            });\n        });\n    }\n}\nexports.DeleteManyOperation = DeleteManyOperation;\nfunction makeDeleteStatement(filter, options) {\n    const op = {\n        q: filter,\n        limit: typeof options.limit === 'number' ? options.limit : 0\n    };\n    if (options.single === true) {\n        op.limit = 1;\n    }\n    if (options.collation) {\n        op.collation = options.collation;\n    }\n    if (options.hint) {\n        op.hint = options.hint;\n    }\n    if (options.comment) {\n        op.comment = options.comment;\n    }\n    return op;\n}\nexports.makeDeleteStatement = makeDeleteStatement;\n(0, operation_1.defineAspects)(DeleteOperation, [operation_1.Aspect.RETRYABLE, operation_1.Aspect.WRITE_OPERATION]);\n(0, operation_1.defineAspects)(DeleteOneOperation, [\n    operation_1.Aspect.RETRYABLE,\n    operation_1.Aspect.WRITE_OPERATION,\n    operation_1.Aspect.EXPLAINABLE,\n    operation_1.Aspect.SKIP_COLLATION\n]);\n(0, operation_1.defineAspects)(DeleteManyOperation, [\n    operation_1.Aspect.WRITE_OPERATION,\n    operation_1.Aspect.EXPLAINABLE,\n    operation_1.Aspect.SKIP_COLLATION\n]);\n//# sourceMappingURL=delete.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.OrderedBulkOperation = void 0;\nconst BSON = require(\"../bson\");\nconst common_1 = require(\"./common\");\nconst error_1 = require(\"../error\");\n/** @public */\nclass OrderedBulkOperation extends common_1.BulkOperationBase {\n    constructor(collection, options) {\n        super(collection, options, true);\n    }\n    addToOperationsList(batchType, document) {\n        // Get the bsonSize\n        const bsonSize = BSON.calculateObjectSize(document, {\n            checkKeys: false,\n            // Since we don't know what the user selected for BSON options here,\n            // err on the safe side, and check the size with ignoreUndefined: false.\n            ignoreUndefined: false\n        });\n        // Throw error if the doc is bigger than the max BSON size\n        if (bsonSize >= this.s.maxBsonObjectSize)\n            // TODO(NODE-3483): Change this to MongoBSONError\n            throw new error_1.MongoInvalidArgumentError(`Document is larger than the maximum size ${this.s.maxBsonObjectSize}`);\n        // Create a new batch object if we don't have a current one\n        if (this.s.currentBatch == null) {\n            this.s.currentBatch = new common_1.Batch(batchType, this.s.currentIndex);\n        }\n        const maxKeySize = this.s.maxKeySize;\n        // Check if we need to create a new batch\n        if (\n        // New batch if we exceed the max batch op size\n        this.s.currentBatchSize + 1 >= this.s.maxWriteBatchSize ||\n            // New batch if we exceed the maxBatchSizeBytes. Only matters if batch already has a doc,\n            // since we can't sent an empty batch\n            (this.s.currentBatchSize > 0 &&\n                this.s.currentBatchSizeBytes + maxKeySize + bsonSize >= this.s.maxBatchSizeBytes) ||\n            // New batch if the new op does not have the same op type as the current batch\n            this.s.currentBatch.batchType !== batchType) {\n            // Save the batch to the execution stack\n            this.s.batches.push(this.s.currentBatch);\n            // Create a new batch\n            this.s.currentBatch = new common_1.Batch(batchType, this.s.currentIndex);\n            // Reset the current size trackers\n            this.s.currentBatchSize = 0;\n            this.s.currentBatchSizeBytes = 0;\n        }\n        if (batchType === common_1.BatchType.INSERT) {\n            this.s.bulkResult.insertedIds.push({\n                index: this.s.currentIndex,\n                _id: document._id\n            });\n        }\n        // We have an array of documents\n        if (Array.isArray(document)) {\n            throw new error_1.MongoInvalidArgumentError('Operation passed in cannot be an Array');\n        }\n        this.s.currentBatch.originalIndexes.push(this.s.currentIndex);\n        this.s.currentBatch.operations.push(document);\n        this.s.currentBatchSize += 1;\n        this.s.currentBatchSizeBytes += maxKeySize + bsonSize;\n        this.s.currentIndex += 1;\n        return this;\n    }\n}\nexports.OrderedBulkOperation = OrderedBulkOperation;\n//# sourceMappingURL=ordered.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.ChangeStreamCursor = exports.ChangeStream = void 0;\nconst Denque = require(\"denque\");\nconst error_1 = require(\"./error\");\nconst aggregate_1 = require(\"./operations/aggregate\");\nconst utils_1 = require(\"./utils\");\nconst mongo_client_1 = require(\"./mongo_client\");\nconst db_1 = require(\"./db\");\nconst collection_1 = require(\"./collection\");\nconst abstract_cursor_1 = require(\"./cursor/abstract_cursor\");\nconst execute_operation_1 = require(\"./operations/execute_operation\");\nconst mongo_types_1 = require(\"./mongo_types\");\n/** @internal */\nconst kResumeQueue = Symbol('resumeQueue');\n/** @internal */\nconst kCursorStream = Symbol('cursorStream');\n/** @internal */\nconst kClosed = Symbol('closed');\n/** @internal */\nconst kMode = Symbol('mode');\nconst CHANGE_STREAM_OPTIONS = ['resumeAfter', 'startAfter', 'startAtOperationTime', 'fullDocument'];\nconst CURSOR_OPTIONS = ['batchSize', 'maxAwaitTimeMS', 'collation', 'readPreference'].concat(CHANGE_STREAM_OPTIONS);\nconst CHANGE_DOMAIN_TYPES = {\n    COLLECTION: Symbol('Collection'),\n    DATABASE: Symbol('Database'),\n    CLUSTER: Symbol('Cluster')\n};\nconst NO_RESUME_TOKEN_ERROR = 'A change stream document has been received that lacks a resume token (_id).';\nconst NO_CURSOR_ERROR = 'ChangeStream has no cursor';\nconst CHANGESTREAM_CLOSED_ERROR = 'ChangeStream is closed';\n/**\n * Creates a new Change Stream instance. Normally created using {@link Collection#watch|Collection.watch()}.\n * @public\n */\nclass ChangeStream extends mongo_types_1.TypedEventEmitter {\n    /**\n     * @internal\n     *\n     * @param parent - The parent object that created this change stream\n     * @param pipeline - An array of {@link https://docs.mongodb.com/manual/reference/operator/aggregation-pipeline/|aggregation pipeline stages} through which to pass change stream documents\n     */\n    constructor(parent, pipeline = [], options = {}) {\n        super();\n        this.pipeline = pipeline;\n        this.options = options;\n        if (parent instanceof collection_1.Collection) {\n            this.type = CHANGE_DOMAIN_TYPES.COLLECTION;\n        }\n        else if (parent instanceof db_1.Db) {\n            this.type = CHANGE_DOMAIN_TYPES.DATABASE;\n        }\n        else if (parent instanceof mongo_client_1.MongoClient) {\n            this.type = CHANGE_DOMAIN_TYPES.CLUSTER;\n        }\n        else {\n            throw new error_1.MongoChangeStreamError('Parent provided to ChangeStream constructor must be an instance of Collection, Db, or MongoClient');\n        }\n        this.parent = parent;\n        this.namespace = parent.s.namespace;\n        if (!this.options.readPreference && parent.readPreference) {\n            this.options.readPreference = parent.readPreference;\n        }\n        this[kResumeQueue] = new Denque();\n        // Create contained Change Stream cursor\n        this.cursor = createChangeStreamCursor(this, options);\n        this[kClosed] = false;\n        this[kMode] = false;\n        // Listen for any `change` listeners being added to ChangeStream\n        this.on('newListener', eventName => {\n            if (eventName === 'change' && this.cursor && this.listenerCount('change') === 0) {\n                streamEvents(this, this.cursor);\n            }\n        });\n        this.on('removeListener', eventName => {\n            var _a;\n            if (eventName === 'change' && this.listenerCount('change') === 0 && this.cursor) {\n                (_a = this[kCursorStream]) === null || _a === void 0 ? void 0 : _a.removeAllListeners('data');\n            }\n        });\n    }\n    /** @internal */\n    get cursorStream() {\n        return this[kCursorStream];\n    }\n    /** The cached resume token that is used to resume after the most recently returned change. */\n    get resumeToken() {\n        var _a;\n        return (_a = this.cursor) === null || _a === void 0 ? void 0 : _a.resumeToken;\n    }\n    hasNext(callback) {\n        setIsIterator(this);\n        return (0, utils_1.maybePromise)(callback, cb => {\n            getCursor(this, (err, cursor) => {\n                if (err || !cursor)\n                    return cb(err); // failed to resume, raise an error\n                cursor.hasNext(cb);\n            });\n        });\n    }\n    next(callback) {\n        setIsIterator(this);\n        return (0, utils_1.maybePromise)(callback, cb => {\n            getCursor(this, (err, cursor) => {\n                if (err || !cursor)\n                    return cb(err); // failed to resume, raise an error\n                cursor.next((error, change) => {\n                    if (error) {\n                        this[kResumeQueue].push(() => this.next(cb));\n                        processError(this, error, cb);\n                        return;\n                    }\n                    processNewChange(this, change, cb);\n                });\n            });\n        });\n    }\n    /** Is the cursor closed */\n    get closed() {\n        var _a, _b;\n        return this[kClosed] || ((_b = (_a = this.cursor) === null || _a === void 0 ? void 0 : _a.closed) !== null && _b !== void 0 ? _b : false);\n    }\n    /** Close the Change Stream */\n    close(callback) {\n        this[kClosed] = true;\n        return (0, utils_1.maybePromise)(callback, cb => {\n            if (!this.cursor) {\n                return cb();\n            }\n            const cursor = this.cursor;\n            return cursor.close(err => {\n                endStream(this);\n                this.cursor = undefined;\n                return cb(err);\n            });\n        });\n    }\n    /**\n     * Return a modified Readable stream including a possible transform method.\n     * @throws MongoDriverError if this.cursor is undefined\n     */\n    stream(options) {\n        this.streamOptions = options;\n        if (!this.cursor)\n            throw new error_1.MongoChangeStreamError(NO_CURSOR_ERROR);\n        return this.cursor.stream(options);\n    }\n    tryNext(callback) {\n        setIsIterator(this);\n        return (0, utils_1.maybePromise)(callback, cb => {\n            getCursor(this, (err, cursor) => {\n                if (err || !cursor)\n                    return cb(err); // failed to resume, raise an error\n                return cursor.tryNext(cb);\n            });\n        });\n    }\n}\nexports.ChangeStream = ChangeStream;\n/** @event */\nChangeStream.RESPONSE = 'response';\n/** @event */\nChangeStream.MORE = 'more';\n/** @event */\nChangeStream.INIT = 'init';\n/** @event */\nChangeStream.CLOSE = 'close';\n/**\n * Fired for each new matching change in the specified namespace. Attaching a `change`\n * event listener to a Change Stream will switch the stream into flowing mode. Data will\n * then be passed as soon as it is available.\n * @event\n */\nChangeStream.CHANGE = 'change';\n/** @event */\nChangeStream.END = 'end';\n/** @event */\nChangeStream.ERROR = 'error';\n/**\n * Emitted each time the change stream stores a new resume token.\n * @event\n */\nChangeStream.RESUME_TOKEN_CHANGED = 'resumeTokenChanged';\n/** @internal */\nclass ChangeStreamCursor extends abstract_cursor_1.AbstractCursor {\n    constructor(topology, namespace, pipeline = [], options = {}) {\n        super(topology, namespace, options);\n        this.pipeline = pipeline;\n        this.options = options;\n        this._resumeToken = null;\n        this.startAtOperationTime = options.startAtOperationTime;\n        if (options.startAfter) {\n            this.resumeToken = options.startAfter;\n        }\n        else if (options.resumeAfter) {\n            this.resumeToken = options.resumeAfter;\n        }\n    }\n    set resumeToken(token) {\n        this._resumeToken = token;\n        this.emit(ChangeStream.RESUME_TOKEN_CHANGED, token);\n    }\n    get resumeToken() {\n        return this._resumeToken;\n    }\n    get resumeOptions() {\n        const result = {};\n        for (const optionName of CURSOR_OPTIONS) {\n            if (Reflect.has(this.options, optionName)) {\n                Reflect.set(result, optionName, Reflect.get(this.options, optionName));\n            }\n        }\n        if (this.resumeToken || this.startAtOperationTime) {\n            ['resumeAfter', 'startAfter', 'startAtOperationTime'].forEach(key => Reflect.deleteProperty(result, key));\n            if (this.resumeToken) {\n                const resumeKey = this.options.startAfter && !this.hasReceived ? 'startAfter' : 'resumeAfter';\n                Reflect.set(result, resumeKey, this.resumeToken);\n            }\n            else if (this.startAtOperationTime && (0, utils_1.maxWireVersion)(this.server) >= 7) {\n                result.startAtOperationTime = this.startAtOperationTime;\n            }\n        }\n        return result;\n    }\n    cacheResumeToken(resumeToken) {\n        if (this.bufferedCount() === 0 && this.postBatchResumeToken) {\n            this.resumeToken = this.postBatchResumeToken;\n        }\n        else {\n            this.resumeToken = resumeToken;\n        }\n        this.hasReceived = true;\n    }\n    _processBatch(batchName, response) {\n        const cursor = (response === null || response === void 0 ? void 0 : response.cursor) || {};\n        if (cursor.postBatchResumeToken) {\n            this.postBatchResumeToken = cursor.postBatchResumeToken;\n            if (cursor[batchName].length === 0) {\n                this.resumeToken = cursor.postBatchResumeToken;\n            }\n        }\n    }\n    clone() {\n        return new ChangeStreamCursor(this.topology, this.namespace, this.pipeline, {\n            ...this.cursorOptions\n        });\n    }\n    _initialize(session, callback) {\n        const aggregateOperation = new aggregate_1.AggregateOperation(this.namespace, this.pipeline, {\n            ...this.cursorOptions,\n            ...this.options,\n            session\n        });\n        (0, execute_operation_1.executeOperation)(this.topology, aggregateOperation, (err, response) => {\n            if (err || response == null) {\n                return callback(err);\n            }\n            const server = aggregateOperation.server;\n            if (this.startAtOperationTime == null &&\n                this.resumeAfter == null &&\n                this.startAfter == null &&\n                (0, utils_1.maxWireVersion)(server) >= 7) {\n                this.startAtOperationTime = response.operationTime;\n            }\n            this._processBatch('firstBatch', response);\n            this.emit(ChangeStream.INIT, response);\n            this.emit(ChangeStream.RESPONSE);\n            // TODO: NODE-2882\n            callback(undefined, { server, session, response });\n        });\n    }\n    _getMore(batchSize, callback) {\n        super._getMore(batchSize, (err, response) => {\n            if (err) {\n                return callback(err);\n            }\n            this._processBatch('nextBatch', response);\n            this.emit(ChangeStream.MORE, response);\n            this.emit(ChangeStream.RESPONSE);\n            callback(err, response);\n        });\n    }\n}\nexports.ChangeStreamCursor = ChangeStreamCursor;\nconst CHANGE_STREAM_EVENTS = [\n    ChangeStream.RESUME_TOKEN_CHANGED,\n    ChangeStream.END,\n    ChangeStream.CLOSE\n];\nfunction setIsEmitter(changeStream) {\n    if (changeStream[kMode] === 'iterator') {\n        // TODO(NODE-3485): Replace with MongoChangeStreamModeError\n        throw new error_1.MongoAPIError('ChangeStream cannot be used as an EventEmitter after being used as an iterator');\n    }\n    changeStream[kMode] = 'emitter';\n}\nfunction setIsIterator(changeStream) {\n    if (changeStream[kMode] === 'emitter') {\n        // TODO(NODE-3485): Replace with MongoChangeStreamModeError\n        throw new error_1.MongoAPIError('ChangeStream cannot be used as an iterator after being used as an EventEmitter');\n    }\n    changeStream[kMode] = 'iterator';\n}\n/**\n * Create a new change stream cursor based on self's configuration\n * @internal\n */\nfunction createChangeStreamCursor(changeStream, options) {\n    const changeStreamStageOptions = { fullDocument: options.fullDocument || 'default' };\n    applyKnownOptions(changeStreamStageOptions, options, CHANGE_STREAM_OPTIONS);\n    if (changeStream.type === CHANGE_DOMAIN_TYPES.CLUSTER) {\n        changeStreamStageOptions.allChangesForCluster = true;\n    }\n    const pipeline = [{ $changeStream: changeStreamStageOptions }].concat(changeStream.pipeline);\n    const cursorOptions = applyKnownOptions({}, options, CURSOR_OPTIONS);\n    const changeStreamCursor = new ChangeStreamCursor((0, utils_1.getTopology)(changeStream.parent), changeStream.namespace, pipeline, cursorOptions);\n    for (const event of CHANGE_STREAM_EVENTS) {\n        changeStreamCursor.on(event, e => changeStream.emit(event, e));\n    }\n    if (changeStream.listenerCount(ChangeStream.CHANGE) > 0) {\n        streamEvents(changeStream, changeStreamCursor);\n    }\n    return changeStreamCursor;\n}\nfunction applyKnownOptions(target, source, optionNames) {\n    optionNames.forEach(name => {\n        if (source[name]) {\n            target[name] = source[name];\n        }\n    });\n    return target;\n}\n// This method performs a basic server selection loop, satisfying the requirements of\n// ChangeStream resumability until the new SDAM layer can be used.\nconst SELECTION_TIMEOUT = 30000;\nfunction waitForTopologyConnected(topology, options, callback) {\n    setTimeout(() => {\n        if (options && options.start == null) {\n            options.start = (0, utils_1.now)();\n        }\n        const start = options.start || (0, utils_1.now)();\n        const timeout = options.timeout || SELECTION_TIMEOUT;\n        if (topology.isConnected()) {\n            return callback();\n        }\n        if ((0, utils_1.calculateDurationInMs)(start) > timeout) {\n            // TODO(NODE-3497): Replace with MongoNetworkTimeoutError\n            return callback(new error_1.MongoRuntimeError('Timed out waiting for connection'));\n        }\n        waitForTopologyConnected(topology, options, callback);\n    }, 500); // this is an arbitrary wait time to allow SDAM to transition\n}\nfunction closeWithError(changeStream, error, callback) {\n    if (!callback) {\n        changeStream.emit(ChangeStream.ERROR, error);\n    }\n    changeStream.close(() => callback && callback(error));\n}\nfunction streamEvents(changeStream, cursor) {\n    setIsEmitter(changeStream);\n    const stream = changeStream[kCursorStream] || cursor.stream();\n    changeStream[kCursorStream] = stream;\n    stream.on('data', change => processNewChange(changeStream, change));\n    stream.on('error', error => processError(changeStream, error));\n}\nfunction endStream(changeStream) {\n    const cursorStream = changeStream[kCursorStream];\n    if (cursorStream) {\n        ['data', 'close', 'end', 'error'].forEach(event => cursorStream.removeAllListeners(event));\n        cursorStream.destroy();\n    }\n    changeStream[kCursorStream] = undefined;\n}\nfunction processNewChange(changeStream, change, callback) {\n    var _a;\n    if (changeStream[kClosed]) {\n        // TODO(NODE-3485): Replace with MongoChangeStreamClosedError\n        if (callback)\n            callback(new error_1.MongoAPIError(CHANGESTREAM_CLOSED_ERROR));\n        return;\n    }\n    // a null change means the cursor has been notified, implicitly closing the change stream\n    if (change == null) {\n        // TODO(NODE-3485): Replace with MongoChangeStreamClosedError\n        return closeWithError(changeStream, new error_1.MongoRuntimeError(CHANGESTREAM_CLOSED_ERROR), callback);\n    }\n    if (change && !change._id) {\n        return closeWithError(changeStream, new error_1.MongoChangeStreamError(NO_RESUME_TOKEN_ERROR), callback);\n    }\n    // cache the resume token\n    (_a = changeStream.cursor) === null || _a === void 0 ? void 0 : _a.cacheResumeToken(change._id);\n    // wipe the startAtOperationTime if there was one so that there won't be a conflict\n    // between resumeToken and startAtOperationTime if we need to reconnect the cursor\n    changeStream.options.startAtOperationTime = undefined;\n    // Return the change\n    if (!callback)\n        return changeStream.emit(ChangeStream.CHANGE, change);\n    return callback(undefined, change);\n}\nfunction processError(changeStream, error, callback) {\n    const cursor = changeStream.cursor;\n    // If the change stream has been closed explicitly, do not process error.\n    if (changeStream[kClosed]) {\n        // TODO(NODE-3485): Replace with MongoChangeStreamClosedError\n        if (callback)\n            callback(new error_1.MongoAPIError(CHANGESTREAM_CLOSED_ERROR));\n        return;\n    }\n    // if the resume succeeds, continue with the new cursor\n    function resumeWithCursor(newCursor) {\n        changeStream.cursor = newCursor;\n        processResumeQueue(changeStream);\n    }\n    // otherwise, raise an error and close the change stream\n    function unresumableError(err) {\n        if (!callback) {\n            changeStream.emit(ChangeStream.ERROR, err);\n        }\n        changeStream.close(() => processResumeQueue(changeStream, err));\n    }\n    if (cursor && (0, error_1.isResumableError)(error, (0, utils_1.maxWireVersion)(cursor.server))) {\n        changeStream.cursor = undefined;\n        // stop listening to all events from old cursor\n        endStream(changeStream);\n        // close internal cursor, ignore errors\n        cursor.close();\n        const topology = (0, utils_1.getTopology)(changeStream.parent);\n        waitForTopologyConnected(topology, { readPreference: cursor.readPreference }, err => {\n            // if the topology can't reconnect, close the stream\n            if (err)\n                return unresumableError(err);\n            // create a new cursor, preserving the old cursor's options\n            const newCursor = createChangeStreamCursor(changeStream, cursor.resumeOptions);\n            // attempt to continue in emitter mode\n            if (!callback)\n                return resumeWithCursor(newCursor);\n            // attempt to continue in iterator mode\n            newCursor.hasNext(err => {\n                // if there's an error immediately after resuming, close the stream\n                if (err)\n                    return unresumableError(err);\n                resumeWithCursor(newCursor);\n            });\n        });\n        return;\n    }\n    // if initial error wasn't resumable, raise an error and close the change stream\n    return closeWithError(changeStream, error, callback);\n}\n/**\n * Safely provides a cursor across resume attempts\n *\n * @param changeStream - the parent ChangeStream\n */\nfunction getCursor(changeStream, callback) {\n    if (changeStream[kClosed]) {\n        // TODO(NODE-3485): Replace with MongoChangeStreamClosedError\n        callback(new error_1.MongoAPIError(CHANGESTREAM_CLOSED_ERROR));\n        return;\n    }\n    // if a cursor exists and it is open, return it\n    if (changeStream.cursor) {\n        callback(undefined, changeStream.cursor);\n        return;\n    }\n    // no cursor, queue callback until topology reconnects\n    changeStream[kResumeQueue].push(callback);\n}\n/**\n * Drain the resume queue when a new has become available\n *\n * @param changeStream - the parent ChangeStream\n * @param err - error getting a new cursor\n */\nfunction processResumeQueue(changeStream, err) {\n    while (changeStream[kResumeQueue].length) {\n        const request = changeStream[kResumeQueue].pop();\n        if (!request)\n            break; // Should never occur but TS can't use the length check in the while condition\n        if (!err) {\n            if (changeStream[kClosed]) {\n                // TODO(NODE-3485): Replace with MongoChangeStreamClosedError\n                request(new error_1.MongoAPIError(CHANGESTREAM_CLOSED_ERROR));\n                return;\n            }\n            if (!changeStream.cursor) {\n                request(new error_1.MongoChangeStreamError(NO_CURSOR_ERROR));\n                return;\n            }\n        }\n        request(err, changeStream.cursor);\n    }\n}\n//# sourceMappingURL=change_stream.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.CountDocumentsOperation = void 0;\nconst aggregate_1 = require(\"./aggregate\");\n/** @internal */\nclass CountDocumentsOperation extends aggregate_1.AggregateOperation {\n    constructor(collection, query, options) {\n        const pipeline = [];\n        pipeline.push({ $match: query });\n        if (typeof options.skip === 'number') {\n            pipeline.push({ $skip: options.skip });\n        }\n        if (typeof options.limit === 'number') {\n            pipeline.push({ $limit: options.limit });\n        }\n        pipeline.push({ $group: { _id: 1, n: { $sum: 1 } } });\n        super(collection.s.namespace, pipeline, options);\n    }\n    execute(server, session, callback) {\n        super.execute(server, session, (err, result) => {\n            if (err || !result) {\n                callback(err);\n                return;\n            }\n            // NOTE: We're avoiding creating a cursor here to reduce the callstack.\n            const response = result;\n            if (response.cursor == null || response.cursor.firstBatch == null) {\n                callback(undefined, 0);\n                return;\n            }\n            const docs = response.cursor.firstBatch;\n            callback(undefined, docs.length ? docs[0].n : 0);\n        });\n    }\n}\nexports.CountDocumentsOperation = CountDocumentsOperation;\n//# sourceMappingURL=count_documents.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.DistinctOperation = void 0;\nconst operation_1 = require(\"./operation\");\nconst command_1 = require(\"./command\");\nconst utils_1 = require(\"../utils\");\nconst error_1 = require(\"../error\");\n/**\n * Return a list of distinct values for the given key across a collection.\n * @internal\n */\nclass DistinctOperation extends command_1.CommandOperation {\n    /**\n     * Construct a Distinct operation.\n     *\n     * @param collection - Collection instance.\n     * @param key - Field of the document to find distinct values for.\n     * @param query - The query for filtering the set of documents to which we apply the distinct filter.\n     * @param options - Optional settings. See Collection.prototype.distinct for a list of options.\n     */\n    constructor(collection, key, query, options) {\n        super(collection, options);\n        this.options = options !== null && options !== void 0 ? options : {};\n        this.collection = collection;\n        this.key = key;\n        this.query = query;\n    }\n    execute(server, session, callback) {\n        const coll = this.collection;\n        const key = this.key;\n        const query = this.query;\n        const options = this.options;\n        // Distinct command\n        const cmd = {\n            distinct: coll.collectionName,\n            key: key,\n            query: query\n        };\n        // Add maxTimeMS if defined\n        if (typeof options.maxTimeMS === 'number') {\n            cmd.maxTimeMS = options.maxTimeMS;\n        }\n        // Do we have a readConcern specified\n        (0, utils_1.decorateWithReadConcern)(cmd, coll, options);\n        // Have we specified collation\n        try {\n            (0, utils_1.decorateWithCollation)(cmd, coll, options);\n        }\n        catch (err) {\n            return callback(err);\n        }\n        if (this.explain && (0, utils_1.maxWireVersion)(server) < 4) {\n            callback(new error_1.MongoCompatibilityError(`Server ${server.name} does not support explain on distinct`));\n            return;\n        }\n        super.executeCommand(server, session, cmd, (err, result) => {\n            if (err) {\n                callback(err);\n                return;\n            }\n            callback(undefined, this.explain ? result : result.values);\n        });\n    }\n}\nexports.DistinctOperation = DistinctOperation;\n(0, operation_1.defineAspects)(DistinctOperation, [operation_1.Aspect.READ_OPERATION, operation_1.Aspect.RETRYABLE, operation_1.Aspect.EXPLAINABLE]);\n//# sourceMappingURL=distinct.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.DropDatabaseOperation = exports.DropCollectionOperation = void 0;\nconst operation_1 = require(\"./operation\");\nconst command_1 = require(\"./command\");\n/** @internal */\nclass DropCollectionOperation extends command_1.CommandOperation {\n    constructor(db, name, options) {\n        super(db, options);\n        this.options = options;\n        this.name = name;\n    }\n    execute(server, session, callback) {\n        super.executeCommand(server, session, { drop: this.name }, (err, result) => {\n            if (err)\n                return callback(err);\n            if (result.ok)\n                return callback(undefined, true);\n            callback(undefined, false);\n        });\n    }\n}\nexports.DropCollectionOperation = DropCollectionOperation;\n/** @internal */\nclass DropDatabaseOperation extends command_1.CommandOperation {\n    constructor(db, options) {\n        super(db, options);\n        this.options = options;\n    }\n    execute(server, session, callback) {\n        super.executeCommand(server, session, { dropDatabase: 1 }, (err, result) => {\n            if (err)\n                return callback(err);\n            if (result.ok)\n                return callback(undefined, true);\n            callback(undefined, false);\n        });\n    }\n}\nexports.DropDatabaseOperation = DropDatabaseOperation;\n(0, operation_1.defineAspects)(DropCollectionOperation, [operation_1.Aspect.WRITE_OPERATION]);\n(0, operation_1.defineAspects)(DropDatabaseOperation, [operation_1.Aspect.WRITE_OPERATION]);\n//# sourceMappingURL=drop.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.EstimatedDocumentCountOperation = void 0;\nconst operation_1 = require(\"./operation\");\nconst command_1 = require(\"./command\");\nconst utils_1 = require(\"../utils\");\n/** @internal */\nclass EstimatedDocumentCountOperation extends command_1.CommandOperation {\n    constructor(collection, options = {}) {\n        super(collection, options);\n        this.options = options;\n        this.collectionName = collection.collectionName;\n    }\n    execute(server, session, callback) {\n        if ((0, utils_1.maxWireVersion)(server) < 12) {\n            return this.executeLegacy(server, session, callback);\n        }\n        const pipeline = [{ $collStats: { count: {} } }, { $group: { _id: 1, n: { $sum: '$count' } } }];\n        const cmd = { aggregate: this.collectionName, pipeline, cursor: {} };\n        if (typeof this.options.maxTimeMS === 'number') {\n            cmd.maxTimeMS = this.options.maxTimeMS;\n        }\n        super.executeCommand(server, session, cmd, (err, response) => {\n            var _a, _b;\n            if (err && err.code !== 26) {\n                callback(err);\n                return;\n            }\n            callback(undefined, ((_b = (_a = response === null || response === void 0 ? void 0 : response.cursor) === null || _a === void 0 ? void 0 : _a.firstBatch[0]) === null || _b === void 0 ? void 0 : _b.n) || 0);\n        });\n    }\n    executeLegacy(server, session, callback) {\n        const cmd = { count: this.collectionName };\n        if (typeof this.options.maxTimeMS === 'number') {\n            cmd.maxTimeMS = this.options.maxTimeMS;\n        }\n        super.executeCommand(server, session, cmd, (err, response) => {\n            if (err) {\n                callback(err);\n                return;\n            }\n            callback(undefined, response.n || 0);\n        });\n    }\n}\nexports.EstimatedDocumentCountOperation = EstimatedDocumentCountOperation;\n(0, operation_1.defineAspects)(EstimatedDocumentCountOperation, [\n    operation_1.Aspect.READ_OPERATION,\n    operation_1.Aspect.RETRYABLE,\n    operation_1.Aspect.CURSOR_CREATING\n]);\n//# sourceMappingURL=estimated_document_count.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.FindOneAndUpdateOperation = exports.FindOneAndReplaceOperation = exports.FindOneAndDeleteOperation = exports.ReturnDocument = void 0;\nconst read_preference_1 = require(\"../read_preference\");\nconst utils_1 = require(\"../utils\");\nconst error_1 = require(\"../error\");\nconst command_1 = require(\"./command\");\nconst operation_1 = require(\"./operation\");\nconst sort_1 = require(\"../sort\");\n/** @public */\nexports.ReturnDocument = Object.freeze({\n    BEFORE: 'before',\n    AFTER: 'after'\n});\nfunction configureFindAndModifyCmdBaseUpdateOpts(cmdBase, options) {\n    cmdBase.new = options.returnDocument === exports.ReturnDocument.AFTER;\n    cmdBase.upsert = options.upsert === true;\n    if (options.bypassDocumentValidation === true) {\n        cmdBase.bypassDocumentValidation = options.bypassDocumentValidation;\n    }\n    return cmdBase;\n}\n/** @internal */\nclass FindAndModifyOperation extends command_1.CommandOperation {\n    constructor(collection, query, options) {\n        super(collection, options);\n        this.options = options !== null && options !== void 0 ? options : {};\n        this.cmdBase = {\n            remove: false,\n            new: false,\n            upsert: false\n        };\n        const sort = (0, sort_1.formatSort)(options.sort);\n        if (sort) {\n            this.cmdBase.sort = sort;\n        }\n        if (options.projection) {\n            this.cmdBase.fields = options.projection;\n        }\n        if (options.maxTimeMS) {\n            this.cmdBase.maxTimeMS = options.maxTimeMS;\n        }\n        // Decorate the findAndModify command with the write Concern\n        if (options.writeConcern) {\n            this.cmdBase.writeConcern = options.writeConcern;\n        }\n        if (options.let) {\n            this.cmdBase.let = options.let;\n        }\n        // force primary read preference\n        this.readPreference = read_preference_1.ReadPreference.primary;\n        this.collection = collection;\n        this.query = query;\n    }\n    execute(server, session, callback) {\n        var _a;\n        const coll = this.collection;\n        const query = this.query;\n        const options = { ...this.options, ...this.bsonOptions };\n        // Create findAndModify command object\n        const cmd = {\n            findAndModify: coll.collectionName,\n            query: query,\n            ...this.cmdBase\n        };\n        // Have we specified collation\n        try {\n            (0, utils_1.decorateWithCollation)(cmd, coll, options);\n        }\n        catch (err) {\n            return callback(err);\n        }\n        if (options.hint) {\n            // TODO: once this method becomes a CommandOperation we will have the server\n            // in place to check.\n            const unacknowledgedWrite = ((_a = this.writeConcern) === null || _a === void 0 ? void 0 : _a.w) === 0;\n            if (unacknowledgedWrite || (0, utils_1.maxWireVersion)(server) < 8) {\n                callback(new error_1.MongoCompatibilityError('The current topology does not support a hint on findAndModify commands'));\n                return;\n            }\n            cmd.hint = options.hint;\n        }\n        if (this.explain && (0, utils_1.maxWireVersion)(server) < 4) {\n            callback(new error_1.MongoCompatibilityError(`Server ${server.name} does not support explain on findAndModify`));\n            return;\n        }\n        // Execute the command\n        super.executeCommand(server, session, cmd, (err, result) => {\n            if (err)\n                return callback(err);\n            return callback(undefined, result);\n        });\n    }\n}\n/** @internal */\nclass FindOneAndDeleteOperation extends FindAndModifyOperation {\n    constructor(collection, filter, options) {\n        // Basic validation\n        if (filter == null || typeof filter !== 'object') {\n            throw new error_1.MongoInvalidArgumentError('Argument \"filter\" must be an object');\n        }\n        super(collection, filter, options);\n        this.cmdBase.remove = true;\n    }\n}\nexports.FindOneAndDeleteOperation = FindOneAndDeleteOperation;\n/** @internal */\nclass FindOneAndReplaceOperation extends FindAndModifyOperation {\n    constructor(collection, filter, replacement, options) {\n        if (filter == null || typeof filter !== 'object') {\n            throw new error_1.MongoInvalidArgumentError('Argument \"filter\" must be an object');\n        }\n        if (replacement == null || typeof replacement !== 'object') {\n            throw new error_1.MongoInvalidArgumentError('Argument \"replacement\" must be an object');\n        }\n        if ((0, utils_1.hasAtomicOperators)(replacement)) {\n            throw new error_1.MongoInvalidArgumentError('Replacement document must not contain atomic operators');\n        }\n        super(collection, filter, options);\n        this.cmdBase.update = replacement;\n        configureFindAndModifyCmdBaseUpdateOpts(this.cmdBase, options);\n    }\n}\nexports.FindOneAndReplaceOperation = FindOneAndReplaceOperation;\n/** @internal */\nclass FindOneAndUpdateOperation extends FindAndModifyOperation {\n    constructor(collection, filter, update, options) {\n        if (filter == null || typeof filter !== 'object') {\n            throw new error_1.MongoInvalidArgumentError('Argument \"filter\" must be an object');\n        }\n        if (update == null || typeof update !== 'object') {\n            throw new error_1.MongoInvalidArgumentError('Argument \"update\" must be an object');\n        }\n        if (!(0, utils_1.hasAtomicOperators)(update)) {\n            throw new error_1.MongoInvalidArgumentError('Update document requires atomic operators');\n        }\n        super(collection, filter, options);\n        this.cmdBase.update = update;\n        configureFindAndModifyCmdBaseUpdateOpts(this.cmdBase, options);\n        if (options.arrayFilters) {\n            this.cmdBase.arrayFilters = options.arrayFilters;\n        }\n    }\n}\nexports.FindOneAndUpdateOperation = FindOneAndUpdateOperation;\n(0, operation_1.defineAspects)(FindAndModifyOperation, [\n    operation_1.Aspect.WRITE_OPERATION,\n    operation_1.Aspect.RETRYABLE,\n    operation_1.Aspect.EXPLAINABLE\n]);\n//# sourceMappingURL=find_and_modify.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.IsCappedOperation = void 0;\nconst operation_1 = require(\"./operation\");\nconst error_1 = require(\"../error\");\n/** @internal */\nclass IsCappedOperation extends operation_1.AbstractOperation {\n    constructor(collection, options) {\n        super(options);\n        this.options = options;\n        this.collection = collection;\n    }\n    execute(server, session, callback) {\n        const coll = this.collection;\n        coll.s.db\n            .listCollections({ name: coll.collectionName }, { ...this.options, nameOnly: false, readPreference: this.readPreference, session })\n            .toArray((err, collections) => {\n            if (err || !collections)\n                return callback(err);\n            if (collections.length === 0) {\n                // TODO(NODE-3485)\n                return callback(new error_1.MongoAPIError(`collection ${coll.namespace} not found`));\n            }\n            const collOptions = collections[0].options;\n            callback(undefined, !!(collOptions && collOptions.capped));\n        });\n    }\n}\nexports.IsCappedOperation = IsCappedOperation;\n//# sourceMappingURL=is_capped.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.MapReduceOperation = void 0;\nconst bson_1 = require(\"../bson\");\nconst utils_1 = require(\"../utils\");\nconst read_preference_1 = require(\"../read_preference\");\nconst command_1 = require(\"./command\");\nconst error_1 = require(\"../error\");\nconst operation_1 = require(\"./operation\");\nconst db_1 = require(\"../db\");\nconst exclusionList = [\n    'explain',\n    'readPreference',\n    'readConcern',\n    'session',\n    'bypassDocumentValidation',\n    'writeConcern',\n    'raw',\n    'fieldsAsRaw',\n    'promoteLongs',\n    'promoteValues',\n    'promoteBuffers',\n    'bsonRegExp',\n    'serializeFunctions',\n    'ignoreUndefined',\n    'scope' // this option is reformatted thus exclude the original\n];\n/**\n * Run Map Reduce across a collection. Be aware that the inline option for out will return an array of results not a collection.\n * @internal\n */\nclass MapReduceOperation extends command_1.CommandOperation {\n    /**\n     * Constructs a MapReduce operation.\n     *\n     * @param collection - Collection instance.\n     * @param map - The mapping function.\n     * @param reduce - The reduce function.\n     * @param options - Optional settings. See Collection.prototype.mapReduce for a list of options.\n     */\n    constructor(collection, map, reduce, options) {\n        super(collection, options);\n        this.options = options !== null && options !== void 0 ? options : {};\n        this.collection = collection;\n        this.map = map;\n        this.reduce = reduce;\n    }\n    execute(server, session, callback) {\n        const coll = this.collection;\n        const map = this.map;\n        const reduce = this.reduce;\n        let options = this.options;\n        const mapCommandHash = {\n            mapReduce: coll.collectionName,\n            map: map,\n            reduce: reduce\n        };\n        if (options.scope) {\n            mapCommandHash.scope = processScope(options.scope);\n        }\n        // Add any other options passed in\n        for (const n in options) {\n            // Only include if not in exclusion list\n            if (exclusionList.indexOf(n) === -1) {\n                mapCommandHash[n] = options[n];\n            }\n        }\n        options = Object.assign({}, options);\n        // If we have a read preference and inline is not set as output fail hard\n        if (this.readPreference.mode === read_preference_1.ReadPreferenceMode.primary &&\n            options.out &&\n            options.out.inline !== 1 &&\n            options.out !== 'inline') {\n            // Force readPreference to primary\n            options.readPreference = read_preference_1.ReadPreference.primary;\n            // Decorate command with writeConcern if supported\n            (0, utils_1.applyWriteConcern)(mapCommandHash, { db: coll.s.db, collection: coll }, options);\n        }\n        else {\n            (0, utils_1.decorateWithReadConcern)(mapCommandHash, coll, options);\n        }\n        // Is bypassDocumentValidation specified\n        if (options.bypassDocumentValidation === true) {\n            mapCommandHash.bypassDocumentValidation = options.bypassDocumentValidation;\n        }\n        // Have we specified collation\n        try {\n            (0, utils_1.decorateWithCollation)(mapCommandHash, coll, options);\n        }\n        catch (err) {\n            return callback(err);\n        }\n        if (this.explain && (0, utils_1.maxWireVersion)(server) < 9) {\n            callback(new error_1.MongoCompatibilityError(`Server ${server.name} does not support explain on mapReduce`));\n            return;\n        }\n        // Execute command\n        super.executeCommand(server, session, mapCommandHash, (err, result) => {\n            if (err)\n                return callback(err);\n            // Check if we have an error\n            if (1 !== result.ok || result.err || result.errmsg) {\n                return callback(new error_1.MongoServerError(result));\n            }\n            // If an explain option was executed, don't process the server results\n            if (this.explain)\n                return callback(undefined, result);\n            // Create statistics value\n            const stats = {};\n            if (result.timeMillis)\n                stats['processtime'] = result.timeMillis;\n            if (result.counts)\n                stats['counts'] = result.counts;\n            if (result.timing)\n                stats['timing'] = result.timing;\n            // invoked with inline?\n            if (result.results) {\n                // If we wish for no verbosity\n                if (options['verbose'] == null || !options['verbose']) {\n                    return callback(undefined, result.results);\n                }\n                return callback(undefined, { results: result.results, stats: stats });\n            }\n            // The returned collection\n            let collection = null;\n            // If we have an object it's a different db\n            if (result.result != null && typeof result.result === 'object') {\n                const doc = result.result;\n                // Return a collection from another db\n                collection = new db_1.Db(coll.s.db.s.client, doc.db, coll.s.db.s.options).collection(doc.collection);\n            }\n            else {\n                // Create a collection object that wraps the result collection\n                collection = coll.s.db.collection(result.result);\n            }\n            // If we wish for no verbosity\n            if (options['verbose'] == null || !options['verbose']) {\n                return callback(err, collection);\n            }\n            // Return stats as third set of values\n            callback(err, { collection, stats });\n        });\n    }\n}\nexports.MapReduceOperation = MapReduceOperation;\n/** Functions that are passed as scope args must be converted to Code instances. */\nfunction processScope(scope) {\n    if (!(0, utils_1.isObject)(scope) || scope._bsontype === 'ObjectID') {\n        return scope;\n    }\n    const newScope = {};\n    for (const key of Object.keys(scope)) {\n        if ('function' === typeof scope[key]) {\n            newScope[key] = new bson_1.Code(String(scope[key]));\n        }\n        else if (scope[key]._bsontype === 'Code') {\n            newScope[key] = scope[key];\n        }\n        else {\n            newScope[key] = processScope(scope[key]);\n        }\n    }\n    return newScope;\n}\n(0, operation_1.defineAspects)(MapReduceOperation, [operation_1.Aspect.EXPLAINABLE]);\n//# sourceMappingURL=map_reduce.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.OptionsOperation = void 0;\nconst operation_1 = require(\"./operation\");\nconst error_1 = require(\"../error\");\n/** @internal */\nclass OptionsOperation extends operation_1.AbstractOperation {\n    constructor(collection, options) {\n        super(options);\n        this.options = options;\n        this.collection = collection;\n    }\n    execute(server, session, callback) {\n        const coll = this.collection;\n        coll.s.db\n            .listCollections({ name: coll.collectionName }, { ...this.options, nameOnly: false, readPreference: this.readPreference, session })\n            .toArray((err, collections) => {\n            if (err || !collections)\n                return callback(err);\n            if (collections.length === 0) {\n                // TODO(NODE-3485)\n                return callback(new error_1.MongoAPIError(`collection ${coll.namespace} not found`));\n            }\n            callback(err, collections[0].options);\n        });\n    }\n}\nexports.OptionsOperation = OptionsOperation;\n//# sourceMappingURL=options_operation.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.RenameOperation = void 0;\nconst utils_1 = require(\"../utils\");\nconst run_command_1 = require(\"./run_command\");\nconst operation_1 = require(\"./operation\");\nconst collection_1 = require(\"../collection\");\nconst error_1 = require(\"../error\");\n/** @internal */\nclass RenameOperation extends run_command_1.RunAdminCommandOperation {\n    constructor(collection, newName, options) {\n        // Check the collection name\n        (0, utils_1.checkCollectionName)(newName);\n        // Build the command\n        const renameCollection = collection.namespace;\n        const toCollection = collection.s.namespace.withCollection(newName).toString();\n        const dropTarget = typeof options.dropTarget === 'boolean' ? options.dropTarget : false;\n        const cmd = { renameCollection: renameCollection, to: toCollection, dropTarget: dropTarget };\n        super(collection, cmd, options);\n        this.options = options;\n        this.collection = collection;\n        this.newName = newName;\n    }\n    execute(server, session, callback) {\n        const coll = this.collection;\n        super.execute(server, session, (err, doc) => {\n            if (err)\n                return callback(err);\n            // We have an error\n            if (doc.errmsg) {\n                return callback(new error_1.MongoServerError(doc));\n            }\n            let newColl;\n            try {\n                newColl = new collection_1.Collection(coll.s.db, this.newName, coll.s.options);\n            }\n            catch (err) {\n                return callback(err);\n            }\n            return callback(undefined, newColl);\n        });\n    }\n}\nexports.RenameOperation = RenameOperation;\n(0, operation_1.defineAspects)(RenameOperation, [operation_1.Aspect.WRITE_OPERATION]);\n//# sourceMappingURL=rename.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.DbStatsOperation = exports.CollStatsOperation = void 0;\nconst operation_1 = require(\"./operation\");\nconst command_1 = require(\"./command\");\n/**\n * Get all the collection statistics.\n * @internal\n */\nclass CollStatsOperation extends command_1.CommandOperation {\n    /**\n     * Construct a Stats operation.\n     *\n     * @param collection - Collection instance\n     * @param options - Optional settings. See Collection.prototype.stats for a list of options.\n     */\n    constructor(collection, options) {\n        super(collection, options);\n        this.options = options !== null && options !== void 0 ? options : {};\n        this.collectionName = collection.collectionName;\n    }\n    execute(server, session, callback) {\n        const command = { collStats: this.collectionName };\n        if (this.options.scale != null) {\n            command.scale = this.options.scale;\n        }\n        super.executeCommand(server, session, command, callback);\n    }\n}\nexports.CollStatsOperation = CollStatsOperation;\n/** @internal */\nclass DbStatsOperation extends command_1.CommandOperation {\n    constructor(db, options) {\n        super(db, options);\n        this.options = options;\n    }\n    execute(server, session, callback) {\n        const command = { dbStats: true };\n        if (this.options.scale != null) {\n            command.scale = this.options.scale;\n        }\n        super.executeCommand(server, session, command, callback);\n    }\n}\nexports.DbStatsOperation = DbStatsOperation;\n(0, operation_1.defineAspects)(CollStatsOperation, [operation_1.Aspect.READ_OPERATION]);\n(0, operation_1.defineAspects)(DbStatsOperation, [operation_1.Aspect.READ_OPERATION]);\n//# sourceMappingURL=stats.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.Logger = exports.LoggerLevel = void 0;\nconst util_1 = require(\"util\");\nconst utils_1 = require(\"./utils\");\nconst error_1 = require(\"./error\");\n// Filters for classes\nconst classFilters = {};\nlet filteredClasses = {};\nlet level;\n// Save the process id\nconst pid = process.pid;\n// current logger\n// eslint-disable-next-line no-console\nlet currentLogger = console.warn;\n/** @public */\nexports.LoggerLevel = Object.freeze({\n    ERROR: 'error',\n    WARN: 'warn',\n    INFO: 'info',\n    DEBUG: 'debug',\n    error: 'error',\n    warn: 'warn',\n    info: 'info',\n    debug: 'debug'\n});\n/**\n * @public\n */\nclass Logger {\n    /**\n     * Creates a new Logger instance\n     *\n     * @param className - The Class name associated with the logging instance\n     * @param options - Optional logging settings\n     */\n    constructor(className, options) {\n        options = options !== null && options !== void 0 ? options : {};\n        // Current reference\n        this.className = className;\n        // Current logger\n        if (!(options.logger instanceof Logger) && typeof options.logger === 'function') {\n            currentLogger = options.logger;\n        }\n        // Set level of logging, default is error\n        if (options.loggerLevel) {\n            level = options.loggerLevel || exports.LoggerLevel.ERROR;\n        }\n        // Add all class names\n        if (filteredClasses[this.className] == null) {\n            classFilters[this.className] = true;\n        }\n    }\n    /**\n     * Log a message at the debug level\n     *\n     * @param message - The message to log\n     * @param object - Additional meta data to log\n     */\n    debug(message, object) {\n        if (this.isDebug() &&\n            ((Object.keys(filteredClasses).length > 0 && filteredClasses[this.className]) ||\n                (Object.keys(filteredClasses).length === 0 && classFilters[this.className]))) {\n            const dateTime = new Date().getTime();\n            const msg = (0, util_1.format)('[%s-%s:%s] %s %s', 'DEBUG', this.className, pid, dateTime, message);\n            const state = {\n                type: exports.LoggerLevel.DEBUG,\n                message,\n                className: this.className,\n                pid,\n                date: dateTime\n            };\n            if (object)\n                state.meta = object;\n            currentLogger(msg, state);\n        }\n    }\n    /**\n     * Log a message at the warn level\n     *\n     * @param message - The message to log\n     * @param object - Additional meta data to log\n     */\n    warn(message, object) {\n        if (this.isWarn() &&\n            ((Object.keys(filteredClasses).length > 0 && filteredClasses[this.className]) ||\n                (Object.keys(filteredClasses).length === 0 && classFilters[this.className]))) {\n            const dateTime = new Date().getTime();\n            const msg = (0, util_1.format)('[%s-%s:%s] %s %s', 'WARN', this.className, pid, dateTime, message);\n            const state = {\n                type: exports.LoggerLevel.WARN,\n                message,\n                className: this.className,\n                pid,\n                date: dateTime\n            };\n            if (object)\n                state.meta = object;\n            currentLogger(msg, state);\n        }\n    }\n    /**\n     * Log a message at the info level\n     *\n     * @param message - The message to log\n     * @param object - Additional meta data to log\n     */\n    info(message, object) {\n        if (this.isInfo() &&\n            ((Object.keys(filteredClasses).length > 0 && filteredClasses[this.className]) ||\n                (Object.keys(filteredClasses).length === 0 && classFilters[this.className]))) {\n            const dateTime = new Date().getTime();\n            const msg = (0, util_1.format)('[%s-%s:%s] %s %s', 'INFO', this.className, pid, dateTime, message);\n            const state = {\n                type: exports.LoggerLevel.INFO,\n                message,\n                className: this.className,\n                pid,\n                date: dateTime\n            };\n            if (object)\n                state.meta = object;\n            currentLogger(msg, state);\n        }\n    }\n    /**\n     * Log a message at the error level\n     *\n     * @param message - The message to log\n     * @param object - Additional meta data to log\n     */\n    error(message, object) {\n        if (this.isError() &&\n            ((Object.keys(filteredClasses).length > 0 && filteredClasses[this.className]) ||\n                (Object.keys(filteredClasses).length === 0 && classFilters[this.className]))) {\n            const dateTime = new Date().getTime();\n            const msg = (0, util_1.format)('[%s-%s:%s] %s %s', 'ERROR', this.className, pid, dateTime, message);\n            const state = {\n                type: exports.LoggerLevel.ERROR,\n                message,\n                className: this.className,\n                pid,\n                date: dateTime\n            };\n            if (object)\n                state.meta = object;\n            currentLogger(msg, state);\n        }\n    }\n    /** Is the logger set at info level */\n    isInfo() {\n        return level === exports.LoggerLevel.INFO || level === exports.LoggerLevel.DEBUG;\n    }\n    /** Is the logger set at error level */\n    isError() {\n        return level === exports.LoggerLevel.ERROR || level === exports.LoggerLevel.INFO || level === exports.LoggerLevel.DEBUG;\n    }\n    /** Is the logger set at error level */\n    isWarn() {\n        return (level === exports.LoggerLevel.ERROR ||\n            level === exports.LoggerLevel.WARN ||\n            level === exports.LoggerLevel.INFO ||\n            level === exports.LoggerLevel.DEBUG);\n    }\n    /** Is the logger set at debug level */\n    isDebug() {\n        return level === exports.LoggerLevel.DEBUG;\n    }\n    /** Resets the logger to default settings, error and no filtered classes */\n    static reset() {\n        level = exports.LoggerLevel.ERROR;\n        filteredClasses = {};\n    }\n    /** Get the current logger function */\n    static currentLogger() {\n        return currentLogger;\n    }\n    /**\n     * Set the current logger function\n     *\n     * @param logger - Custom logging function\n     */\n    static setCurrentLogger(logger) {\n        if (typeof logger !== 'function') {\n            throw new error_1.MongoInvalidArgumentError('Current logger must be a function');\n        }\n        currentLogger = logger;\n    }\n    /**\n     * Filter log messages for a particular class\n     *\n     * @param type - The type of filter (currently only class)\n     * @param values - The filters to apply\n     */\n    static filter(type, values) {\n        if (type === 'class' && Array.isArray(values)) {\n            filteredClasses = {};\n            values.forEach(x => (filteredClasses[x] = true));\n        }\n    }\n    /**\n     * Set the current log level\n     *\n     * @param newLevel - Set current log level (debug, warn, info, error)\n     */\n    static setLevel(newLevel) {\n        if (newLevel !== exports.LoggerLevel.INFO &&\n            newLevel !== exports.LoggerLevel.ERROR &&\n            newLevel !== exports.LoggerLevel.DEBUG &&\n            newLevel !== exports.LoggerLevel.WARN) {\n            throw new error_1.MongoInvalidArgumentError(`Argument \"newLevel\" should be one of ${(0, utils_1.enumToString)(exports.LoggerLevel)}`);\n        }\n        level = newLevel;\n    }\n}\nexports.Logger = Logger;\n//# sourceMappingURL=logger.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.CollectionsOperation = void 0;\nconst operation_1 = require(\"./operation\");\nconst collection_1 = require(\"../collection\");\n/** @internal */\nclass CollectionsOperation extends operation_1.AbstractOperation {\n    constructor(db, options) {\n        super(options);\n        this.options = options;\n        this.db = db;\n    }\n    execute(server, session, callback) {\n        const db = this.db;\n        // Let's get the collection names\n        db.listCollections({}, { ...this.options, nameOnly: true, readPreference: this.readPreference, session }).toArray((err, documents) => {\n            if (err || !documents)\n                return callback(err);\n            // Filter collections removing any illegal ones\n            documents = documents.filter(doc => doc.name.indexOf('$') === -1);\n            // Return the collection objects\n            callback(undefined, documents.map(d => {\n                return new collection_1.Collection(db, d.name, db.s.options);\n            }));\n        });\n    }\n}\nexports.CollectionsOperation = CollectionsOperation;\n//# sourceMappingURL=collections.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.CreateCollectionOperation = void 0;\nconst command_1 = require(\"./command\");\nconst operation_1 = require(\"./operation\");\nconst collection_1 = require(\"../collection\");\nconst ILLEGAL_COMMAND_FIELDS = new Set([\n    'w',\n    'wtimeout',\n    'j',\n    'fsync',\n    'autoIndexId',\n    'pkFactory',\n    'raw',\n    'readPreference',\n    'session',\n    'readConcern',\n    'writeConcern',\n    'raw',\n    'fieldsAsRaw',\n    'promoteLongs',\n    'promoteValues',\n    'promoteBuffers',\n    'bsonRegExp',\n    'serializeFunctions',\n    'ignoreUndefined'\n]);\n/** @internal */\nclass CreateCollectionOperation extends command_1.CommandOperation {\n    constructor(db, name, options = {}) {\n        super(db, options);\n        this.options = options;\n        this.db = db;\n        this.name = name;\n    }\n    execute(server, session, callback) {\n        const db = this.db;\n        const name = this.name;\n        const options = this.options;\n        const done = err => {\n            if (err) {\n                return callback(err);\n            }\n            callback(undefined, new collection_1.Collection(db, name, options));\n        };\n        const cmd = { create: name };\n        for (const n in options) {\n            if (options[n] != null &&\n                typeof options[n] !== 'function' &&\n                !ILLEGAL_COMMAND_FIELDS.has(n)) {\n                cmd[n] = options[n];\n            }\n        }\n        // otherwise just execute the command\n        super.executeCommand(server, session, cmd, done);\n    }\n}\nexports.CreateCollectionOperation = CreateCollectionOperation;\n(0, operation_1.defineAspects)(CreateCollectionOperation, [operation_1.Aspect.WRITE_OPERATION]);\n//# sourceMappingURL=create_collection.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.ProfilingLevelOperation = void 0;\nconst command_1 = require(\"./command\");\nconst error_1 = require(\"../error\");\n/** @internal */\nclass ProfilingLevelOperation extends command_1.CommandOperation {\n    constructor(db, options) {\n        super(db, options);\n        this.options = options;\n    }\n    execute(server, session, callback) {\n        super.executeCommand(server, session, { profile: -1 }, (err, doc) => {\n            if (err == null && doc.ok === 1) {\n                const was = doc.was;\n                if (was === 0)\n                    return callback(undefined, 'off');\n                if (was === 1)\n                    return callback(undefined, 'slow_only');\n                if (was === 2)\n                    return callback(undefined, 'all');\n                // TODO(NODE-3483)\n                return callback(new error_1.MongoRuntimeError(`Illegal profiling level value ${was}`));\n            }\n            else {\n                // TODO(NODE-3483): Consider MongoUnexpectedServerResponseError\n                err != null ? callback(err) : callback(new error_1.MongoRuntimeError('Error with profile command'));\n            }\n        });\n    }\n}\nexports.ProfilingLevelOperation = ProfilingLevelOperation;\n//# sourceMappingURL=profiling_level.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.SetProfilingLevelOperation = exports.ProfilingLevel = void 0;\nconst command_1 = require(\"./command\");\nconst utils_1 = require(\"../utils\");\nconst error_1 = require(\"../error\");\nconst levelValues = new Set(['off', 'slow_only', 'all']);\n/** @public */\nexports.ProfilingLevel = Object.freeze({\n    off: 'off',\n    slowOnly: 'slow_only',\n    all: 'all'\n});\n/** @internal */\nclass SetProfilingLevelOperation extends command_1.CommandOperation {\n    constructor(db, level, options) {\n        super(db, options);\n        this.options = options;\n        switch (level) {\n            case exports.ProfilingLevel.off:\n                this.profile = 0;\n                break;\n            case exports.ProfilingLevel.slowOnly:\n                this.profile = 1;\n                break;\n            case exports.ProfilingLevel.all:\n                this.profile = 2;\n                break;\n            default:\n                this.profile = 0;\n                break;\n        }\n        this.level = level;\n    }\n    execute(server, session, callback) {\n        const level = this.level;\n        if (!levelValues.has(level)) {\n            return callback(new error_1.MongoInvalidArgumentError(`Profiling level must be one of \"${(0, utils_1.enumToString)(exports.ProfilingLevel)}\"`));\n        }\n        // TODO(NODE-3483): Determine error to put here\n        super.executeCommand(server, session, { profile: this.profile }, (err, doc) => {\n            if (err == null && doc.ok === 1)\n                return callback(undefined, level);\n            return err != null\n                ? callback(err)\n                : callback(new error_1.MongoRuntimeError('Error with profile command'));\n        });\n    }\n}\nexports.SetProfilingLevelOperation = SetProfilingLevelOperation;\n//# sourceMappingURL=set_profiling_level.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.connect = exports.MONGO_CLIENT_EVENTS = void 0;\nconst error_1 = require(\"../error\");\nconst topology_1 = require(\"../sdam/topology\");\nconst connection_string_1 = require(\"../connection_string\");\nconst connection_pool_1 = require(\"../cmap/connection_pool\");\nconst connection_1 = require(\"../cmap/connection\");\nconst server_1 = require(\"../sdam/server\");\n/** @public */\nexports.MONGO_CLIENT_EVENTS = [\n    ...connection_pool_1.CMAP_EVENTS,\n    ...connection_1.APM_EVENTS,\n    ...topology_1.TOPOLOGY_EVENTS,\n    ...server_1.HEARTBEAT_EVENTS\n];\nfunction connect(mongoClient, options, callback) {\n    if (!callback) {\n        throw new error_1.MongoInvalidArgumentError('Callback function must be provided');\n    }\n    // If a connection already been established, we can terminate early\n    if (mongoClient.topology && mongoClient.topology.isConnected()) {\n        return callback(undefined, mongoClient);\n    }\n    const logger = mongoClient.logger;\n    const connectCallback = err => {\n        const warningMessage = 'seed list contains no mongos proxies, replicaset connections requires ' +\n            'the parameter replicaSet to be supplied in the URI or options object, ' +\n            'mongodb://server:port/db?replicaSet=name';\n        if (err && err.message === 'no mongos proxies found in seed list') {\n            if (logger.isWarn()) {\n                logger.warn(warningMessage);\n            }\n            // Return a more specific error message for MongoClient.connect\n            // TODO(NODE-3483)\n            return callback(new error_1.MongoRuntimeError(warningMessage));\n        }\n        callback(err, mongoClient);\n    };\n    if (typeof options.srvHost === 'string') {\n        return (0, connection_string_1.resolveSRVRecord)(options, (err, hosts) => {\n            if (err || !hosts)\n                return callback(err);\n            for (const [index, host] of hosts.entries()) {\n                options.hosts[index] = host;\n            }\n            return createTopology(mongoClient, options, connectCallback);\n        });\n    }\n    return createTopology(mongoClient, options, connectCallback);\n}\nexports.connect = connect;\nfunction createTopology(mongoClient, options, callback) {\n    // Create the topology\n    const topology = new topology_1.Topology(options.hosts, options);\n    // Events can be emitted before initialization is complete so we have to\n    // save the reference to the topology on the client ASAP if the event handlers need to access it\n    mongoClient.topology = topology;\n    topology.once(topology_1.Topology.OPEN, () => mongoClient.emit('open', mongoClient));\n    for (const event of exports.MONGO_CLIENT_EVENTS) {\n        topology.on(event, (...args) => mongoClient.emit(event, ...args));\n    }\n    // initialize CSFLE if requested\n    if (mongoClient.autoEncrypter) {\n        mongoClient.autoEncrypter.init(err => {\n            if (err) {\n                return callback(err);\n            }\n            topology.connect(options, err => {\n                if (err) {\n                    topology.close({ force: true });\n                    return callback(err);\n                }\n                options.encrypter.connectInternalClient(error => {\n                    if (error)\n                        return callback(error);\n                    callback(undefined, topology);\n                });\n            });\n        });\n        return;\n    }\n    // otherwise connect normally\n    topology.connect(options, err => {\n        if (err) {\n            topology.close({ force: true });\n            return callback(err);\n        }\n        callback(undefined, topology);\n        return;\n    });\n}\n//# sourceMappingURL=connect.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.ServerCapabilities = exports.TOPOLOGY_EVENTS = exports.Topology = void 0;\nconst Denque = require(\"denque\");\nconst read_preference_1 = require(\"../read_preference\");\nconst server_description_1 = require(\"./server_description\");\nconst topology_description_1 = require(\"./topology_description\");\nconst server_1 = require(\"./server\");\nconst sessions_1 = require(\"../sessions\");\nconst srv_polling_1 = require(\"./srv_polling\");\nconst connection_pool_1 = require(\"../cmap/connection_pool\");\nconst error_1 = require(\"../error\");\nconst server_selection_1 = require(\"./server_selection\");\nconst utils_1 = require(\"../utils\");\nconst common_1 = require(\"./common\");\nconst events_1 = require(\"./events\");\nconst connection_1 = require(\"../cmap/connection\");\nconst connection_string_1 = require(\"../connection_string\");\nconst bson_1 = require(\"../bson\");\nconst mongo_types_1 = require(\"../mongo_types\");\n// Global state\nlet globalTopologyCounter = 0;\n// events that we relay to the `Topology`\nconst SERVER_RELAY_EVENTS = [\n    server_1.Server.SERVER_HEARTBEAT_STARTED,\n    server_1.Server.SERVER_HEARTBEAT_SUCCEEDED,\n    server_1.Server.SERVER_HEARTBEAT_FAILED,\n    connection_1.Connection.COMMAND_STARTED,\n    connection_1.Connection.COMMAND_SUCCEEDED,\n    connection_1.Connection.COMMAND_FAILED,\n    ...connection_pool_1.CMAP_EVENTS\n];\n// all events we listen to from `Server` instances\nconst LOCAL_SERVER_EVENTS = [\n    server_1.Server.CONNECT,\n    server_1.Server.DESCRIPTION_RECEIVED,\n    server_1.Server.CLOSED,\n    server_1.Server.ENDED\n];\nconst stateTransition = (0, utils_1.makeStateMachine)({\n    [common_1.STATE_CLOSED]: [common_1.STATE_CLOSED, common_1.STATE_CONNECTING],\n    [common_1.STATE_CONNECTING]: [common_1.STATE_CONNECTING, common_1.STATE_CLOSING, common_1.STATE_CONNECTED, common_1.STATE_CLOSED],\n    [common_1.STATE_CONNECTED]: [common_1.STATE_CONNECTED, common_1.STATE_CLOSING, common_1.STATE_CLOSED],\n    [common_1.STATE_CLOSING]: [common_1.STATE_CLOSING, common_1.STATE_CLOSED]\n});\n/** @internal */\nconst kCancelled = Symbol('cancelled');\n/** @internal */\nconst kWaitQueue = Symbol('waitQueue');\n/**\n * A container of server instances representing a connection to a MongoDB topology.\n * @internal\n */\nclass Topology extends mongo_types_1.TypedEventEmitter {\n    /**\n     * @param seedlist - a list of HostAddress instances to connect to\n     */\n    constructor(seeds, options) {\n        var _a;\n        super();\n        // Legacy CSFLE support\n        this.bson = Object.create(null);\n        this.bson.serialize = bson_1.serialize;\n        this.bson.deserialize = bson_1.deserialize;\n        // Options should only be undefined in tests, MongoClient will always have defined options\n        options = options !== null && options !== void 0 ? options : {\n            hosts: [utils_1.HostAddress.fromString('localhost:27017')],\n            retryReads: connection_string_1.DEFAULT_OPTIONS.get('retryReads'),\n            retryWrites: connection_string_1.DEFAULT_OPTIONS.get('retryWrites'),\n            serverSelectionTimeoutMS: connection_string_1.DEFAULT_OPTIONS.get('serverSelectionTimeoutMS'),\n            directConnection: connection_string_1.DEFAULT_OPTIONS.get('directConnection'),\n            loadBalanced: connection_string_1.DEFAULT_OPTIONS.get('loadBalanced'),\n            metadata: connection_string_1.DEFAULT_OPTIONS.get('metadata'),\n            monitorCommands: connection_string_1.DEFAULT_OPTIONS.get('monitorCommands'),\n            tls: connection_string_1.DEFAULT_OPTIONS.get('tls'),\n            maxPoolSize: connection_string_1.DEFAULT_OPTIONS.get('maxPoolSize'),\n            minPoolSize: connection_string_1.DEFAULT_OPTIONS.get('minPoolSize'),\n            waitQueueTimeoutMS: connection_string_1.DEFAULT_OPTIONS.get('waitQueueTimeoutMS'),\n            connectionType: connection_string_1.DEFAULT_OPTIONS.get('connectionType'),\n            connectTimeoutMS: connection_string_1.DEFAULT_OPTIONS.get('connectTimeoutMS'),\n            maxIdleTimeMS: connection_string_1.DEFAULT_OPTIONS.get('maxIdleTimeMS'),\n            heartbeatFrequencyMS: connection_string_1.DEFAULT_OPTIONS.get('heartbeatFrequencyMS'),\n            minHeartbeatFrequencyMS: connection_string_1.DEFAULT_OPTIONS.get('minHeartbeatFrequencyMS')\n        };\n        if (typeof seeds === 'string') {\n            seeds = [utils_1.HostAddress.fromString(seeds)];\n        }\n        else if (!Array.isArray(seeds)) {\n            seeds = [seeds];\n        }\n        const seedlist = [];\n        for (const seed of seeds) {\n            if (typeof seed === 'string') {\n                seedlist.push(utils_1.HostAddress.fromString(seed));\n            }\n            else if (seed instanceof utils_1.HostAddress) {\n                seedlist.push(seed);\n            }\n            else {\n                // FIXME(NODE-3483): May need to be a MongoParseError\n                throw new error_1.MongoRuntimeError(`Topology cannot be constructed from ${JSON.stringify(seed)}`);\n            }\n        }\n        const topologyType = topologyTypeFromOptions(options);\n        const topologyId = globalTopologyCounter++;\n        const selectedHosts = options.srvMaxHosts == null ||\n            options.srvMaxHosts === 0 ||\n            options.srvMaxHosts >= seedlist.length\n            ? seedlist\n            : (0, utils_1.shuffle)(seedlist, options.srvMaxHosts);\n        const serverDescriptions = new Map();\n        for (const hostAddress of selectedHosts) {\n            serverDescriptions.set(hostAddress.toString(), new server_description_1.ServerDescription(hostAddress));\n        }\n        this[kWaitQueue] = new Denque();\n        this.s = {\n            // the id of this topology\n            id: topologyId,\n            // passed in options\n            options,\n            // initial seedlist of servers to connect to\n            seedlist,\n            // initial state\n            state: common_1.STATE_CLOSED,\n            // the topology description\n            description: new topology_description_1.TopologyDescription(topologyType, serverDescriptions, options.replicaSet, undefined, undefined, undefined, options),\n            serverSelectionTimeoutMS: options.serverSelectionTimeoutMS,\n            heartbeatFrequencyMS: options.heartbeatFrequencyMS,\n            minHeartbeatFrequencyMS: options.minHeartbeatFrequencyMS,\n            // a map of server instances to normalized addresses\n            servers: new Map(),\n            // Server Session Pool\n            sessionPool: new sessions_1.ServerSessionPool(this),\n            // Active client sessions\n            sessions: new Set(),\n            credentials: options === null || options === void 0 ? void 0 : options.credentials,\n            clusterTime: undefined,\n            // timer management\n            connectionTimers: new Set(),\n            detectShardedTopology: ev => this.detectShardedTopology(ev),\n            detectSrvRecords: ev => this.detectSrvRecords(ev)\n        };\n        if (options.srvHost && !options.loadBalanced) {\n            this.s.srvPoller =\n                (_a = options.srvPoller) !== null && _a !== void 0 ? _a : new srv_polling_1.SrvPoller({\n                    heartbeatFrequencyMS: this.s.heartbeatFrequencyMS,\n                    srvHost: options.srvHost,\n                    srvMaxHosts: options.srvMaxHosts,\n                    srvServiceName: options.srvServiceName\n                });\n            this.on(Topology.TOPOLOGY_DESCRIPTION_CHANGED, this.s.detectShardedTopology);\n        }\n    }\n    detectShardedTopology(event) {\n        var _a, _b, _c;\n        const previousType = event.previousDescription.type;\n        const newType = event.newDescription.type;\n        const transitionToSharded = previousType !== common_1.TopologyType.Sharded && newType === common_1.TopologyType.Sharded;\n        const srvListeners = (_a = this.s.srvPoller) === null || _a === void 0 ? void 0 : _a.listeners(srv_polling_1.SrvPoller.SRV_RECORD_DISCOVERY);\n        const listeningToSrvPolling = !!(srvListeners === null || srvListeners === void 0 ? void 0 : srvListeners.includes(this.s.detectSrvRecords));\n        if (transitionToSharded && !listeningToSrvPolling) {\n            (_b = this.s.srvPoller) === null || _b === void 0 ? void 0 : _b.on(srv_polling_1.SrvPoller.SRV_RECORD_DISCOVERY, this.s.detectSrvRecords);\n            (_c = this.s.srvPoller) === null || _c === void 0 ? void 0 : _c.start();\n        }\n    }\n    detectSrvRecords(ev) {\n        const previousTopologyDescription = this.s.description;\n        this.s.description = this.s.description.updateFromSrvPollingEvent(ev, this.s.options.srvMaxHosts);\n        if (this.s.description === previousTopologyDescription) {\n            // Nothing changed, so return\n            return;\n        }\n        updateServers(this);\n        this.emit(Topology.TOPOLOGY_DESCRIPTION_CHANGED, new events_1.TopologyDescriptionChangedEvent(this.s.id, previousTopologyDescription, this.s.description));\n    }\n    /**\n     * @returns A `TopologyDescription` for this topology\n     */\n    get description() {\n        return this.s.description;\n    }\n    get loadBalanced() {\n        return this.s.options.loadBalanced;\n    }\n    get capabilities() {\n        return new ServerCapabilities(this.lastIsMaster());\n    }\n    /** Initiate server connect */\n    connect(options, callback) {\n        var _a;\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = options !== null && options !== void 0 ? options : {};\n        if (this.s.state === common_1.STATE_CONNECTED) {\n            if (typeof callback === 'function') {\n                callback();\n            }\n            return;\n        }\n        stateTransition(this, common_1.STATE_CONNECTING);\n        // emit SDAM monitoring events\n        this.emit(Topology.TOPOLOGY_OPENING, new events_1.TopologyOpeningEvent(this.s.id));\n        // emit an event for the topology change\n        this.emit(Topology.TOPOLOGY_DESCRIPTION_CHANGED, new events_1.TopologyDescriptionChangedEvent(this.s.id, new topology_description_1.TopologyDescription(common_1.TopologyType.Unknown), // initial is always Unknown\n        this.s.description));\n        // connect all known servers, then attempt server selection to connect\n        const serverDescriptions = Array.from(this.s.description.servers.values());\n        connectServers(this, serverDescriptions);\n        // In load balancer mode we need to fake a server description getting\n        // emitted from the monitor, since the monitor doesn't exist.\n        if (this.s.options.loadBalanced) {\n            for (const description of serverDescriptions) {\n                const newDescription = new server_description_1.ServerDescription(description.hostAddress, undefined, {\n                    loadBalanced: this.s.options.loadBalanced\n                });\n                this.serverUpdateHandler(newDescription);\n            }\n        }\n        const readPreference = (_a = options.readPreference) !== null && _a !== void 0 ? _a : read_preference_1.ReadPreference.primary;\n        this.selectServer((0, server_selection_1.readPreferenceServerSelector)(readPreference), options, (err, server) => {\n            if (err) {\n                this.close();\n                typeof callback === 'function' ? callback(err) : this.emit(Topology.ERROR, err);\n                return;\n            }\n            // TODO: NODE-2471\n            if (server && this.s.credentials) {\n                server.command((0, utils_1.ns)('admin.$cmd'), { ping: 1 }, err => {\n                    if (err) {\n                        typeof callback === 'function' ? callback(err) : this.emit(Topology.ERROR, err);\n                        return;\n                    }\n                    stateTransition(this, common_1.STATE_CONNECTED);\n                    this.emit(Topology.OPEN, this);\n                    this.emit(Topology.CONNECT, this);\n                    if (typeof callback === 'function')\n                        callback(undefined, this);\n                });\n                return;\n            }\n            stateTransition(this, common_1.STATE_CONNECTED);\n            this.emit(Topology.OPEN, this);\n            this.emit(Topology.CONNECT, this);\n            if (typeof callback === 'function')\n                callback(undefined, this);\n        });\n    }\n    /** Close this topology */\n    close(options, callback) {\n        if (typeof options === 'function') {\n            callback = options;\n            options = {};\n        }\n        if (typeof options === 'boolean') {\n            options = { force: options };\n        }\n        options = options !== null && options !== void 0 ? options : {};\n        if (this.s.state === common_1.STATE_CLOSED || this.s.state === common_1.STATE_CLOSING) {\n            if (typeof callback === 'function') {\n                callback();\n            }\n            return;\n        }\n        stateTransition(this, common_1.STATE_CLOSING);\n        drainWaitQueue(this[kWaitQueue], new error_1.MongoTopologyClosedError());\n        (0, common_1.drainTimerQueue)(this.s.connectionTimers);\n        if (this.s.srvPoller) {\n            this.s.srvPoller.stop();\n            this.s.srvPoller.removeListener(srv_polling_1.SrvPoller.SRV_RECORD_DISCOVERY, this.s.detectSrvRecords);\n        }\n        this.removeListener(Topology.TOPOLOGY_DESCRIPTION_CHANGED, this.s.detectShardedTopology);\n        (0, utils_1.eachAsync)(Array.from(this.s.sessions.values()), (session, cb) => session.endSession(cb), () => {\n            this.s.sessionPool.endAllPooledSessions(() => {\n                (0, utils_1.eachAsync)(Array.from(this.s.servers.values()), (server, cb) => destroyServer(server, this, options, cb), err => {\n                    this.s.servers.clear();\n                    // emit an event for close\n                    this.emit(Topology.TOPOLOGY_CLOSED, new events_1.TopologyClosedEvent(this.s.id));\n                    stateTransition(this, common_1.STATE_CLOSED);\n                    if (typeof callback === 'function') {\n                        callback(err);\n                    }\n                });\n            });\n        });\n    }\n    selectServer(selector, _options, _callback) {\n        let options = _options;\n        const callback = (_callback !== null && _callback !== void 0 ? _callback : _options);\n        if (typeof options === 'function') {\n            options = {};\n        }\n        let serverSelector;\n        if (typeof selector !== 'function') {\n            if (typeof selector === 'string') {\n                serverSelector = (0, server_selection_1.readPreferenceServerSelector)(read_preference_1.ReadPreference.fromString(selector));\n            }\n            else {\n                let readPreference;\n                if (selector instanceof read_preference_1.ReadPreference) {\n                    readPreference = selector;\n                }\n                else {\n                    read_preference_1.ReadPreference.translate(options);\n                    readPreference = options.readPreference || read_preference_1.ReadPreference.primary;\n                }\n                serverSelector = (0, server_selection_1.readPreferenceServerSelector)(readPreference);\n            }\n        }\n        else {\n            serverSelector = selector;\n        }\n        options = Object.assign({}, { serverSelectionTimeoutMS: this.s.serverSelectionTimeoutMS }, options);\n        const isSharded = this.description.type === common_1.TopologyType.Sharded;\n        const session = options.session;\n        const transaction = session && session.transaction;\n        if (isSharded && transaction && transaction.server) {\n            callback(undefined, transaction.server);\n            return;\n        }\n        const waitQueueMember = {\n            serverSelector,\n            transaction,\n            callback\n        };\n        const serverSelectionTimeoutMS = options.serverSelectionTimeoutMS;\n        if (serverSelectionTimeoutMS) {\n            waitQueueMember.timer = setTimeout(() => {\n                waitQueueMember[kCancelled] = true;\n                waitQueueMember.timer = undefined;\n                const timeoutError = new error_1.MongoServerSelectionError(`Server selection timed out after ${serverSelectionTimeoutMS} ms`, this.description);\n                waitQueueMember.callback(timeoutError);\n            }, serverSelectionTimeoutMS);\n        }\n        this[kWaitQueue].push(waitQueueMember);\n        processWaitQueue(this);\n    }\n    // Sessions related methods\n    /**\n     * @returns Whether the topology should initiate selection to determine session support\n     */\n    shouldCheckForSessionSupport() {\n        if (this.description.type === common_1.TopologyType.Single) {\n            return !this.description.hasKnownServers;\n        }\n        return !this.description.hasDataBearingServers;\n    }\n    /**\n     * @returns Whether sessions are supported on the current topology\n     */\n    hasSessionSupport() {\n        return this.loadBalanced || this.description.logicalSessionTimeoutMinutes != null;\n    }\n    /** Start a logical session */\n    startSession(options, clientOptions) {\n        const session = new sessions_1.ClientSession(this, this.s.sessionPool, options, clientOptions);\n        session.once('ended', () => {\n            this.s.sessions.delete(session);\n        });\n        this.s.sessions.add(session);\n        return session;\n    }\n    /** Send endSessions command(s) with the given session ids */\n    endSessions(sessions, callback) {\n        if (!Array.isArray(sessions)) {\n            sessions = [sessions];\n        }\n        this.selectServer((0, server_selection_1.readPreferenceServerSelector)(read_preference_1.ReadPreference.primaryPreferred), (err, server) => {\n            if (err || !server) {\n                if (typeof callback === 'function')\n                    callback(err);\n                return;\n            }\n            server.command((0, utils_1.ns)('admin.$cmd'), { endSessions: sessions }, { noResponse: true }, (err, result) => {\n                if (typeof callback === 'function')\n                    callback(err, result);\n            });\n        });\n    }\n    /**\n     * Update the internal TopologyDescription with a ServerDescription\n     *\n     * @param serverDescription - The server to update in the internal list of server descriptions\n     */\n    serverUpdateHandler(serverDescription) {\n        if (!this.s.description.hasServer(serverDescription.address)) {\n            return;\n        }\n        // ignore this server update if its from an outdated topologyVersion\n        if (isStaleServerDescription(this.s.description, serverDescription)) {\n            return;\n        }\n        // these will be used for monitoring events later\n        const previousTopologyDescription = this.s.description;\n        const previousServerDescription = this.s.description.servers.get(serverDescription.address);\n        if (!previousServerDescription) {\n            return;\n        }\n        // Driver Sessions Spec: \"Whenever a driver receives a cluster time from\n        // a server it MUST compare it to the current highest seen cluster time\n        // for the deployment. If the new cluster time is higher than the\n        // highest seen cluster time it MUST become the new highest seen cluster\n        // time. Two cluster times are compared using only the BsonTimestamp\n        // value of the clusterTime embedded field.\"\n        const clusterTime = serverDescription.$clusterTime;\n        if (clusterTime) {\n            (0, common_1._advanceClusterTime)(this, clusterTime);\n        }\n        // If we already know all the information contained in this updated description, then\n        // we don't need to emit SDAM events, but still need to update the description, in order\n        // to keep client-tracked attributes like last update time and round trip time up to date\n        const equalDescriptions = previousServerDescription && previousServerDescription.equals(serverDescription);\n        // first update the TopologyDescription\n        this.s.description = this.s.description.update(serverDescription);\n        if (this.s.description.compatibilityError) {\n            this.emit(Topology.ERROR, new error_1.MongoCompatibilityError(this.s.description.compatibilityError));\n            return;\n        }\n        // emit monitoring events for this change\n        if (!equalDescriptions) {\n            const newDescription = this.s.description.servers.get(serverDescription.address);\n            if (newDescription) {\n                this.emit(Topology.SERVER_DESCRIPTION_CHANGED, new events_1.ServerDescriptionChangedEvent(this.s.id, serverDescription.address, previousServerDescription, newDescription));\n            }\n        }\n        // update server list from updated descriptions\n        updateServers(this, serverDescription);\n        // attempt to resolve any outstanding server selection attempts\n        if (this[kWaitQueue].length > 0) {\n            processWaitQueue(this);\n        }\n        if (!equalDescriptions) {\n            this.emit(Topology.TOPOLOGY_DESCRIPTION_CHANGED, new events_1.TopologyDescriptionChangedEvent(this.s.id, previousTopologyDescription, this.s.description));\n        }\n    }\n    auth(credentials, callback) {\n        if (typeof credentials === 'function')\n            (callback = credentials), (credentials = undefined);\n        if (typeof callback === 'function')\n            callback(undefined, true);\n    }\n    get clientMetadata() {\n        return this.s.options.metadata;\n    }\n    isConnected() {\n        return this.s.state === common_1.STATE_CONNECTED;\n    }\n    isDestroyed() {\n        return this.s.state === common_1.STATE_CLOSED;\n    }\n    /**\n     * @deprecated This function is deprecated and will be removed in the next major version.\n     */\n    unref() {\n        (0, utils_1.emitWarning)('`unref` is a noop and will be removed in the next major version');\n    }\n    // NOTE: There are many places in code where we explicitly check the last isMaster\n    //       to do feature support detection. This should be done any other way, but for\n    //       now we will just return the first isMaster seen, which should suffice.\n    lastIsMaster() {\n        const serverDescriptions = Array.from(this.description.servers.values());\n        if (serverDescriptions.length === 0)\n            return {};\n        const sd = serverDescriptions.filter((sd) => sd.type !== common_1.ServerType.Unknown)[0];\n        const result = sd || { maxWireVersion: this.description.commonWireVersion };\n        return result;\n    }\n    get commonWireVersion() {\n        return this.description.commonWireVersion;\n    }\n    get logicalSessionTimeoutMinutes() {\n        return this.description.logicalSessionTimeoutMinutes;\n    }\n    get clusterTime() {\n        return this.s.clusterTime;\n    }\n    set clusterTime(clusterTime) {\n        this.s.clusterTime = clusterTime;\n    }\n}\nexports.Topology = Topology;\n/** @event */\nTopology.SERVER_OPENING = 'serverOpening';\n/** @event */\nTopology.SERVER_CLOSED = 'serverClosed';\n/** @event */\nTopology.SERVER_DESCRIPTION_CHANGED = 'serverDescriptionChanged';\n/** @event */\nTopology.TOPOLOGY_OPENING = 'topologyOpening';\n/** @event */\nTopology.TOPOLOGY_CLOSED = 'topologyClosed';\n/** @event */\nTopology.TOPOLOGY_DESCRIPTION_CHANGED = 'topologyDescriptionChanged';\n/** @event */\nTopology.ERROR = 'error';\n/** @event */\nTopology.OPEN = 'open';\n/** @event */\nTopology.CONNECT = 'connect';\n/** @event */\nTopology.CLOSE = 'close';\n/** @event */\nTopology.TIMEOUT = 'timeout';\n/** @public */\nexports.TOPOLOGY_EVENTS = [\n    Topology.SERVER_OPENING,\n    Topology.SERVER_CLOSED,\n    Topology.SERVER_DESCRIPTION_CHANGED,\n    Topology.TOPOLOGY_OPENING,\n    Topology.TOPOLOGY_CLOSED,\n    Topology.TOPOLOGY_DESCRIPTION_CHANGED,\n    Topology.ERROR,\n    Topology.TIMEOUT,\n    Topology.CLOSE\n];\n/** Destroys a server, and removes all event listeners from the instance */\nfunction destroyServer(server, topology, options, callback) {\n    options = options !== null && options !== void 0 ? options : {};\n    for (const event of LOCAL_SERVER_EVENTS) {\n        server.removeAllListeners(event);\n    }\n    server.destroy(options, () => {\n        topology.emit(Topology.SERVER_CLOSED, new events_1.ServerClosedEvent(topology.s.id, server.description.address));\n        for (const event of SERVER_RELAY_EVENTS) {\n            server.removeAllListeners(event);\n        }\n        if (typeof callback === 'function') {\n            callback();\n        }\n    });\n}\n/** Predicts the TopologyType from options */\nfunction topologyTypeFromOptions(options) {\n    if (options === null || options === void 0 ? void 0 : options.directConnection) {\n        return common_1.TopologyType.Single;\n    }\n    if (options === null || options === void 0 ? void 0 : options.replicaSet) {\n        return common_1.TopologyType.ReplicaSetNoPrimary;\n    }\n    if (options === null || options === void 0 ? void 0 : options.loadBalanced) {\n        return common_1.TopologyType.LoadBalanced;\n    }\n    return common_1.TopologyType.Unknown;\n}\nfunction randomSelection(array) {\n    return array[Math.floor(Math.random() * array.length)];\n}\n/**\n * Creates new server instances and attempts to connect them\n *\n * @param topology - The topology that this server belongs to\n * @param serverDescription - The description for the server to initialize and connect to\n * @param connectDelay - Time to wait before attempting initial connection\n */\nfunction createAndConnectServer(topology, serverDescription, connectDelay) {\n    topology.emit(Topology.SERVER_OPENING, new events_1.ServerOpeningEvent(topology.s.id, serverDescription.address));\n    const server = new server_1.Server(topology, serverDescription, topology.s.options);\n    for (const event of SERVER_RELAY_EVENTS) {\n        server.on(event, (e) => topology.emit(event, e));\n    }\n    server.on(server_1.Server.DESCRIPTION_RECEIVED, description => topology.serverUpdateHandler(description));\n    if (connectDelay) {\n        const connectTimer = setTimeout(() => {\n            (0, common_1.clearAndRemoveTimerFrom)(connectTimer, topology.s.connectionTimers);\n            server.connect();\n        }, connectDelay);\n        topology.s.connectionTimers.add(connectTimer);\n        return server;\n    }\n    server.connect();\n    return server;\n}\n/**\n * Create `Server` instances for all initially known servers, connect them, and assign\n * them to the passed in `Topology`.\n *\n * @param topology - The topology responsible for the servers\n * @param serverDescriptions - A list of server descriptions to connect\n */\nfunction connectServers(topology, serverDescriptions) {\n    topology.s.servers = serverDescriptions.reduce((servers, serverDescription) => {\n        const server = createAndConnectServer(topology, serverDescription);\n        servers.set(serverDescription.address, server);\n        return servers;\n    }, new Map());\n}\n/**\n * @param topology - Topology to update.\n * @param incomingServerDescription - New server description.\n */\nfunction updateServers(topology, incomingServerDescription) {\n    // update the internal server's description\n    if (incomingServerDescription && topology.s.servers.has(incomingServerDescription.address)) {\n        const server = topology.s.servers.get(incomingServerDescription.address);\n        if (server) {\n            server.s.description = incomingServerDescription;\n        }\n    }\n    // add new servers for all descriptions we currently don't know about locally\n    for (const serverDescription of topology.description.servers.values()) {\n        if (!topology.s.servers.has(serverDescription.address)) {\n            const server = createAndConnectServer(topology, serverDescription);\n            topology.s.servers.set(serverDescription.address, server);\n        }\n    }\n    // for all servers no longer known, remove their descriptions and destroy their instances\n    for (const entry of topology.s.servers) {\n        const serverAddress = entry[0];\n        if (topology.description.hasServer(serverAddress)) {\n            continue;\n        }\n        if (!topology.s.servers.has(serverAddress)) {\n            continue;\n        }\n        const server = topology.s.servers.get(serverAddress);\n        topology.s.servers.delete(serverAddress);\n        // prepare server for garbage collection\n        if (server) {\n            destroyServer(server, topology);\n        }\n    }\n}\nfunction drainWaitQueue(queue, err) {\n    while (queue.length) {\n        const waitQueueMember = queue.shift();\n        if (!waitQueueMember) {\n            continue;\n        }\n        if (waitQueueMember.timer) {\n            clearTimeout(waitQueueMember.timer);\n        }\n        if (!waitQueueMember[kCancelled]) {\n            waitQueueMember.callback(err);\n        }\n    }\n}\nfunction processWaitQueue(topology) {\n    if (topology.s.state === common_1.STATE_CLOSED) {\n        drainWaitQueue(topology[kWaitQueue], new error_1.MongoTopologyClosedError());\n        return;\n    }\n    const isSharded = topology.description.type === common_1.TopologyType.Sharded;\n    const serverDescriptions = Array.from(topology.description.servers.values());\n    const membersToProcess = topology[kWaitQueue].length;\n    for (let i = 0; i < membersToProcess; ++i) {\n        const waitQueueMember = topology[kWaitQueue].shift();\n        if (!waitQueueMember) {\n            continue;\n        }\n        if (waitQueueMember[kCancelled]) {\n            continue;\n        }\n        let selectedDescriptions;\n        try {\n            const serverSelector = waitQueueMember.serverSelector;\n            selectedDescriptions = serverSelector\n                ? serverSelector(topology.description, serverDescriptions)\n                : serverDescriptions;\n        }\n        catch (e) {\n            if (waitQueueMember.timer) {\n                clearTimeout(waitQueueMember.timer);\n            }\n            waitQueueMember.callback(e);\n            continue;\n        }\n        if (selectedDescriptions.length === 0) {\n            topology[kWaitQueue].push(waitQueueMember);\n            continue;\n        }\n        const selectedServerDescription = randomSelection(selectedDescriptions);\n        const selectedServer = topology.s.servers.get(selectedServerDescription.address);\n        const transaction = waitQueueMember.transaction;\n        if (isSharded && transaction && transaction.isActive && selectedServer) {\n            transaction.pinServer(selectedServer);\n        }\n        if (waitQueueMember.timer) {\n            clearTimeout(waitQueueMember.timer);\n        }\n        waitQueueMember.callback(undefined, selectedServer);\n    }\n    if (topology[kWaitQueue].length > 0) {\n        // ensure all server monitors attempt monitoring soon\n        for (const [, server] of topology.s.servers) {\n            process.nextTick(function scheduleServerCheck() {\n                return server.requestCheck();\n            });\n        }\n    }\n}\nfunction isStaleServerDescription(topologyDescription, incomingServerDescription) {\n    const currentServerDescription = topologyDescription.servers.get(incomingServerDescription.address);\n    const currentTopologyVersion = currentServerDescription === null || currentServerDescription === void 0 ? void 0 : currentServerDescription.topologyVersion;\n    return ((0, server_description_1.compareTopologyVersion)(currentTopologyVersion, incomingServerDescription.topologyVersion) > 0);\n}\n/** @public */\nclass ServerCapabilities {\n    constructor(ismaster) {\n        this.minWireVersion = ismaster.minWireVersion || 0;\n        this.maxWireVersion = ismaster.maxWireVersion || 0;\n    }\n    get hasAggregationCursor() {\n        return this.maxWireVersion >= 1;\n    }\n    get hasWriteCommands() {\n        return this.maxWireVersion >= 2;\n    }\n    get hasTextSearch() {\n        return this.minWireVersion >= 0;\n    }\n    get hasAuthCommands() {\n        return this.maxWireVersion >= 1;\n    }\n    get hasListCollectionsCommand() {\n        return this.maxWireVersion >= 3;\n    }\n    get hasListIndexesCommand() {\n        return this.maxWireVersion >= 3;\n    }\n    get supportsSnapshotReads() {\n        return this.maxWireVersion >= 13;\n    }\n    get commandsTakeWriteConcern() {\n        return this.maxWireVersion >= 5;\n    }\n    get commandsTakeCollation() {\n        return this.maxWireVersion >= 5;\n    }\n}\nexports.ServerCapabilities = ServerCapabilities;\n//# sourceMappingURL=topology.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.HEARTBEAT_EVENTS = exports.Server = void 0;\nconst logger_1 = require(\"../logger\");\nconst connection_pool_1 = require(\"../cmap/connection_pool\");\nconst server_description_1 = require(\"./server_description\");\nconst monitor_1 = require(\"./monitor\");\nconst transactions_1 = require(\"../transactions\");\nconst utils_1 = require(\"../utils\");\nconst common_1 = require(\"./common\");\nconst error_1 = require(\"../error\");\nconst connection_1 = require(\"../cmap/connection\");\nconst mongo_types_1 = require(\"../mongo_types\");\nconst utils_2 = require(\"../utils\");\nconst stateTransition = (0, utils_1.makeStateMachine)({\n    [common_1.STATE_CLOSED]: [common_1.STATE_CLOSED, common_1.STATE_CONNECTING],\n    [common_1.STATE_CONNECTING]: [common_1.STATE_CONNECTING, common_1.STATE_CLOSING, common_1.STATE_CONNECTED, common_1.STATE_CLOSED],\n    [common_1.STATE_CONNECTED]: [common_1.STATE_CONNECTED, common_1.STATE_CLOSING, common_1.STATE_CLOSED],\n    [common_1.STATE_CLOSING]: [common_1.STATE_CLOSING, common_1.STATE_CLOSED]\n});\n/** @internal */\nconst kMonitor = Symbol('monitor');\n/** @internal */\nclass Server extends mongo_types_1.TypedEventEmitter {\n    /**\n     * Create a server\n     */\n    constructor(topology, description, options) {\n        super();\n        this.serverApi = options.serverApi;\n        const poolOptions = { hostAddress: description.hostAddress, ...options };\n        this.s = {\n            description,\n            options,\n            logger: new logger_1.Logger('Server'),\n            state: common_1.STATE_CLOSED,\n            topology,\n            pool: new connection_pool_1.ConnectionPool(poolOptions)\n        };\n        for (const event of [...connection_pool_1.CMAP_EVENTS, ...connection_1.APM_EVENTS]) {\n            this.s.pool.on(event, (e) => this.emit(event, e));\n        }\n        this.s.pool.on(connection_1.Connection.CLUSTER_TIME_RECEIVED, (clusterTime) => {\n            this.clusterTime = clusterTime;\n        });\n        // monitoring is disabled in load balancing mode\n        if (this.loadBalanced)\n            return;\n        // create the monitor\n        this[kMonitor] = new monitor_1.Monitor(this, this.s.options);\n        for (const event of exports.HEARTBEAT_EVENTS) {\n            this[kMonitor].on(event, (e) => this.emit(event, e));\n        }\n        this[kMonitor].on('resetConnectionPool', () => {\n            this.s.pool.clear();\n        });\n        this[kMonitor].on('resetServer', (error) => markServerUnknown(this, error));\n        this[kMonitor].on(Server.SERVER_HEARTBEAT_SUCCEEDED, (event) => {\n            this.emit(Server.DESCRIPTION_RECEIVED, new server_description_1.ServerDescription(this.description.hostAddress, event.reply, {\n                roundTripTime: calculateRoundTripTime(this.description.roundTripTime, event.duration)\n            }));\n            if (this.s.state === common_1.STATE_CONNECTING) {\n                stateTransition(this, common_1.STATE_CONNECTED);\n                this.emit(Server.CONNECT, this);\n            }\n        });\n    }\n    get description() {\n        return this.s.description;\n    }\n    get name() {\n        return this.s.description.address;\n    }\n    get autoEncrypter() {\n        if (this.s.options && this.s.options.autoEncrypter) {\n            return this.s.options.autoEncrypter;\n        }\n    }\n    get loadBalanced() {\n        return this.s.topology.description.type === common_1.TopologyType.LoadBalanced;\n    }\n    /**\n     * Initiate server connect\n     */\n    connect() {\n        if (this.s.state !== common_1.STATE_CLOSED) {\n            return;\n        }\n        stateTransition(this, common_1.STATE_CONNECTING);\n        // If in load balancer mode we automatically set the server to\n        // a load balancer. It never transitions out of this state and\n        // has no monitor.\n        if (!this.loadBalanced) {\n            this[kMonitor].connect();\n        }\n        else {\n            stateTransition(this, common_1.STATE_CONNECTED);\n            this.emit(Server.CONNECT, this);\n        }\n    }\n    /** Destroy the server connection */\n    destroy(options, callback) {\n        if (typeof options === 'function')\n            (callback = options), (options = {});\n        options = Object.assign({}, { force: false }, options);\n        if (this.s.state === common_1.STATE_CLOSED) {\n            if (typeof callback === 'function') {\n                callback();\n            }\n            return;\n        }\n        stateTransition(this, common_1.STATE_CLOSING);\n        if (!this.loadBalanced) {\n            this[kMonitor].close();\n        }\n        this.s.pool.close(options, err => {\n            stateTransition(this, common_1.STATE_CLOSED);\n            this.emit('closed');\n            if (typeof callback === 'function') {\n                callback(err);\n            }\n        });\n    }\n    /**\n     * Immediately schedule monitoring of this server. If there already an attempt being made\n     * this will be a no-op.\n     */\n    requestCheck() {\n        if (!this.loadBalanced) {\n            this[kMonitor].requestCheck();\n        }\n    }\n    command(ns, cmd, options, callback) {\n        if (typeof options === 'function') {\n            (callback = options), (options = {}), (options = options !== null && options !== void 0 ? options : {});\n        }\n        if (callback == null) {\n            throw new error_1.MongoInvalidArgumentError('Callback must be provided');\n        }\n        if (ns.db == null || typeof ns === 'string') {\n            throw new error_1.MongoInvalidArgumentError('Namespace must not be a string');\n        }\n        if (this.s.state === common_1.STATE_CLOSING || this.s.state === common_1.STATE_CLOSED) {\n            callback(new error_1.MongoServerClosedError());\n            return;\n        }\n        // Clone the options\n        const finalOptions = Object.assign({}, options, { wireProtocolCommand: false });\n        // There are cases where we need to flag the read preference not to get sent in\n        // the command, such as pre-5.0 servers attempting to perform an aggregate write\n        // with a non-primary read preference. In this case the effective read preference\n        // (primary) is not the same as the provided and must be removed completely.\n        if (finalOptions.omitReadPreference) {\n            delete finalOptions.readPreference;\n        }\n        // error if collation not supported\n        if ((0, utils_1.collationNotSupported)(this, cmd)) {\n            callback(new error_1.MongoCompatibilityError(`Server ${this.name} does not support collation`));\n            return;\n        }\n        const session = finalOptions.session;\n        const conn = session === null || session === void 0 ? void 0 : session.pinnedConnection;\n        // NOTE: This is a hack! We can't retrieve the connections used for executing an operation\n        //       (and prevent them from being checked back in) at the point of operation execution.\n        //       This should be considered as part of the work for NODE-2882\n        if (this.loadBalanced && session && conn == null && isPinnableCommand(cmd, session)) {\n            this.s.pool.checkOut((err, checkedOut) => {\n                if (err || checkedOut == null) {\n                    if (callback)\n                        return callback(err);\n                    return;\n                }\n                session.pin(checkedOut);\n                this.command(ns, cmd, finalOptions, callback);\n            });\n            return;\n        }\n        this.s.pool.withConnection(conn, (err, conn, cb) => {\n            if (err || !conn) {\n                markServerUnknown(this, err);\n                return cb(err);\n            }\n            conn.command(ns, cmd, finalOptions, makeOperationHandler(this, conn, cmd, finalOptions, cb));\n        }, callback);\n    }\n    /**\n     * Execute a query against the server\n     * @internal\n     */\n    query(ns, cmd, options, callback) {\n        if (this.s.state === common_1.STATE_CLOSING || this.s.state === common_1.STATE_CLOSED) {\n            callback(new error_1.MongoServerClosedError());\n            return;\n        }\n        this.s.pool.withConnection(undefined, (err, conn, cb) => {\n            if (err || !conn) {\n                markServerUnknown(this, err);\n                return cb(err);\n            }\n            conn.query(ns, cmd, options, makeOperationHandler(this, conn, cmd, options, cb));\n        }, callback);\n    }\n    /**\n     * Execute a `getMore` against the server\n     * @internal\n     */\n    getMore(ns, cursorId, options, callback) {\n        var _a;\n        if (this.s.state === common_1.STATE_CLOSING || this.s.state === common_1.STATE_CLOSED) {\n            callback(new error_1.MongoServerClosedError());\n            return;\n        }\n        this.s.pool.withConnection((_a = options.session) === null || _a === void 0 ? void 0 : _a.pinnedConnection, (err, conn, cb) => {\n            if (err || !conn) {\n                markServerUnknown(this, err);\n                return cb(err);\n            }\n            conn.getMore(ns, cursorId, options, makeOperationHandler(this, conn, {}, options, cb));\n        }, callback);\n    }\n    /**\n     * Execute a `killCursors` command against the server\n     * @internal\n     */\n    killCursors(ns, cursorIds, options, callback) {\n        var _a;\n        if (this.s.state === common_1.STATE_CLOSING || this.s.state === common_1.STATE_CLOSED) {\n            if (typeof callback === 'function') {\n                callback(new error_1.MongoServerClosedError());\n            }\n            return;\n        }\n        this.s.pool.withConnection((_a = options.session) === null || _a === void 0 ? void 0 : _a.pinnedConnection, (err, conn, cb) => {\n            if (err || !conn) {\n                markServerUnknown(this, err);\n                return cb(err);\n            }\n            conn.killCursors(ns, cursorIds, options, makeOperationHandler(this, conn, {}, undefined, cb));\n        }, callback);\n    }\n}\nexports.Server = Server;\n/** @event */\nServer.SERVER_HEARTBEAT_STARTED = 'serverHeartbeatStarted';\n/** @event */\nServer.SERVER_HEARTBEAT_SUCCEEDED = 'serverHeartbeatSucceeded';\n/** @event */\nServer.SERVER_HEARTBEAT_FAILED = 'serverHeartbeatFailed';\n/** @event */\nServer.CONNECT = 'connect';\n/** @event */\nServer.DESCRIPTION_RECEIVED = 'descriptionReceived';\n/** @event */\nServer.CLOSED = 'closed';\n/** @event */\nServer.ENDED = 'ended';\nexports.HEARTBEAT_EVENTS = [\n    Server.SERVER_HEARTBEAT_STARTED,\n    Server.SERVER_HEARTBEAT_SUCCEEDED,\n    Server.SERVER_HEARTBEAT_FAILED\n];\nObject.defineProperty(Server.prototype, 'clusterTime', {\n    get() {\n        return this.s.topology.clusterTime;\n    },\n    set(clusterTime) {\n        this.s.topology.clusterTime = clusterTime;\n    }\n});\nfunction calculateRoundTripTime(oldRtt, duration) {\n    if (oldRtt === -1) {\n        return duration;\n    }\n    const alpha = 0.2;\n    return alpha * duration + (1 - alpha) * oldRtt;\n}\nfunction markServerUnknown(server, error) {\n    // Load balancer servers can never be marked unknown.\n    if (server.loadBalanced) {\n        return;\n    }\n    if (error instanceof error_1.MongoNetworkError && !(error instanceof error_1.MongoNetworkTimeoutError)) {\n        server[kMonitor].reset();\n    }\n    server.emit(Server.DESCRIPTION_RECEIVED, new server_description_1.ServerDescription(server.description.hostAddress, undefined, {\n        error,\n        topologyVersion: error && error.topologyVersion ? error.topologyVersion : server.description.topologyVersion\n    }));\n}\nfunction isPinnableCommand(cmd, session) {\n    if (session) {\n        return (session.inTransaction() ||\n            'aggregate' in cmd ||\n            'find' in cmd ||\n            'getMore' in cmd ||\n            'listCollections' in cmd ||\n            'listIndexes' in cmd);\n    }\n    return false;\n}\nfunction connectionIsStale(pool, connection) {\n    if (connection.serviceId) {\n        return (connection.generation !== pool.serviceGenerations.get(connection.serviceId.toHexString()));\n    }\n    return connection.generation !== pool.generation;\n}\nfunction shouldHandleStateChangeError(server, err) {\n    const etv = err.topologyVersion;\n    const stv = server.description.topologyVersion;\n    return (0, server_description_1.compareTopologyVersion)(stv, etv) < 0;\n}\nfunction inActiveTransaction(session, cmd) {\n    return session && session.inTransaction() && !(0, transactions_1.isTransactionCommand)(cmd);\n}\n/** this checks the retryWrites option passed down from the client options, it\n * does not check if the server supports retryable writes */\nfunction isRetryableWritesEnabled(topology) {\n    return topology.s.options.retryWrites !== false;\n}\nfunction makeOperationHandler(server, connection, cmd, options, callback) {\n    const session = options === null || options === void 0 ? void 0 : options.session;\n    return function handleOperationResult(err, result) {\n        if (err && !connectionIsStale(server.s.pool, connection)) {\n            if (err instanceof error_1.MongoNetworkError) {\n                if (session && !session.hasEnded && session.serverSession) {\n                    session.serverSession.isDirty = true;\n                }\n                // inActiveTransaction check handles commit and abort.\n                if (inActiveTransaction(session, cmd) && !err.hasErrorLabel('TransientTransactionError')) {\n                    err.addErrorLabel('TransientTransactionError');\n                }\n                if ((isRetryableWritesEnabled(server.s.topology) || (0, transactions_1.isTransactionCommand)(cmd)) &&\n                    (0, utils_2.supportsRetryableWrites)(server) &&\n                    !inActiveTransaction(session, cmd)) {\n                    err.addErrorLabel('RetryableWriteError');\n                }\n                if (!(err instanceof error_1.MongoNetworkTimeoutError) || (0, error_1.isNetworkErrorBeforeHandshake)(err)) {\n                    // In load balanced mode we never mark the server as unknown and always\n                    // clear for the specific service id.\n                    server.s.pool.clear(connection.serviceId);\n                    if (!server.loadBalanced) {\n                        markServerUnknown(server, err);\n                    }\n                }\n            }\n            else {\n                // if pre-4.4 server, then add error label if its a retryable write error\n                if ((isRetryableWritesEnabled(server.s.topology) || (0, transactions_1.isTransactionCommand)(cmd)) &&\n                    (0, utils_1.maxWireVersion)(server) < 9 &&\n                    (0, error_1.isRetryableWriteError)(err) &&\n                    !inActiveTransaction(session, cmd)) {\n                    err.addErrorLabel('RetryableWriteError');\n                }\n                if ((0, error_1.isSDAMUnrecoverableError)(err)) {\n                    if (shouldHandleStateChangeError(server, err)) {\n                        if ((0, utils_1.maxWireVersion)(server) <= 7 || (0, error_1.isNodeShuttingDownError)(err)) {\n                            server.s.pool.clear(connection.serviceId);\n                        }\n                        if (!server.loadBalanced) {\n                            markServerUnknown(server, err);\n                            process.nextTick(() => server.requestCheck());\n                        }\n                    }\n                }\n            }\n            if (session && session.isPinned && err.hasErrorLabel('TransientTransactionError')) {\n                session.unpin({ force: true });\n            }\n        }\n        callback(err, result);\n    };\n}\n//# sourceMappingURL=server.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.CMAP_EVENTS = exports.ConnectionPool = void 0;\nconst Denque = require(\"denque\");\nconst connection_1 = require(\"./connection\");\nconst logger_1 = require(\"../logger\");\nconst metrics_1 = require(\"./metrics\");\nconst connect_1 = require(\"./connect\");\nconst utils_1 = require(\"../utils\");\nconst error_1 = require(\"../error\");\nconst errors_1 = require(\"./errors\");\nconst connection_pool_events_1 = require(\"./connection_pool_events\");\nconst mongo_types_1 = require(\"../mongo_types\");\n/** @internal */\nconst kLogger = Symbol('logger');\n/** @internal */\nconst kConnections = Symbol('connections');\n/** @internal */\nconst kPermits = Symbol('permits');\n/** @internal */\nconst kMinPoolSizeTimer = Symbol('minPoolSizeTimer');\n/** @internal */\nconst kGeneration = Symbol('generation');\n/** @internal */\nconst kServiceGenerations = Symbol('serviceGenerations');\n/** @internal */\nconst kConnectionCounter = Symbol('connectionCounter');\n/** @internal */\nconst kCancellationToken = Symbol('cancellationToken');\n/** @internal */\nconst kWaitQueue = Symbol('waitQueue');\n/** @internal */\nconst kCancelled = Symbol('cancelled');\n/** @internal */\nconst kMetrics = Symbol('metrics');\n/** @internal */\nconst kCheckedOut = Symbol('checkedOut');\n/** @internal */\nconst kProcessingWaitQueue = Symbol('processingWaitQueue');\n/**\n * A pool of connections which dynamically resizes, and emit events related to pool activity\n * @internal\n */\nclass ConnectionPool extends mongo_types_1.TypedEventEmitter {\n    /** @internal */\n    constructor(options) {\n        var _a, _b, _c, _d;\n        super();\n        this.closed = false;\n        this.options = Object.freeze({\n            ...options,\n            connectionType: connection_1.Connection,\n            maxPoolSize: (_a = options.maxPoolSize) !== null && _a !== void 0 ? _a : 100,\n            minPoolSize: (_b = options.minPoolSize) !== null && _b !== void 0 ? _b : 0,\n            maxIdleTimeMS: (_c = options.maxIdleTimeMS) !== null && _c !== void 0 ? _c : 0,\n            waitQueueTimeoutMS: (_d = options.waitQueueTimeoutMS) !== null && _d !== void 0 ? _d : 0,\n            autoEncrypter: options.autoEncrypter,\n            metadata: options.metadata\n        });\n        if (this.options.minPoolSize > this.options.maxPoolSize) {\n            throw new error_1.MongoInvalidArgumentError('Connection pool minimum size must not be greater than maximum pool size');\n        }\n        this[kLogger] = new logger_1.Logger('ConnectionPool');\n        this[kConnections] = new Denque();\n        this[kPermits] = this.options.maxPoolSize;\n        this[kMinPoolSizeTimer] = undefined;\n        this[kGeneration] = 0;\n        this[kServiceGenerations] = new Map();\n        this[kConnectionCounter] = (0, utils_1.makeCounter)(1);\n        this[kCancellationToken] = new mongo_types_1.CancellationToken();\n        this[kCancellationToken].setMaxListeners(Infinity);\n        this[kWaitQueue] = new Denque();\n        this[kMetrics] = new metrics_1.ConnectionPoolMetrics();\n        this[kCheckedOut] = 0;\n        this[kProcessingWaitQueue] = false;\n        process.nextTick(() => {\n            this.emit(ConnectionPool.CONNECTION_POOL_CREATED, new connection_pool_events_1.ConnectionPoolCreatedEvent(this));\n            ensureMinPoolSize(this);\n        });\n    }\n    /** The address of the endpoint the pool is connected to */\n    get address() {\n        return this.options.hostAddress.toString();\n    }\n    /** An integer representing the SDAM generation of the pool */\n    get generation() {\n        return this[kGeneration];\n    }\n    /** An integer expressing how many total connections (active + in use) the pool currently has */\n    get totalConnectionCount() {\n        return this[kConnections].length + (this.options.maxPoolSize - this[kPermits]);\n    }\n    /** An integer expressing how many connections are currently available in the pool. */\n    get availableConnectionCount() {\n        return this[kConnections].length;\n    }\n    get waitQueueSize() {\n        return this[kWaitQueue].length;\n    }\n    get loadBalanced() {\n        return this.options.loadBalanced;\n    }\n    get serviceGenerations() {\n        return this[kServiceGenerations];\n    }\n    get currentCheckedOutCount() {\n        return this[kCheckedOut];\n    }\n    /**\n     * Get the metrics information for the pool when a wait queue timeout occurs.\n     */\n    waitQueueErrorMetrics() {\n        return this[kMetrics].info(this.options.maxPoolSize);\n    }\n    /**\n     * Check a connection out of this pool. The connection will continue to be tracked, but no reference to it\n     * will be held by the pool. This means that if a connection is checked out it MUST be checked back in or\n     * explicitly destroyed by the new owner.\n     */\n    checkOut(callback) {\n        this.emit(ConnectionPool.CONNECTION_CHECK_OUT_STARTED, new connection_pool_events_1.ConnectionCheckOutStartedEvent(this));\n        if (this.closed) {\n            this.emit(ConnectionPool.CONNECTION_CHECK_OUT_FAILED, new connection_pool_events_1.ConnectionCheckOutFailedEvent(this, 'poolClosed'));\n            callback(new errors_1.PoolClosedError(this));\n            return;\n        }\n        const waitQueueMember = { callback };\n        const waitQueueTimeoutMS = this.options.waitQueueTimeoutMS;\n        if (waitQueueTimeoutMS) {\n            waitQueueMember.timer = setTimeout(() => {\n                waitQueueMember[kCancelled] = true;\n                waitQueueMember.timer = undefined;\n                this.emit(ConnectionPool.CONNECTION_CHECK_OUT_FAILED, new connection_pool_events_1.ConnectionCheckOutFailedEvent(this, 'timeout'));\n                waitQueueMember.callback(new errors_1.WaitQueueTimeoutError(this.loadBalanced\n                    ? this.waitQueueErrorMetrics()\n                    : 'Timed out while checking out a connection from connection pool', this.address));\n            }, waitQueueTimeoutMS);\n        }\n        this[kCheckedOut] = this[kCheckedOut] + 1;\n        this[kWaitQueue].push(waitQueueMember);\n        process.nextTick(processWaitQueue, this);\n    }\n    /**\n     * Check a connection into the pool.\n     *\n     * @param connection - The connection to check in\n     */\n    checkIn(connection) {\n        const poolClosed = this.closed;\n        const stale = connectionIsStale(this, connection);\n        const willDestroy = !!(poolClosed || stale || connection.closed);\n        if (!willDestroy) {\n            connection.markAvailable();\n            this[kConnections].unshift(connection);\n        }\n        this[kCheckedOut] = this[kCheckedOut] - 1;\n        this.emit(ConnectionPool.CONNECTION_CHECKED_IN, new connection_pool_events_1.ConnectionCheckedInEvent(this, connection));\n        if (willDestroy) {\n            const reason = connection.closed ? 'error' : poolClosed ? 'poolClosed' : 'stale';\n            destroyConnection(this, connection, reason);\n        }\n        process.nextTick(processWaitQueue, this);\n    }\n    /**\n     * Clear the pool\n     *\n     * Pool reset is handled by incrementing the pool's generation count. Any existing connection of a\n     * previous generation will eventually be pruned during subsequent checkouts.\n     */\n    clear(serviceId) {\n        if (this.loadBalanced && serviceId) {\n            const sid = serviceId.toHexString();\n            const generation = this.serviceGenerations.get(sid);\n            // Only need to worry if the generation exists, since it should\n            // always be there but typescript needs the check.\n            if (generation == null) {\n                // TODO(NODE-3483)\n                throw new error_1.MongoRuntimeError('Service generations are required in load balancer mode.');\n            }\n            else {\n                // Increment the generation for the service id.\n                this.serviceGenerations.set(sid, generation + 1);\n            }\n        }\n        else {\n            this[kGeneration] += 1;\n        }\n        this.emit('connectionPoolCleared', new connection_pool_events_1.ConnectionPoolClearedEvent(this, serviceId));\n    }\n    close(_options, _cb) {\n        let options = _options;\n        const callback = (_cb !== null && _cb !== void 0 ? _cb : _options);\n        if (typeof options === 'function') {\n            options = {};\n        }\n        options = Object.assign({ force: false }, options);\n        if (this.closed) {\n            return callback();\n        }\n        // immediately cancel any in-flight connections\n        this[kCancellationToken].emit('cancel');\n        // drain the wait queue\n        while (this.waitQueueSize) {\n            const waitQueueMember = this[kWaitQueue].pop();\n            if (waitQueueMember) {\n                if (waitQueueMember.timer) {\n                    clearTimeout(waitQueueMember.timer);\n                }\n                if (!waitQueueMember[kCancelled]) {\n                    // TODO(NODE-3483): Replace with MongoConnectionPoolClosedError\n                    waitQueueMember.callback(new error_1.MongoRuntimeError('Connection pool closed'));\n                }\n            }\n        }\n        // clear the min pool size timer\n        const minPoolSizeTimer = this[kMinPoolSizeTimer];\n        if (minPoolSizeTimer) {\n            clearTimeout(minPoolSizeTimer);\n        }\n        // end the connection counter\n        if (typeof this[kConnectionCounter].return === 'function') {\n            this[kConnectionCounter].return(undefined);\n        }\n        // mark the pool as closed immediately\n        this.closed = true;\n        (0, utils_1.eachAsync)(this[kConnections].toArray(), (conn, cb) => {\n            this.emit(ConnectionPool.CONNECTION_CLOSED, new connection_pool_events_1.ConnectionClosedEvent(this, conn, 'poolClosed'));\n            conn.destroy(options, cb);\n        }, err => {\n            this[kConnections].clear();\n            this.emit(ConnectionPool.CONNECTION_POOL_CLOSED, new connection_pool_events_1.ConnectionPoolClosedEvent(this));\n            callback(err);\n        });\n    }\n    /**\n     * Runs a lambda with an implicitly checked out connection, checking that connection back in when the lambda\n     * has completed by calling back.\n     *\n     * NOTE: please note the required signature of `fn`\n     *\n     * @remarks When in load balancer mode, connections can be pinned to cursors or transactions.\n     *   In these cases we pass the connection in to this method to ensure it is used and a new\n     *   connection is not checked out.\n     *\n     * @param conn - A pinned connection for use in load balancing mode.\n     * @param fn - A function which operates on a managed connection\n     * @param callback - The original callback\n     */\n    withConnection(conn, fn, callback) {\n        if (conn) {\n            // use the provided connection, and do _not_ check it in after execution\n            fn(undefined, conn, (fnErr, result) => {\n                if (typeof callback === 'function') {\n                    if (fnErr) {\n                        callback(fnErr);\n                    }\n                    else {\n                        callback(undefined, result);\n                    }\n                }\n            });\n            return;\n        }\n        this.checkOut((err, conn) => {\n            // don't callback with `err` here, we might want to act upon it inside `fn`\n            fn(err, conn, (fnErr, result) => {\n                if (typeof callback === 'function') {\n                    if (fnErr) {\n                        callback(fnErr);\n                    }\n                    else {\n                        callback(undefined, result);\n                    }\n                }\n                if (conn) {\n                    this.checkIn(conn);\n                }\n            });\n        });\n    }\n}\nexports.ConnectionPool = ConnectionPool;\n/**\n * Emitted when the connection pool is created.\n * @event\n */\nConnectionPool.CONNECTION_POOL_CREATED = 'connectionPoolCreated';\n/**\n * Emitted once when the connection pool is closed\n * @event\n */\nConnectionPool.CONNECTION_POOL_CLOSED = 'connectionPoolClosed';\n/**\n * Emitted each time the connection pool is cleared and it's generation incremented\n * @event\n */\nConnectionPool.CONNECTION_POOL_CLEARED = 'connectionPoolCleared';\n/**\n * Emitted when a connection is created.\n * @event\n */\nConnectionPool.CONNECTION_CREATED = 'connectionCreated';\n/**\n * Emitted when a connection becomes established, and is ready to use\n * @event\n */\nConnectionPool.CONNECTION_READY = 'connectionReady';\n/**\n * Emitted when a connection is closed\n * @event\n */\nConnectionPool.CONNECTION_CLOSED = 'connectionClosed';\n/**\n * Emitted when an attempt to check out a connection begins\n * @event\n */\nConnectionPool.CONNECTION_CHECK_OUT_STARTED = 'connectionCheckOutStarted';\n/**\n * Emitted when an attempt to check out a connection fails\n * @event\n */\nConnectionPool.CONNECTION_CHECK_OUT_FAILED = 'connectionCheckOutFailed';\n/**\n * Emitted each time a connection is successfully checked out of the connection pool\n * @event\n */\nConnectionPool.CONNECTION_CHECKED_OUT = 'connectionCheckedOut';\n/**\n * Emitted each time a connection is successfully checked into the connection pool\n * @event\n */\nConnectionPool.CONNECTION_CHECKED_IN = 'connectionCheckedIn';\nfunction ensureMinPoolSize(pool) {\n    if (pool.closed || pool.options.minPoolSize === 0) {\n        return;\n    }\n    const minPoolSize = pool.options.minPoolSize;\n    for (let i = pool.totalConnectionCount; i < minPoolSize; ++i) {\n        createConnection(pool);\n    }\n    pool[kMinPoolSizeTimer] = setTimeout(() => ensureMinPoolSize(pool), 10);\n}\nfunction connectionIsStale(pool, connection) {\n    const serviceId = connection.serviceId;\n    if (pool.loadBalanced && serviceId) {\n        const sid = serviceId.toHexString();\n        const generation = pool.serviceGenerations.get(sid);\n        return connection.generation !== generation;\n    }\n    return connection.generation !== pool[kGeneration];\n}\nfunction connectionIsIdle(pool, connection) {\n    return !!(pool.options.maxIdleTimeMS && connection.idleTime > pool.options.maxIdleTimeMS);\n}\nfunction createConnection(pool, callback) {\n    const connectOptions = {\n        ...pool.options,\n        id: pool[kConnectionCounter].next().value,\n        generation: pool[kGeneration],\n        cancellationToken: pool[kCancellationToken]\n    };\n    pool[kPermits]--;\n    (0, connect_1.connect)(connectOptions, (err, connection) => {\n        if (err || !connection) {\n            pool[kPermits]++;\n            pool[kLogger].debug(`connection attempt failed with error [${JSON.stringify(err)}]`);\n            if (typeof callback === 'function') {\n                callback(err);\n            }\n            return;\n        }\n        // The pool might have closed since we started trying to create a connection\n        if (pool.closed) {\n            connection.destroy({ force: true });\n            return;\n        }\n        // forward all events from the connection to the pool\n        for (const event of [...connection_1.APM_EVENTS, connection_1.Connection.CLUSTER_TIME_RECEIVED]) {\n            connection.on(event, (e) => pool.emit(event, e));\n        }\n        pool.emit(ConnectionPool.CONNECTION_CREATED, new connection_pool_events_1.ConnectionCreatedEvent(pool, connection));\n        if (pool.loadBalanced) {\n            connection.on(connection_1.Connection.PINNED, pinType => pool[kMetrics].markPinned(pinType));\n            connection.on(connection_1.Connection.UNPINNED, pinType => pool[kMetrics].markUnpinned(pinType));\n            const serviceId = connection.serviceId;\n            if (serviceId) {\n                let generation;\n                const sid = serviceId.toHexString();\n                if ((generation = pool.serviceGenerations.get(sid))) {\n                    connection.generation = generation;\n                }\n                else {\n                    pool.serviceGenerations.set(sid, 0);\n                    connection.generation = 0;\n                }\n            }\n        }\n        connection.markAvailable();\n        pool.emit(ConnectionPool.CONNECTION_READY, new connection_pool_events_1.ConnectionReadyEvent(pool, connection));\n        // if a callback has been provided, check out the connection immediately\n        if (typeof callback === 'function') {\n            callback(undefined, connection);\n            return;\n        }\n        // otherwise add it to the pool for later acquisition, and try to process the wait queue\n        pool[kConnections].push(connection);\n        process.nextTick(processWaitQueue, pool);\n    });\n}\nfunction destroyConnection(pool, connection, reason) {\n    pool.emit(ConnectionPool.CONNECTION_CLOSED, new connection_pool_events_1.ConnectionClosedEvent(pool, connection, reason));\n    // allow more connections to be created\n    pool[kPermits]++;\n    // destroy the connection\n    process.nextTick(() => connection.destroy());\n}\nfunction processWaitQueue(pool) {\n    if (pool.closed || pool[kProcessingWaitQueue]) {\n        return;\n    }\n    pool[kProcessingWaitQueue] = true;\n    while (pool.waitQueueSize) {\n        const waitQueueMember = pool[kWaitQueue].peekFront();\n        if (!waitQueueMember) {\n            pool[kWaitQueue].shift();\n            continue;\n        }\n        if (waitQueueMember[kCancelled]) {\n            pool[kWaitQueue].shift();\n            continue;\n        }\n        if (!pool.availableConnectionCount) {\n            break;\n        }\n        const connection = pool[kConnections].shift();\n        if (!connection) {\n            break;\n        }\n        const isStale = connectionIsStale(pool, connection);\n        const isIdle = connectionIsIdle(pool, connection);\n        if (!isStale && !isIdle && !connection.closed) {\n            pool.emit(ConnectionPool.CONNECTION_CHECKED_OUT, new connection_pool_events_1.ConnectionCheckedOutEvent(pool, connection));\n            if (waitQueueMember.timer) {\n                clearTimeout(waitQueueMember.timer);\n            }\n            pool[kWaitQueue].shift();\n            waitQueueMember.callback(undefined, connection);\n        }\n        else {\n            const reason = connection.closed ? 'error' : isStale ? 'stale' : 'idle';\n            destroyConnection(pool, connection, reason);\n        }\n    }\n    const maxPoolSize = pool.options.maxPoolSize;\n    if (pool.waitQueueSize && (maxPoolSize <= 0 || pool.totalConnectionCount < maxPoolSize)) {\n        createConnection(pool, (err, connection) => {\n            const waitQueueMember = pool[kWaitQueue].shift();\n            if (!waitQueueMember || waitQueueMember[kCancelled]) {\n                if (!err && connection) {\n                    pool[kConnections].push(connection);\n                }\n                pool[kProcessingWaitQueue] = false;\n                return;\n            }\n            if (err) {\n                pool.emit(ConnectionPool.CONNECTION_CHECK_OUT_FAILED, new connection_pool_events_1.ConnectionCheckOutFailedEvent(pool, err));\n            }\n            else if (connection) {\n                pool.emit(ConnectionPool.CONNECTION_CHECKED_OUT, new connection_pool_events_1.ConnectionCheckedOutEvent(pool, connection));\n            }\n            if (waitQueueMember.timer) {\n                clearTimeout(waitQueueMember.timer);\n            }\n            waitQueueMember.callback(err, connection);\n            pool[kProcessingWaitQueue] = false;\n            process.nextTick(() => processWaitQueue(pool));\n        });\n    }\n    else {\n        pool[kProcessingWaitQueue] = false;\n    }\n}\nexports.CMAP_EVENTS = [\n    ConnectionPool.CONNECTION_POOL_CREATED,\n    ConnectionPool.CONNECTION_POOL_CLOSED,\n    ConnectionPool.CONNECTION_CREATED,\n    ConnectionPool.CONNECTION_READY,\n    ConnectionPool.CONNECTION_CLOSED,\n    ConnectionPool.CONNECTION_CHECK_OUT_STARTED,\n    ConnectionPool.CONNECTION_CHECK_OUT_FAILED,\n    ConnectionPool.CONNECTION_CHECKED_OUT,\n    ConnectionPool.CONNECTION_CHECKED_IN,\n    ConnectionPool.CONNECTION_POOL_CLEARED\n];\n//# sourceMappingURL=connection_pool.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.LEGAL_TCP_SOCKET_OPTIONS = exports.LEGAL_TLS_SOCKET_OPTIONS = exports.connect = void 0;\nconst net = require(\"net\");\nconst tls = require(\"tls\");\nconst connection_1 = require(\"./connection\");\nconst error_1 = require(\"../error\");\nconst defaultAuthProviders_1 = require(\"./auth/defaultAuthProviders\");\nconst auth_provider_1 = require(\"./auth/auth_provider\");\nconst utils_1 = require(\"../utils\");\nconst constants_1 = require(\"./wire_protocol/constants\");\nconst bson_1 = require(\"../bson\");\nconst FAKE_MONGODB_SERVICE_ID = typeof process.env.FAKE_MONGODB_SERVICE_ID === 'string' &&\n    process.env.FAKE_MONGODB_SERVICE_ID.toLowerCase() === 'true';\nfunction connect(options, callback) {\n    makeConnection(options, (err, socket) => {\n        var _a;\n        if (err || !socket) {\n            return callback(err);\n        }\n        let ConnectionType = (_a = options.connectionType) !== null && _a !== void 0 ? _a : connection_1.Connection;\n        if (options.autoEncrypter) {\n            ConnectionType = connection_1.CryptoConnection;\n        }\n        performInitialHandshake(new ConnectionType(socket, options), options, callback);\n    });\n}\nexports.connect = connect;\nfunction checkSupportedServer(ismaster, options) {\n    var _a;\n    const serverVersionHighEnough = ismaster &&\n        (typeof ismaster.maxWireVersion === 'number' || ismaster.maxWireVersion instanceof bson_1.Int32) &&\n        ismaster.maxWireVersion >= constants_1.MIN_SUPPORTED_WIRE_VERSION;\n    const serverVersionLowEnough = ismaster &&\n        (typeof ismaster.minWireVersion === 'number' || ismaster.minWireVersion instanceof bson_1.Int32) &&\n        ismaster.minWireVersion <= constants_1.MAX_SUPPORTED_WIRE_VERSION;\n    if (serverVersionHighEnough) {\n        if (serverVersionLowEnough) {\n            return null;\n        }\n        const message = `Server at ${options.hostAddress} reports minimum wire version ${JSON.stringify(ismaster.minWireVersion)}, but this version of the Node.js Driver requires at most ${constants_1.MAX_SUPPORTED_WIRE_VERSION} (MongoDB ${constants_1.MAX_SUPPORTED_SERVER_VERSION})`;\n        return new error_1.MongoCompatibilityError(message);\n    }\n    const message = `Server at ${options.hostAddress} reports maximum wire version ${(_a = JSON.stringify(ismaster.maxWireVersion)) !== null && _a !== void 0 ? _a : 0}, but this version of the Node.js Driver requires at least ${constants_1.MIN_SUPPORTED_WIRE_VERSION} (MongoDB ${constants_1.MIN_SUPPORTED_SERVER_VERSION})`;\n    return new error_1.MongoCompatibilityError(message);\n}\nfunction performInitialHandshake(conn, options, _callback) {\n    const callback = function (err, ret) {\n        if (err && conn) {\n            conn.destroy();\n        }\n        _callback(err, ret);\n    };\n    const credentials = options.credentials;\n    if (credentials) {\n        if (!(credentials.mechanism === defaultAuthProviders_1.AuthMechanism.MONGODB_DEFAULT) &&\n            !defaultAuthProviders_1.AUTH_PROVIDERS.get(credentials.mechanism)) {\n            callback(new error_1.MongoInvalidArgumentError(`AuthMechanism '${credentials.mechanism}' not supported`));\n            return;\n        }\n    }\n    const authContext = new auth_provider_1.AuthContext(conn, credentials, options);\n    prepareHandshakeDocument(authContext, (err, handshakeDoc) => {\n        if (err || !handshakeDoc) {\n            return callback(err);\n        }\n        const handshakeOptions = Object.assign({}, options);\n        if (typeof options.connectTimeoutMS === 'number') {\n            // The handshake technically is a monitoring check, so its socket timeout should be connectTimeoutMS\n            handshakeOptions.socketTimeoutMS = options.connectTimeoutMS;\n        }\n        const start = new Date().getTime();\n        conn.command((0, utils_1.ns)('admin.$cmd'), handshakeDoc, handshakeOptions, (err, response) => {\n            if (err) {\n                callback(err);\n                return;\n            }\n            if ((response === null || response === void 0 ? void 0 : response.ok) === 0) {\n                callback(new error_1.MongoServerError(response));\n                return;\n            }\n            if ('isWritablePrimary' in response) {\n                // Provide pre-hello-style response document.\n                response.ismaster = response.isWritablePrimary;\n            }\n            if (response.helloOk) {\n                conn.helloOk = true;\n            }\n            const supportedServerErr = checkSupportedServer(response, options);\n            if (supportedServerErr) {\n                callback(supportedServerErr);\n                return;\n            }\n            if (options.loadBalanced) {\n                // TODO: Durran: Remove when server support exists. (NODE-3431)\n                if (FAKE_MONGODB_SERVICE_ID) {\n                    response.serviceId = response.topologyVersion.processId;\n                }\n                if (!response.serviceId) {\n                    return callback(new error_1.MongoCompatibilityError('Driver attempted to initialize in load balancing mode, ' +\n                        'but the server does not support this mode.'));\n                }\n            }\n            // NOTE: This is metadata attached to the connection while porting away from\n            //       handshake being done in the `Server` class. Likely, it should be\n            //       relocated, or at very least restructured.\n            conn.ismaster = response;\n            conn.lastIsMasterMS = new Date().getTime() - start;\n            if (!response.arbiterOnly && credentials) {\n                // store the response on auth context\n                authContext.response = response;\n                const resolvedCredentials = credentials.resolveAuthMechanism(response);\n                const provider = defaultAuthProviders_1.AUTH_PROVIDERS.get(resolvedCredentials.mechanism);\n                if (!provider) {\n                    return callback(new error_1.MongoInvalidArgumentError(`No AuthProvider for ${resolvedCredentials.mechanism} defined.`));\n                }\n                provider.auth(authContext, err => {\n                    if (err)\n                        return callback(err);\n                    callback(undefined, conn);\n                });\n                return;\n            }\n            callback(undefined, conn);\n        });\n    });\n}\nfunction prepareHandshakeDocument(authContext, callback) {\n    const options = authContext.options;\n    const compressors = options.compressors ? options.compressors : [];\n    const { serverApi } = authContext.connection;\n    const handshakeDoc = {\n        [(serverApi === null || serverApi === void 0 ? void 0 : serverApi.version) ? 'hello' : 'ismaster']: true,\n        helloOk: true,\n        client: options.metadata || (0, utils_1.makeClientMetadata)(options),\n        compression: compressors,\n        loadBalanced: options.loadBalanced\n    };\n    const credentials = authContext.credentials;\n    if (credentials) {\n        if (credentials.mechanism === defaultAuthProviders_1.AuthMechanism.MONGODB_DEFAULT && credentials.username) {\n            handshakeDoc.saslSupportedMechs = `${credentials.source}.${credentials.username}`;\n            const provider = defaultAuthProviders_1.AUTH_PROVIDERS.get(defaultAuthProviders_1.AuthMechanism.MONGODB_SCRAM_SHA256);\n            if (!provider) {\n                // This auth mechanism is always present.\n                return callback(new error_1.MongoInvalidArgumentError(`No AuthProvider for ${defaultAuthProviders_1.AuthMechanism.MONGODB_SCRAM_SHA256} defined.`));\n            }\n            return provider.prepare(handshakeDoc, authContext, callback);\n        }\n        const provider = defaultAuthProviders_1.AUTH_PROVIDERS.get(credentials.mechanism);\n        if (!provider) {\n            return callback(new error_1.MongoInvalidArgumentError(`No AuthProvider for ${credentials.mechanism} defined.`));\n        }\n        return provider.prepare(handshakeDoc, authContext, callback);\n    }\n    callback(undefined, handshakeDoc);\n}\n/** @public */\nexports.LEGAL_TLS_SOCKET_OPTIONS = [\n    'ALPNProtocols',\n    'ca',\n    'cert',\n    'checkServerIdentity',\n    'ciphers',\n    'crl',\n    'ecdhCurve',\n    'key',\n    'minDHSize',\n    'passphrase',\n    'pfx',\n    'rejectUnauthorized',\n    'secureContext',\n    'secureProtocol',\n    'servername',\n    'session'\n];\n/** @public */\nexports.LEGAL_TCP_SOCKET_OPTIONS = [\n    'family',\n    'hints',\n    'localAddress',\n    'localPort',\n    'lookup'\n];\nfunction parseConnectOptions(options) {\n    const hostAddress = options.hostAddress;\n    if (!hostAddress)\n        throw new error_1.MongoInvalidArgumentError('Option \"hostAddress\" is required');\n    const result = {};\n    for (const name of exports.LEGAL_TCP_SOCKET_OPTIONS) {\n        if (options[name] != null) {\n            result[name] = options[name];\n        }\n    }\n    if (typeof hostAddress.socketPath === 'string') {\n        result.path = hostAddress.socketPath;\n        return result;\n    }\n    else if (typeof hostAddress.host === 'string') {\n        result.host = hostAddress.host;\n        result.port = hostAddress.port;\n        return result;\n    }\n    else {\n        // This should never happen since we set up HostAddresses\n        // But if we don't throw here the socket could hang until timeout\n        // TODO(NODE-3483)\n        throw new error_1.MongoRuntimeError(`Unexpected HostAddress ${JSON.stringify(hostAddress)}`);\n    }\n}\nfunction parseSslOptions(options) {\n    const result = parseConnectOptions(options);\n    // Merge in valid SSL options\n    for (const name of exports.LEGAL_TLS_SOCKET_OPTIONS) {\n        if (options[name] != null) {\n            result[name] = options[name];\n        }\n    }\n    // Set default sni servername to be the same as host\n    if (result.servername == null && result.host && !net.isIP(result.host)) {\n        result.servername = result.host;\n    }\n    return result;\n}\nconst SOCKET_ERROR_EVENT_LIST = ['error', 'close', 'timeout', 'parseError'];\nconst SOCKET_ERROR_EVENTS = new Set(SOCKET_ERROR_EVENT_LIST);\nfunction makeConnection(options, _callback) {\n    var _a, _b, _c, _d, _e, _f, _g, _h, _j;\n    const useTLS = (_a = options.tls) !== null && _a !== void 0 ? _a : false;\n    const keepAlive = (_b = options.keepAlive) !== null && _b !== void 0 ? _b : true;\n    const socketTimeoutMS = (_d = (_c = options.socketTimeoutMS) !== null && _c !== void 0 ? _c : Reflect.get(options, 'socketTimeout')) !== null && _d !== void 0 ? _d : 0;\n    const noDelay = (_e = options.noDelay) !== null && _e !== void 0 ? _e : true;\n    const connectionTimeout = (_f = options.connectTimeoutMS) !== null && _f !== void 0 ? _f : 30000;\n    const rejectUnauthorized = (_g = options.rejectUnauthorized) !== null && _g !== void 0 ? _g : true;\n    const keepAliveInitialDelay = (_j = (((_h = options.keepAliveInitialDelay) !== null && _h !== void 0 ? _h : 120000) > socketTimeoutMS\n        ? Math.round(socketTimeoutMS / 2)\n        : options.keepAliveInitialDelay)) !== null && _j !== void 0 ? _j : 120000;\n    let socket;\n    const callback = function (err, ret) {\n        if (err && socket) {\n            socket.destroy();\n        }\n        _callback(err, ret);\n    };\n    if (useTLS) {\n        const tlsSocket = tls.connect(parseSslOptions(options));\n        if (typeof tlsSocket.disableRenegotiation === 'function') {\n            tlsSocket.disableRenegotiation();\n        }\n        socket = tlsSocket;\n    }\n    else {\n        socket = net.createConnection(parseConnectOptions(options));\n    }\n    socket.setKeepAlive(keepAlive, keepAliveInitialDelay);\n    socket.setTimeout(connectionTimeout);\n    socket.setNoDelay(noDelay);\n    const connectEvent = useTLS ? 'secureConnect' : 'connect';\n    let cancellationHandler;\n    function errorHandler(eventName) {\n        return (err) => {\n            SOCKET_ERROR_EVENTS.forEach(event => socket.removeAllListeners(event));\n            if (cancellationHandler && options.cancellationToken) {\n                options.cancellationToken.removeListener('cancel', cancellationHandler);\n            }\n            socket.removeListener(connectEvent, connectHandler);\n            callback(connectionFailureError(eventName, err));\n        };\n    }\n    function connectHandler() {\n        SOCKET_ERROR_EVENTS.forEach(event => socket.removeAllListeners(event));\n        if (cancellationHandler && options.cancellationToken) {\n            options.cancellationToken.removeListener('cancel', cancellationHandler);\n        }\n        if ('authorizationError' in socket) {\n            if (socket.authorizationError && rejectUnauthorized) {\n                return callback(socket.authorizationError);\n            }\n        }\n        socket.setTimeout(socketTimeoutMS);\n        callback(undefined, socket);\n    }\n    SOCKET_ERROR_EVENTS.forEach(event => socket.once(event, errorHandler(event)));\n    if (options.cancellationToken) {\n        cancellationHandler = errorHandler('cancel');\n        options.cancellationToken.once('cancel', cancellationHandler);\n    }\n    socket.once(connectEvent, connectHandler);\n}\nfunction connectionFailureError(type, err) {\n    switch (type) {\n        case 'error':\n            return new error_1.MongoNetworkError(err);\n        case 'timeout':\n            return new error_1.MongoNetworkTimeoutError('connection timed out');\n        case 'close':\n            return new error_1.MongoNetworkError('connection closed');\n        case 'cancel':\n            return new error_1.MongoNetworkError('connection establishment was cancelled');\n        default:\n            return new error_1.MongoNetworkError('unknown network error');\n    }\n}\n//# sourceMappingURL=connect.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.AUTH_PROVIDERS = exports.AuthMechanism = void 0;\nconst mongocr_1 = require(\"./mongocr\");\nconst x509_1 = require(\"./x509\");\nconst plain_1 = require(\"./plain\");\nconst gssapi_1 = require(\"./gssapi\");\nconst scram_1 = require(\"./scram\");\nconst mongodb_aws_1 = require(\"./mongodb_aws\");\n/** @public */\nexports.AuthMechanism = Object.freeze({\n    MONGODB_AWS: 'MONGODB-AWS',\n    MONGODB_CR: 'MONGODB-CR',\n    MONGODB_DEFAULT: 'DEFAULT',\n    MONGODB_GSSAPI: 'GSSAPI',\n    MONGODB_PLAIN: 'PLAIN',\n    MONGODB_SCRAM_SHA1: 'SCRAM-SHA-1',\n    MONGODB_SCRAM_SHA256: 'SCRAM-SHA-256',\n    MONGODB_X509: 'MONGODB-X509'\n});\nexports.AUTH_PROVIDERS = new Map([\n    [exports.AuthMechanism.MONGODB_AWS, new mongodb_aws_1.MongoDBAWS()],\n    [exports.AuthMechanism.MONGODB_CR, new mongocr_1.MongoCR()],\n    [exports.AuthMechanism.MONGODB_GSSAPI, new gssapi_1.GSSAPI()],\n    [exports.AuthMechanism.MONGODB_PLAIN, new plain_1.Plain()],\n    [exports.AuthMechanism.MONGODB_SCRAM_SHA1, new scram_1.ScramSHA1()],\n    [exports.AuthMechanism.MONGODB_SCRAM_SHA256, new scram_1.ScramSHA256()],\n    [exports.AuthMechanism.MONGODB_X509, new x509_1.X509()]\n]);\n//# sourceMappingURL=defaultAuthProviders.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.MongoCR = void 0;\nconst crypto = require(\"crypto\");\nconst auth_provider_1 = require(\"./auth_provider\");\nconst utils_1 = require(\"../../utils\");\nconst error_1 = require(\"../../error\");\nclass MongoCR extends auth_provider_1.AuthProvider {\n    auth(authContext, callback) {\n        const { connection, credentials } = authContext;\n        if (!credentials) {\n            return callback(new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.'));\n        }\n        const username = credentials.username;\n        const password = credentials.password;\n        const source = credentials.source;\n        connection.command((0, utils_1.ns)(`${source}.$cmd`), { getnonce: 1 }, undefined, (err, r) => {\n            let nonce = null;\n            let key = null;\n            // Get nonce\n            if (err == null) {\n                nonce = r.nonce;\n                // Use node md5 generator\n                let md5 = crypto.createHash('md5');\n                // Generate keys used for authentication\n                md5.update(`${username}:mongo:${password}`, 'utf8');\n                const hash_password = md5.digest('hex');\n                // Final key\n                md5 = crypto.createHash('md5');\n                md5.update(nonce + username + hash_password, 'utf8');\n                key = md5.digest('hex');\n            }\n            const authenticateCommand = {\n                authenticate: 1,\n                user: username,\n                nonce,\n                key\n            };\n            connection.command((0, utils_1.ns)(`${source}.$cmd`), authenticateCommand, undefined, callback);\n        });\n    }\n}\nexports.MongoCR = MongoCR;\n//# sourceMappingURL=mongocr.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.AuthProvider = exports.AuthContext = void 0;\nconst error_1 = require(\"../../error\");\n/** Context used during authentication */\nclass AuthContext {\n    constructor(connection, credentials, options) {\n        this.connection = connection;\n        this.credentials = credentials;\n        this.options = options;\n    }\n}\nexports.AuthContext = AuthContext;\nclass AuthProvider {\n    /**\n     * Prepare the handshake document before the initial handshake.\n     *\n     * @param handshakeDoc - The document used for the initial handshake on a connection\n     * @param authContext - Context for authentication flow\n     */\n    prepare(handshakeDoc, authContext, callback) {\n        callback(undefined, handshakeDoc);\n    }\n    /**\n     * Authenticate\n     *\n     * @param context - A shared context for authentication flow\n     * @param callback - The callback to return the result from the authentication\n     */\n    auth(context, callback) {\n        // TODO(NODE-3483): Replace this with MongoMethodOverrideError\n        callback(new error_1.MongoRuntimeError('`auth` method must be overridden by subclass'));\n    }\n}\nexports.AuthProvider = AuthProvider;\n//# sourceMappingURL=auth_provider.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.X509 = void 0;\nconst auth_provider_1 = require(\"./auth_provider\");\nconst error_1 = require(\"../../error\");\nconst utils_1 = require(\"../../utils\");\nclass X509 extends auth_provider_1.AuthProvider {\n    prepare(handshakeDoc, authContext, callback) {\n        const { credentials } = authContext;\n        if (!credentials) {\n            return callback(new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.'));\n        }\n        Object.assign(handshakeDoc, {\n            speculativeAuthenticate: x509AuthenticateCommand(credentials)\n        });\n        callback(undefined, handshakeDoc);\n    }\n    auth(authContext, callback) {\n        const connection = authContext.connection;\n        const credentials = authContext.credentials;\n        if (!credentials) {\n            return callback(new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.'));\n        }\n        const response = authContext.response;\n        if (response && response.speculativeAuthenticate) {\n            return callback();\n        }\n        connection.command((0, utils_1.ns)('$external.$cmd'), x509AuthenticateCommand(credentials), undefined, callback);\n    }\n}\nexports.X509 = X509;\nfunction x509AuthenticateCommand(credentials) {\n    const command = { authenticate: 1, mechanism: 'MONGODB-X509' };\n    if (credentials.username) {\n        command.user = credentials.username;\n    }\n    return command;\n}\n//# sourceMappingURL=x509.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.Plain = void 0;\nconst bson_1 = require(\"../../bson\");\nconst auth_provider_1 = require(\"./auth_provider\");\nconst error_1 = require(\"../../error\");\nconst utils_1 = require(\"../../utils\");\nclass Plain extends auth_provider_1.AuthProvider {\n    auth(authContext, callback) {\n        const { connection, credentials } = authContext;\n        if (!credentials) {\n            return callback(new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.'));\n        }\n        const username = credentials.username;\n        const password = credentials.password;\n        const payload = new bson_1.Binary(Buffer.from(`\\x00${username}\\x00${password}`));\n        const command = {\n            saslStart: 1,\n            mechanism: 'PLAIN',\n            payload: payload,\n            autoAuthorize: 1\n        };\n        connection.command((0, utils_1.ns)('$external.$cmd'), command, undefined, callback);\n    }\n}\nexports.Plain = Plain;\n//# sourceMappingURL=plain.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.GSSAPI = void 0;\nconst auth_provider_1 = require(\"./auth_provider\");\nconst error_1 = require(\"../../error\");\nconst deps_1 = require(\"../../deps\");\nconst utils_1 = require(\"../../utils\");\nconst dns = require(\"dns\");\nclass GSSAPI extends auth_provider_1.AuthProvider {\n    auth(authContext, callback) {\n        const { connection, credentials } = authContext;\n        if (credentials == null)\n            return callback(new error_1.MongoMissingCredentialsError('Credentials required for GSSAPI authentication'));\n        const { username } = credentials;\n        function externalCommand(command, cb) {\n            return connection.command((0, utils_1.ns)('$external.$cmd'), command, undefined, cb);\n        }\n        makeKerberosClient(authContext, (err, client) => {\n            if (err)\n                return callback(err);\n            if (client == null)\n                return callback(new error_1.MongoMissingDependencyError('GSSAPI client missing'));\n            client.step('', (err, payload) => {\n                if (err)\n                    return callback(err);\n                externalCommand(saslStart(payload), (err, result) => {\n                    if (err)\n                        return callback(err);\n                    if (result == null)\n                        return callback();\n                    negotiate(client, 10, result.payload, (err, payload) => {\n                        if (err)\n                            return callback(err);\n                        externalCommand(saslContinue(payload, result.conversationId), (err, result) => {\n                            if (err)\n                                return callback(err);\n                            if (result == null)\n                                return callback();\n                            finalize(client, username, result.payload, (err, payload) => {\n                                if (err)\n                                    return callback(err);\n                                externalCommand({\n                                    saslContinue: 1,\n                                    conversationId: result.conversationId,\n                                    payload\n                                }, (err, result) => {\n                                    if (err)\n                                        return callback(err);\n                                    callback(undefined, result);\n                                });\n                            });\n                        });\n                    });\n                });\n            });\n        });\n    }\n}\nexports.GSSAPI = GSSAPI;\nfunction makeKerberosClient(authContext, callback) {\n    var _a;\n    const { hostAddress } = authContext.options;\n    const { credentials } = authContext;\n    if (!hostAddress || typeof hostAddress.host !== 'string' || !credentials) {\n        return callback(new error_1.MongoInvalidArgumentError('Connection must have host and port and credentials defined.'));\n    }\n    if ('kModuleError' in deps_1.Kerberos) {\n        return callback(deps_1.Kerberos['kModuleError']);\n    }\n    const { initializeClient } = deps_1.Kerberos;\n    const { username, password } = credentials;\n    const mechanismProperties = credentials.mechanismProperties;\n    const serviceName = (_a = mechanismProperties.SERVICE_NAME) !== null && _a !== void 0 ? _a : 'mongodb';\n    performGssapiCanonicalizeHostName(hostAddress.host, mechanismProperties, (err, host) => {\n        if (err)\n            return callback(err);\n        const initOptions = {};\n        if (password != null) {\n            Object.assign(initOptions, { user: username, password: password });\n        }\n        let spn = `${serviceName}${process.platform === 'win32' ? '/' : '@'}${host}`;\n        if ('SERVICE_REALM' in mechanismProperties) {\n            spn = `${spn}@${mechanismProperties.SERVICE_REALM}`;\n        }\n        initializeClient(spn, initOptions, (err, client) => {\n            // TODO(NODE-3483)\n            if (err)\n                return callback(new error_1.MongoRuntimeError(err));\n            callback(undefined, client);\n        });\n    });\n}\nfunction saslStart(payload) {\n    return {\n        saslStart: 1,\n        mechanism: 'GSSAPI',\n        payload,\n        autoAuthorize: 1\n    };\n}\nfunction saslContinue(payload, conversationId) {\n    return {\n        saslContinue: 1,\n        conversationId,\n        payload\n    };\n}\nfunction negotiate(client, retries, payload, callback) {\n    client.step(payload, (err, response) => {\n        // Retries exhausted, raise error\n        if (err && retries === 0)\n            return callback(err);\n        // Adjust number of retries and call step again\n        if (err)\n            return negotiate(client, retries - 1, payload, callback);\n        // Return the payload\n        callback(undefined, response || '');\n    });\n}\nfunction finalize(client, user, payload, callback) {\n    // GSS Client Unwrap\n    client.unwrap(payload, (err, response) => {\n        if (err)\n            return callback(err);\n        // Wrap the response\n        client.wrap(response || '', { user }, (err, wrapped) => {\n            if (err)\n                return callback(err);\n            // Return the payload\n            callback(undefined, wrapped);\n        });\n    });\n}\nfunction performGssapiCanonicalizeHostName(host, mechanismProperties, callback) {\n    if (!mechanismProperties.gssapiCanonicalizeHostName)\n        return callback(undefined, host);\n    // Attempt to resolve the host name\n    dns.resolveCname(host, (err, r) => {\n        if (err)\n            return callback(err);\n        // Get the first resolve host id\n        if (Array.isArray(r) && r.length > 0) {\n            return callback(undefined, r[0]);\n        }\n        callback(undefined, host);\n    });\n}\n//# sourceMappingURL=gssapi.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.ScramSHA256 = exports.ScramSHA1 = void 0;\nconst crypto = require(\"crypto\");\nconst bson_1 = require(\"../../bson\");\nconst error_1 = require(\"../../error\");\nconst auth_provider_1 = require(\"./auth_provider\");\nconst utils_1 = require(\"../../utils\");\nconst deps_1 = require(\"../../deps\");\nconst defaultAuthProviders_1 = require(\"./defaultAuthProviders\");\nclass ScramSHA extends auth_provider_1.AuthProvider {\n    constructor(cryptoMethod) {\n        super();\n        this.cryptoMethod = cryptoMethod || 'sha1';\n    }\n    prepare(handshakeDoc, authContext, callback) {\n        const cryptoMethod = this.cryptoMethod;\n        const credentials = authContext.credentials;\n        if (!credentials) {\n            return callback(new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.'));\n        }\n        if (cryptoMethod === 'sha256' && deps_1.saslprep == null) {\n            (0, utils_1.emitWarning)('Warning: no saslprep library specified. Passwords will not be sanitized');\n        }\n        crypto.randomBytes(24, (err, nonce) => {\n            if (err) {\n                return callback(err);\n            }\n            // store the nonce for later use\n            Object.assign(authContext, { nonce });\n            const request = Object.assign({}, handshakeDoc, {\n                speculativeAuthenticate: Object.assign(makeFirstMessage(cryptoMethod, credentials, nonce), {\n                    db: credentials.source\n                })\n            });\n            callback(undefined, request);\n        });\n    }\n    auth(authContext, callback) {\n        const response = authContext.response;\n        if (response && response.speculativeAuthenticate) {\n            continueScramConversation(this.cryptoMethod, response.speculativeAuthenticate, authContext, callback);\n            return;\n        }\n        executeScram(this.cryptoMethod, authContext, callback);\n    }\n}\nfunction cleanUsername(username) {\n    return username.replace('=', '=3D').replace(',', '=2C');\n}\nfunction clientFirstMessageBare(username, nonce) {\n    // NOTE: This is done b/c Javascript uses UTF-16, but the server is hashing in UTF-8.\n    // Since the username is not sasl-prep-d, we need to do this here.\n    return Buffer.concat([\n        Buffer.from('n=', 'utf8'),\n        Buffer.from(username, 'utf8'),\n        Buffer.from(',r=', 'utf8'),\n        Buffer.from(nonce.toString('base64'), 'utf8')\n    ]);\n}\nfunction makeFirstMessage(cryptoMethod, credentials, nonce) {\n    const username = cleanUsername(credentials.username);\n    const mechanism = cryptoMethod === 'sha1' ? defaultAuthProviders_1.AuthMechanism.MONGODB_SCRAM_SHA1 : defaultAuthProviders_1.AuthMechanism.MONGODB_SCRAM_SHA256;\n    // NOTE: This is done b/c Javascript uses UTF-16, but the server is hashing in UTF-8.\n    // Since the username is not sasl-prep-d, we need to do this here.\n    return {\n        saslStart: 1,\n        mechanism,\n        payload: new bson_1.Binary(Buffer.concat([Buffer.from('n,,', 'utf8'), clientFirstMessageBare(username, nonce)])),\n        autoAuthorize: 1,\n        options: { skipEmptyExchange: true }\n    };\n}\nfunction executeScram(cryptoMethod, authContext, callback) {\n    const { connection, credentials } = authContext;\n    if (!credentials) {\n        return callback(new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.'));\n    }\n    if (!authContext.nonce) {\n        return callback(new error_1.MongoInvalidArgumentError('AuthContext must contain a valid nonce property'));\n    }\n    const nonce = authContext.nonce;\n    const db = credentials.source;\n    const saslStartCmd = makeFirstMessage(cryptoMethod, credentials, nonce);\n    connection.command((0, utils_1.ns)(`${db}.$cmd`), saslStartCmd, undefined, (_err, result) => {\n        const err = resolveError(_err, result);\n        if (err) {\n            return callback(err);\n        }\n        continueScramConversation(cryptoMethod, result, authContext, callback);\n    });\n}\nfunction continueScramConversation(cryptoMethod, response, authContext, callback) {\n    const connection = authContext.connection;\n    const credentials = authContext.credentials;\n    if (!credentials) {\n        return callback(new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.'));\n    }\n    if (!authContext.nonce) {\n        return callback(new error_1.MongoInvalidArgumentError('Unable to continue SCRAM without valid nonce'));\n    }\n    const nonce = authContext.nonce;\n    const db = credentials.source;\n    const username = cleanUsername(credentials.username);\n    const password = credentials.password;\n    let processedPassword;\n    if (cryptoMethod === 'sha256') {\n        processedPassword = 'kModuleError' in deps_1.saslprep ? password : (0, deps_1.saslprep)(password);\n    }\n    else {\n        try {\n            processedPassword = passwordDigest(username, password);\n        }\n        catch (e) {\n            return callback(e);\n        }\n    }\n    const payload = Buffer.isBuffer(response.payload)\n        ? new bson_1.Binary(response.payload)\n        : response.payload;\n    const dict = parsePayload(payload.value());\n    const iterations = parseInt(dict.i, 10);\n    if (iterations && iterations < 4096) {\n        callback(\n        // TODO(NODE-3483)\n        new error_1.MongoRuntimeError(`Server returned an invalid iteration count ${iterations}`), false);\n        return;\n    }\n    const salt = dict.s;\n    const rnonce = dict.r;\n    if (rnonce.startsWith('nonce')) {\n        // TODO(NODE-3483)\n        callback(new error_1.MongoRuntimeError(`Server returned an invalid nonce: ${rnonce}`), false);\n        return;\n    }\n    // Set up start of proof\n    const withoutProof = `c=biws,r=${rnonce}`;\n    const saltedPassword = HI(processedPassword, Buffer.from(salt, 'base64'), iterations, cryptoMethod);\n    const clientKey = HMAC(cryptoMethod, saltedPassword, 'Client Key');\n    const serverKey = HMAC(cryptoMethod, saltedPassword, 'Server Key');\n    const storedKey = H(cryptoMethod, clientKey);\n    const authMessage = [clientFirstMessageBare(username, nonce), payload.value(), withoutProof].join(',');\n    const clientSignature = HMAC(cryptoMethod, storedKey, authMessage);\n    const clientProof = `p=${xor(clientKey, clientSignature)}`;\n    const clientFinal = [withoutProof, clientProof].join(',');\n    const serverSignature = HMAC(cryptoMethod, serverKey, authMessage);\n    const saslContinueCmd = {\n        saslContinue: 1,\n        conversationId: response.conversationId,\n        payload: new bson_1.Binary(Buffer.from(clientFinal))\n    };\n    connection.command((0, utils_1.ns)(`${db}.$cmd`), saslContinueCmd, undefined, (_err, r) => {\n        const err = resolveError(_err, r);\n        if (err) {\n            return callback(err);\n        }\n        const parsedResponse = parsePayload(r.payload.value());\n        if (!compareDigest(Buffer.from(parsedResponse.v, 'base64'), serverSignature)) {\n            callback(new error_1.MongoRuntimeError('Server returned an invalid signature'));\n            return;\n        }\n        if (!r || r.done !== false) {\n            return callback(err, r);\n        }\n        const retrySaslContinueCmd = {\n            saslContinue: 1,\n            conversationId: r.conversationId,\n            payload: Buffer.alloc(0)\n        };\n        connection.command((0, utils_1.ns)(`${db}.$cmd`), retrySaslContinueCmd, undefined, callback);\n    });\n}\nfunction parsePayload(payload) {\n    const dict = {};\n    const parts = payload.split(',');\n    for (let i = 0; i < parts.length; i++) {\n        const valueParts = parts[i].split('=');\n        dict[valueParts[0]] = valueParts[1];\n    }\n    return dict;\n}\nfunction passwordDigest(username, password) {\n    if (typeof username !== 'string') {\n        throw new error_1.MongoInvalidArgumentError('Username must be a string');\n    }\n    if (typeof password !== 'string') {\n        throw new error_1.MongoInvalidArgumentError('Password must be a string');\n    }\n    if (password.length === 0) {\n        throw new error_1.MongoInvalidArgumentError('Password cannot be empty');\n    }\n    const md5 = crypto.createHash('md5');\n    md5.update(`${username}:mongo:${password}`, 'utf8');\n    return md5.digest('hex');\n}\n// XOR two buffers\nfunction xor(a, b) {\n    if (!Buffer.isBuffer(a)) {\n        a = Buffer.from(a);\n    }\n    if (!Buffer.isBuffer(b)) {\n        b = Buffer.from(b);\n    }\n    const length = Math.max(a.length, b.length);\n    const res = [];\n    for (let i = 0; i < length; i += 1) {\n        res.push(a[i] ^ b[i]);\n    }\n    return Buffer.from(res).toString('base64');\n}\nfunction H(method, text) {\n    return crypto.createHash(method).update(text).digest();\n}\nfunction HMAC(method, key, text) {\n    return crypto.createHmac(method, key).update(text).digest();\n}\nlet _hiCache = {};\nlet _hiCacheCount = 0;\nfunction _hiCachePurge() {\n    _hiCache = {};\n    _hiCacheCount = 0;\n}\nconst hiLengthMap = {\n    sha256: 32,\n    sha1: 20\n};\nfunction HI(data, salt, iterations, cryptoMethod) {\n    // omit the work if already generated\n    const key = [data, salt.toString('base64'), iterations].join('_');\n    if (_hiCache[key] != null) {\n        return _hiCache[key];\n    }\n    // generate the salt\n    const saltedData = crypto.pbkdf2Sync(data, salt, iterations, hiLengthMap[cryptoMethod], cryptoMethod);\n    // cache a copy to speed up the next lookup, but prevent unbounded cache growth\n    if (_hiCacheCount >= 200) {\n        _hiCachePurge();\n    }\n    _hiCache[key] = saltedData;\n    _hiCacheCount += 1;\n    return saltedData;\n}\nfunction compareDigest(lhs, rhs) {\n    if (lhs.length !== rhs.length) {\n        return false;\n    }\n    if (typeof crypto.timingSafeEqual === 'function') {\n        return crypto.timingSafeEqual(lhs, rhs);\n    }\n    let result = 0;\n    for (let i = 0; i < lhs.length; i++) {\n        result |= lhs[i] ^ rhs[i];\n    }\n    return result === 0;\n}\nfunction resolveError(err, result) {\n    if (err)\n        return err;\n    if (result) {\n        if (result.$err || result.errmsg)\n            return new error_1.MongoServerError(result);\n    }\n}\nclass ScramSHA1 extends ScramSHA {\n    constructor() {\n        super('sha1');\n    }\n}\nexports.ScramSHA1 = ScramSHA1;\nclass ScramSHA256 extends ScramSHA {\n    constructor() {\n        super('sha256');\n    }\n}\nexports.ScramSHA256 = ScramSHA256;\n//# sourceMappingURL=scram.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.MongoDBAWS = void 0;\nconst http = require(\"http\");\nconst crypto = require(\"crypto\");\nconst url = require(\"url\");\nconst BSON = require(\"../../bson\");\nconst auth_provider_1 = require(\"./auth_provider\");\nconst mongo_credentials_1 = require(\"./mongo_credentials\");\nconst error_1 = require(\"../../error\");\nconst utils_1 = require(\"../../utils\");\nconst deps_1 = require(\"../../deps\");\nconst defaultAuthProviders_1 = require(\"./defaultAuthProviders\");\nconst ASCII_N = 110;\nconst AWS_RELATIVE_URI = 'http://169.254.170.2';\nconst AWS_EC2_URI = 'http://169.254.169.254';\nconst AWS_EC2_PATH = '/latest/meta-data/iam/security-credentials';\nconst bsonOptions = {\n    promoteLongs: true,\n    promoteValues: true,\n    promoteBuffers: false,\n    bsonRegExp: false\n};\nclass MongoDBAWS extends auth_provider_1.AuthProvider {\n    auth(authContext, callback) {\n        const { connection, credentials } = authContext;\n        if (!credentials) {\n            return callback(new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.'));\n        }\n        if ('kModuleError' in deps_1.aws4) {\n            return callback(deps_1.aws4['kModuleError']);\n        }\n        const { sign } = deps_1.aws4;\n        if ((0, utils_1.maxWireVersion)(connection) < 9) {\n            callback(new error_1.MongoCompatibilityError('MONGODB-AWS authentication requires MongoDB version 4.4 or later'));\n            return;\n        }\n        if (!credentials.username) {\n            makeTempCredentials(credentials, (err, tempCredentials) => {\n                if (err || !tempCredentials)\n                    return callback(err);\n                authContext.credentials = tempCredentials;\n                this.auth(authContext, callback);\n            });\n            return;\n        }\n        const accessKeyId = credentials.username;\n        const secretAccessKey = credentials.password;\n        const sessionToken = credentials.mechanismProperties.AWS_SESSION_TOKEN;\n        // If all three defined, include sessionToken, else include username and pass, else no credentials\n        const awsCredentials = accessKeyId && secretAccessKey && sessionToken\n            ? { accessKeyId, secretAccessKey, sessionToken }\n            : accessKeyId && secretAccessKey\n                ? { accessKeyId, secretAccessKey }\n                : undefined;\n        const db = credentials.source;\n        crypto.randomBytes(32, (err, nonce) => {\n            if (err) {\n                callback(err);\n                return;\n            }\n            const saslStart = {\n                saslStart: 1,\n                mechanism: 'MONGODB-AWS',\n                payload: BSON.serialize({ r: nonce, p: ASCII_N }, bsonOptions)\n            };\n            connection.command((0, utils_1.ns)(`${db}.$cmd`), saslStart, undefined, (err, res) => {\n                if (err)\n                    return callback(err);\n                const serverResponse = BSON.deserialize(res.payload.buffer, bsonOptions);\n                const host = serverResponse.h;\n                const serverNonce = serverResponse.s.buffer;\n                if (serverNonce.length !== 64) {\n                    callback(\n                    // TODO(NODE-3483)\n                    new error_1.MongoRuntimeError(`Invalid server nonce length ${serverNonce.length}, expected 64`));\n                    return;\n                }\n                if (serverNonce.compare(nonce, 0, nonce.length, 0, nonce.length) !== 0) {\n                    // TODO(NODE-3483)\n                    callback(new error_1.MongoRuntimeError('Server nonce does not begin with client nonce'));\n                    return;\n                }\n                if (host.length < 1 || host.length > 255 || host.indexOf('..') !== -1) {\n                    // TODO(NODE-3483)\n                    callback(new error_1.MongoRuntimeError(`Server returned an invalid host: \"${host}\"`));\n                    return;\n                }\n                const body = 'Action=GetCallerIdentity&Version=2011-06-15';\n                const options = sign({\n                    method: 'POST',\n                    host,\n                    region: deriveRegion(serverResponse.h),\n                    service: 'sts',\n                    headers: {\n                        'Content-Type': 'application/x-www-form-urlencoded',\n                        'Content-Length': body.length,\n                        'X-MongoDB-Server-Nonce': serverNonce.toString('base64'),\n                        'X-MongoDB-GS2-CB-Flag': 'n'\n                    },\n                    path: '/',\n                    body\n                }, awsCredentials);\n                const payload = {\n                    a: options.headers.Authorization,\n                    d: options.headers['X-Amz-Date']\n                };\n                if (sessionToken) {\n                    payload.t = sessionToken;\n                }\n                const saslContinue = {\n                    saslContinue: 1,\n                    conversationId: 1,\n                    payload: BSON.serialize(payload, bsonOptions)\n                };\n                connection.command((0, utils_1.ns)(`${db}.$cmd`), saslContinue, undefined, callback);\n            });\n        });\n    }\n}\nexports.MongoDBAWS = MongoDBAWS;\nfunction makeTempCredentials(credentials, callback) {\n    function done(creds) {\n        if (!creds.AccessKeyId || !creds.SecretAccessKey || !creds.Token) {\n            callback(new error_1.MongoMissingCredentialsError('Could not obtain temporary MONGODB-AWS credentials'));\n            return;\n        }\n        callback(undefined, new mongo_credentials_1.MongoCredentials({\n            username: creds.AccessKeyId,\n            password: creds.SecretAccessKey,\n            source: credentials.source,\n            mechanism: defaultAuthProviders_1.AuthMechanism.MONGODB_AWS,\n            mechanismProperties: {\n                AWS_SESSION_TOKEN: creds.Token\n            }\n        }));\n    }\n    // If the environment variable AWS_CONTAINER_CREDENTIALS_RELATIVE_URI\n    // is set then drivers MUST assume that it was set by an AWS ECS agent\n    if (process.env.AWS_CONTAINER_CREDENTIALS_RELATIVE_URI) {\n        request(`${AWS_RELATIVE_URI}${process.env.AWS_CONTAINER_CREDENTIALS_RELATIVE_URI}`, undefined, (err, res) => {\n            if (err)\n                return callback(err);\n            done(res);\n        });\n        return;\n    }\n    // Otherwise assume we are on an EC2 instance\n    // get a token\n    request(`${AWS_EC2_URI}/latest/api/token`, { method: 'PUT', json: false, headers: { 'X-aws-ec2-metadata-token-ttl-seconds': 30 } }, (err, token) => {\n        if (err)\n            return callback(err);\n        // get role name\n        request(`${AWS_EC2_URI}/${AWS_EC2_PATH}`, { json: false, headers: { 'X-aws-ec2-metadata-token': token } }, (err, roleName) => {\n            if (err)\n                return callback(err);\n            // get temp credentials\n            request(`${AWS_EC2_URI}/${AWS_EC2_PATH}/${roleName}`, { headers: { 'X-aws-ec2-metadata-token': token } }, (err, creds) => {\n                if (err)\n                    return callback(err);\n                done(creds);\n            });\n        });\n    });\n}\nfunction deriveRegion(host) {\n    const parts = host.split('.');\n    if (parts.length === 1 || parts[1] === 'amazonaws') {\n        return 'us-east-1';\n    }\n    return parts[1];\n}\nfunction request(uri, _options, callback) {\n    const options = Object.assign({\n        method: 'GET',\n        timeout: 10000,\n        json: true\n    }, url.parse(uri), _options);\n    const req = http.request(options, res => {\n        res.setEncoding('utf8');\n        let data = '';\n        res.on('data', d => (data += d));\n        res.on('end', () => {\n            if (options.json === false) {\n                callback(undefined, data);\n                return;\n            }\n            try {\n                const parsed = JSON.parse(data);\n                callback(undefined, parsed);\n            }\n            catch (err) {\n                // TODO(NODE-3483)\n                callback(new error_1.MongoRuntimeError(`Invalid JSON response: \"${data}\"`));\n            }\n        });\n    });\n    req.on('error', err => callback(err));\n    req.end();\n}\n//# sourceMappingURL=mongodb_aws.js.map","\n// Resolves the default auth mechanism according to\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.MongoCredentials = void 0;\nconst error_1 = require(\"../../error\");\nconst defaultAuthProviders_1 = require(\"./defaultAuthProviders\");\n// https://github.com/mongodb/specifications/blob/master/source/auth/auth.rst\nfunction getDefaultAuthMechanism(ismaster) {\n    if (ismaster) {\n        // If ismaster contains saslSupportedMechs, use scram-sha-256\n        // if it is available, else scram-sha-1\n        if (Array.isArray(ismaster.saslSupportedMechs)) {\n            return ismaster.saslSupportedMechs.includes(defaultAuthProviders_1.AuthMechanism.MONGODB_SCRAM_SHA256)\n                ? defaultAuthProviders_1.AuthMechanism.MONGODB_SCRAM_SHA256\n                : defaultAuthProviders_1.AuthMechanism.MONGODB_SCRAM_SHA1;\n        }\n        // Fallback to legacy selection method. If wire version >= 3, use scram-sha-1\n        if (ismaster.maxWireVersion >= 3) {\n            return defaultAuthProviders_1.AuthMechanism.MONGODB_SCRAM_SHA1;\n        }\n    }\n    // Default for wireprotocol < 3\n    return defaultAuthProviders_1.AuthMechanism.MONGODB_CR;\n}\n/**\n * A representation of the credentials used by MongoDB\n * @public\n */\nclass MongoCredentials {\n    constructor(options) {\n        this.username = options.username;\n        this.password = options.password;\n        this.source = options.source;\n        if (!this.source && options.db) {\n            this.source = options.db;\n        }\n        this.mechanism = options.mechanism || defaultAuthProviders_1.AuthMechanism.MONGODB_DEFAULT;\n        this.mechanismProperties = options.mechanismProperties || {};\n        if (this.mechanism.match(/MONGODB-AWS/i)) {\n            if (!this.username && process.env.AWS_ACCESS_KEY_ID) {\n                this.username = process.env.AWS_ACCESS_KEY_ID;\n            }\n            if (!this.password && process.env.AWS_SECRET_ACCESS_KEY) {\n                this.password = process.env.AWS_SECRET_ACCESS_KEY;\n            }\n            if (this.mechanismProperties.AWS_SESSION_TOKEN == null &&\n                process.env.AWS_SESSION_TOKEN != null) {\n                this.mechanismProperties = {\n                    ...this.mechanismProperties,\n                    AWS_SESSION_TOKEN: process.env.AWS_SESSION_TOKEN\n                };\n            }\n        }\n        Object.freeze(this.mechanismProperties);\n        Object.freeze(this);\n    }\n    /** Determines if two MongoCredentials objects are equivalent */\n    equals(other) {\n        return (this.mechanism === other.mechanism &&\n            this.username === other.username &&\n            this.password === other.password &&\n            this.source === other.source);\n    }\n    /**\n     * If the authentication mechanism is set to \"default\", resolves the authMechanism\n     * based on the server version and server supported sasl mechanisms.\n     *\n     * @param ismaster - An ismaster response from the server\n     */\n    resolveAuthMechanism(ismaster) {\n        // If the mechanism is not \"default\", then it does not need to be resolved\n        if (this.mechanism.match(/DEFAULT/i)) {\n            return new MongoCredentials({\n                username: this.username,\n                password: this.password,\n                source: this.source,\n                mechanism: getDefaultAuthMechanism(ismaster),\n                mechanismProperties: this.mechanismProperties\n            });\n        }\n        return this;\n    }\n    validate() {\n        if ((this.mechanism === defaultAuthProviders_1.AuthMechanism.MONGODB_GSSAPI ||\n            this.mechanism === defaultAuthProviders_1.AuthMechanism.MONGODB_CR ||\n            this.mechanism === defaultAuthProviders_1.AuthMechanism.MONGODB_PLAIN ||\n            this.mechanism === defaultAuthProviders_1.AuthMechanism.MONGODB_SCRAM_SHA1 ||\n            this.mechanism === defaultAuthProviders_1.AuthMechanism.MONGODB_SCRAM_SHA256) &&\n            !this.username) {\n            throw new error_1.MongoMissingCredentialsError(`Username required for mechanism '${this.mechanism}'`);\n        }\n        if (this.mechanism === defaultAuthProviders_1.AuthMechanism.MONGODB_GSSAPI ||\n            this.mechanism === defaultAuthProviders_1.AuthMechanism.MONGODB_AWS ||\n            this.mechanism === defaultAuthProviders_1.AuthMechanism.MONGODB_X509) {\n            if (this.source != null && this.source !== '$external') {\n                // TODO(NODE-3485): Replace this with a MongoAuthValidationError\n                throw new error_1.MongoAPIError(`Invalid source '${this.source}' for mechanism '${this.mechanism}' specified.`);\n            }\n        }\n        if (this.mechanism === defaultAuthProviders_1.AuthMechanism.MONGODB_PLAIN && this.source == null) {\n            // TODO(NODE-3485): Replace this with a MongoAuthValidationError\n            throw new error_1.MongoAPIError('PLAIN Authentication Mechanism needs an auth source');\n        }\n        if (this.mechanism === defaultAuthProviders_1.AuthMechanism.MONGODB_X509 && this.password != null) {\n            if (this.password === '') {\n                Reflect.set(this, 'password', undefined);\n                return;\n            }\n            // TODO(NODE-3485): Replace this with a MongoAuthValidationError\n            throw new error_1.MongoAPIError(`Password not allowed for mechanism MONGODB-X509`);\n        }\n    }\n    static merge(creds, options) {\n        var _a, _b, _c, _d, _e, _f, _g, _h, _j, _k, _l;\n        return new MongoCredentials({\n            username: (_b = (_a = options.username) !== null && _a !== void 0 ? _a : creds === null || creds === void 0 ? void 0 : creds.username) !== null && _b !== void 0 ? _b : '',\n            password: (_d = (_c = options.password) !== null && _c !== void 0 ? _c : creds === null || creds === void 0 ? void 0 : creds.password) !== null && _d !== void 0 ? _d : '',\n            mechanism: (_f = (_e = options.mechanism) !== null && _e !== void 0 ? _e : creds === null || creds === void 0 ? void 0 : creds.mechanism) !== null && _f !== void 0 ? _f : defaultAuthProviders_1.AuthMechanism.MONGODB_DEFAULT,\n            mechanismProperties: (_h = (_g = options.mechanismProperties) !== null && _g !== void 0 ? _g : creds === null || creds === void 0 ? void 0 : creds.mechanismProperties) !== null && _h !== void 0 ? _h : {},\n            source: (_l = (_k = (_j = options.source) !== null && _j !== void 0 ? _j : options.db) !== null && _k !== void 0 ? _k : creds === null || creds === void 0 ? void 0 : creds.source) !== null && _l !== void 0 ? _l : 'admin'\n        });\n    }\n}\nexports.MongoCredentials = MongoCredentials;\n//# sourceMappingURL=mongo_credentials.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.WaitQueueTimeoutError = exports.PoolClosedError = void 0;\nconst error_1 = require(\"../error\");\n/**\n * An error indicating a connection pool is closed\n * @category Error\n */\nclass PoolClosedError extends error_1.MongoDriverError {\n    constructor(pool) {\n        super('Attempted to check out a connection from closed connection pool');\n        this.address = pool.address;\n    }\n    get name() {\n        return 'MongoPoolClosedError';\n    }\n}\nexports.PoolClosedError = PoolClosedError;\n/**\n * An error thrown when a request to check out a connection times out\n * @category Error\n */\nclass WaitQueueTimeoutError extends error_1.MongoDriverError {\n    constructor(message, address) {\n        super(message);\n        this.address = address;\n    }\n    get name() {\n        return 'MongoWaitQueueTimeoutError';\n    }\n}\nexports.WaitQueueTimeoutError = WaitQueueTimeoutError;\n//# sourceMappingURL=errors.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.ConnectionPoolClearedEvent = exports.ConnectionCheckedInEvent = exports.ConnectionCheckedOutEvent = exports.ConnectionCheckOutFailedEvent = exports.ConnectionCheckOutStartedEvent = exports.ConnectionClosedEvent = exports.ConnectionReadyEvent = exports.ConnectionCreatedEvent = exports.ConnectionPoolClosedEvent = exports.ConnectionPoolCreatedEvent = exports.ConnectionPoolMonitoringEvent = void 0;\n/**\n * The base export class for all monitoring events published from the connection pool\n * @public\n * @category Event\n */\nclass ConnectionPoolMonitoringEvent {\n    /** @internal */\n    constructor(pool) {\n        this.time = new Date();\n        this.address = pool.address;\n    }\n}\nexports.ConnectionPoolMonitoringEvent = ConnectionPoolMonitoringEvent;\n/**\n * An event published when a connection pool is created\n * @public\n * @category Event\n */\nclass ConnectionPoolCreatedEvent extends ConnectionPoolMonitoringEvent {\n    /** @internal */\n    constructor(pool) {\n        super(pool);\n        this.options = pool.options;\n    }\n}\nexports.ConnectionPoolCreatedEvent = ConnectionPoolCreatedEvent;\n/**\n * An event published when a connection pool is closed\n * @public\n * @category Event\n */\nclass ConnectionPoolClosedEvent extends ConnectionPoolMonitoringEvent {\n    /** @internal */\n    constructor(pool) {\n        super(pool);\n    }\n}\nexports.ConnectionPoolClosedEvent = ConnectionPoolClosedEvent;\n/**\n * An event published when a connection pool creates a new connection\n * @public\n * @category Event\n */\nclass ConnectionCreatedEvent extends ConnectionPoolMonitoringEvent {\n    /** @internal */\n    constructor(pool, connection) {\n        super(pool);\n        this.connectionId = connection.id;\n    }\n}\nexports.ConnectionCreatedEvent = ConnectionCreatedEvent;\n/**\n * An event published when a connection is ready for use\n * @public\n * @category Event\n */\nclass ConnectionReadyEvent extends ConnectionPoolMonitoringEvent {\n    /** @internal */\n    constructor(pool, connection) {\n        super(pool);\n        this.connectionId = connection.id;\n    }\n}\nexports.ConnectionReadyEvent = ConnectionReadyEvent;\n/**\n * An event published when a connection is closed\n * @public\n * @category Event\n */\nclass ConnectionClosedEvent extends ConnectionPoolMonitoringEvent {\n    /** @internal */\n    constructor(pool, connection, reason) {\n        super(pool);\n        this.connectionId = connection.id;\n        this.reason = reason || 'unknown';\n        this.serviceId = connection.serviceId;\n    }\n}\nexports.ConnectionClosedEvent = ConnectionClosedEvent;\n/**\n * An event published when a request to check a connection out begins\n * @public\n * @category Event\n */\nclass ConnectionCheckOutStartedEvent extends ConnectionPoolMonitoringEvent {\n    /** @internal */\n    constructor(pool) {\n        super(pool);\n    }\n}\nexports.ConnectionCheckOutStartedEvent = ConnectionCheckOutStartedEvent;\n/**\n * An event published when a request to check a connection out fails\n * @public\n * @category Event\n */\nclass ConnectionCheckOutFailedEvent extends ConnectionPoolMonitoringEvent {\n    /** @internal */\n    constructor(pool, reason) {\n        super(pool);\n        this.reason = reason;\n    }\n}\nexports.ConnectionCheckOutFailedEvent = ConnectionCheckOutFailedEvent;\n/**\n * An event published when a connection is checked out of the connection pool\n * @public\n * @category Event\n */\nclass ConnectionCheckedOutEvent extends ConnectionPoolMonitoringEvent {\n    /** @internal */\n    constructor(pool, connection) {\n        super(pool);\n        this.connectionId = connection.id;\n    }\n}\nexports.ConnectionCheckedOutEvent = ConnectionCheckedOutEvent;\n/**\n * An event published when a connection is checked into the connection pool\n * @public\n * @category Event\n */\nclass ConnectionCheckedInEvent extends ConnectionPoolMonitoringEvent {\n    /** @internal */\n    constructor(pool, connection) {\n        super(pool);\n        this.connectionId = connection.id;\n    }\n}\nexports.ConnectionCheckedInEvent = ConnectionCheckedInEvent;\n/**\n * An event published when a connection pool is cleared\n * @public\n * @category Event\n */\nclass ConnectionPoolClearedEvent extends ConnectionPoolMonitoringEvent {\n    /** @internal */\n    constructor(pool, serviceId) {\n        super(pool);\n        this.serviceId = serviceId;\n    }\n}\nexports.ConnectionPoolClearedEvent = ConnectionPoolClearedEvent;\n//# sourceMappingURL=connection_pool_events.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.RTTPinger = exports.Monitor = void 0;\nconst common_1 = require(\"./common\");\nconst utils_1 = require(\"../utils\");\nconst connect_1 = require(\"../cmap/connect\");\nconst connection_1 = require(\"../cmap/connection\");\nconst error_1 = require(\"../error\");\nconst bson_1 = require(\"../bson\");\nconst events_1 = require(\"./events\");\nconst server_1 = require(\"./server\");\nconst mongo_types_1 = require(\"../mongo_types\");\n/** @internal */\nconst kServer = Symbol('server');\n/** @internal */\nconst kMonitorId = Symbol('monitorId');\n/** @internal */\nconst kConnection = Symbol('connection');\n/** @internal */\nconst kCancellationToken = Symbol('cancellationToken');\n/** @internal */\nconst kRTTPinger = Symbol('rttPinger');\n/** @internal */\nconst kRoundTripTime = Symbol('roundTripTime');\nconst STATE_IDLE = 'idle';\nconst STATE_MONITORING = 'monitoring';\nconst stateTransition = (0, utils_1.makeStateMachine)({\n    [common_1.STATE_CLOSING]: [common_1.STATE_CLOSING, STATE_IDLE, common_1.STATE_CLOSED],\n    [common_1.STATE_CLOSED]: [common_1.STATE_CLOSED, STATE_MONITORING],\n    [STATE_IDLE]: [STATE_IDLE, STATE_MONITORING, common_1.STATE_CLOSING],\n    [STATE_MONITORING]: [STATE_MONITORING, STATE_IDLE, common_1.STATE_CLOSING]\n});\nconst INVALID_REQUEST_CHECK_STATES = new Set([common_1.STATE_CLOSING, common_1.STATE_CLOSED, STATE_MONITORING]);\nfunction isInCloseState(monitor) {\n    return monitor.s.state === common_1.STATE_CLOSED || monitor.s.state === common_1.STATE_CLOSING;\n}\n/** @internal */\nclass Monitor extends mongo_types_1.TypedEventEmitter {\n    constructor(server, options) {\n        var _a, _b, _c;\n        super();\n        this[kServer] = server;\n        this[kConnection] = undefined;\n        this[kCancellationToken] = new mongo_types_1.CancellationToken();\n        this[kCancellationToken].setMaxListeners(Infinity);\n        this[kMonitorId] = undefined;\n        this.s = {\n            state: common_1.STATE_CLOSED\n        };\n        this.address = server.description.address;\n        this.options = Object.freeze({\n            connectTimeoutMS: (_a = options.connectTimeoutMS) !== null && _a !== void 0 ? _a : 10000,\n            heartbeatFrequencyMS: (_b = options.heartbeatFrequencyMS) !== null && _b !== void 0 ? _b : 10000,\n            minHeartbeatFrequencyMS: (_c = options.minHeartbeatFrequencyMS) !== null && _c !== void 0 ? _c : 500\n        });\n        const cancellationToken = this[kCancellationToken];\n        // TODO: refactor this to pull it directly from the pool, requires new ConnectionPool integration\n        const connectOptions = Object.assign({\n            id: '<monitor>',\n            generation: server.s.pool.generation,\n            connectionType: connection_1.Connection,\n            cancellationToken,\n            hostAddress: server.description.hostAddress\n        }, options, \n        // force BSON serialization options\n        {\n            raw: false,\n            promoteLongs: true,\n            promoteValues: true,\n            promoteBuffers: true\n        });\n        // ensure no authentication is used for monitoring\n        delete connectOptions.credentials;\n        if (connectOptions.autoEncrypter) {\n            delete connectOptions.autoEncrypter;\n        }\n        this.connectOptions = Object.freeze(connectOptions);\n    }\n    connect() {\n        if (this.s.state !== common_1.STATE_CLOSED) {\n            return;\n        }\n        // start\n        const heartbeatFrequencyMS = this.options.heartbeatFrequencyMS;\n        const minHeartbeatFrequencyMS = this.options.minHeartbeatFrequencyMS;\n        this[kMonitorId] = (0, utils_1.makeInterruptibleAsyncInterval)(monitorServer(this), {\n            interval: heartbeatFrequencyMS,\n            minInterval: minHeartbeatFrequencyMS,\n            immediate: true\n        });\n    }\n    requestCheck() {\n        var _a;\n        if (INVALID_REQUEST_CHECK_STATES.has(this.s.state)) {\n            return;\n        }\n        (_a = this[kMonitorId]) === null || _a === void 0 ? void 0 : _a.wake();\n    }\n    reset() {\n        const topologyVersion = this[kServer].description.topologyVersion;\n        if (isInCloseState(this) || topologyVersion == null) {\n            return;\n        }\n        stateTransition(this, common_1.STATE_CLOSING);\n        resetMonitorState(this);\n        // restart monitor\n        stateTransition(this, STATE_IDLE);\n        // restart monitoring\n        const heartbeatFrequencyMS = this.options.heartbeatFrequencyMS;\n        const minHeartbeatFrequencyMS = this.options.minHeartbeatFrequencyMS;\n        this[kMonitorId] = (0, utils_1.makeInterruptibleAsyncInterval)(monitorServer(this), {\n            interval: heartbeatFrequencyMS,\n            minInterval: minHeartbeatFrequencyMS\n        });\n    }\n    close() {\n        if (isInCloseState(this)) {\n            return;\n        }\n        stateTransition(this, common_1.STATE_CLOSING);\n        resetMonitorState(this);\n        // close monitor\n        this.emit('close');\n        stateTransition(this, common_1.STATE_CLOSED);\n    }\n}\nexports.Monitor = Monitor;\nfunction resetMonitorState(monitor) {\n    var _a, _b, _c;\n    (_a = monitor[kMonitorId]) === null || _a === void 0 ? void 0 : _a.stop();\n    monitor[kMonitorId] = undefined;\n    (_b = monitor[kRTTPinger]) === null || _b === void 0 ? void 0 : _b.close();\n    monitor[kRTTPinger] = undefined;\n    monitor[kCancellationToken].emit('cancel');\n    (_c = monitor[kConnection]) === null || _c === void 0 ? void 0 : _c.destroy({ force: true });\n    monitor[kConnection] = undefined;\n}\nfunction checkServer(monitor, callback) {\n    let start = (0, utils_1.now)();\n    monitor.emit(server_1.Server.SERVER_HEARTBEAT_STARTED, new events_1.ServerHeartbeatStartedEvent(monitor.address));\n    function failureHandler(err) {\n        var _a;\n        (_a = monitor[kConnection]) === null || _a === void 0 ? void 0 : _a.destroy({ force: true });\n        monitor[kConnection] = undefined;\n        monitor.emit(server_1.Server.SERVER_HEARTBEAT_FAILED, new events_1.ServerHeartbeatFailedEvent(monitor.address, (0, utils_1.calculateDurationInMs)(start), err));\n        monitor.emit('resetServer', err);\n        monitor.emit('resetConnectionPool');\n        callback(err);\n    }\n    const connection = monitor[kConnection];\n    if (connection && !connection.closed) {\n        const { serverApi, helloOk } = connection;\n        const connectTimeoutMS = monitor.options.connectTimeoutMS;\n        const maxAwaitTimeMS = monitor.options.heartbeatFrequencyMS;\n        const topologyVersion = monitor[kServer].description.topologyVersion;\n        const isAwaitable = topologyVersion != null;\n        const cmd = {\n            [(serverApi === null || serverApi === void 0 ? void 0 : serverApi.version) || helloOk ? 'hello' : 'ismaster']: true,\n            ...(isAwaitable && topologyVersion\n                ? { maxAwaitTimeMS, topologyVersion: makeTopologyVersion(topologyVersion) }\n                : {})\n        };\n        const options = isAwaitable\n            ? {\n                socketTimeoutMS: connectTimeoutMS ? connectTimeoutMS + maxAwaitTimeMS : 0,\n                exhaustAllowed: true\n            }\n            : { socketTimeoutMS: connectTimeoutMS };\n        if (isAwaitable && monitor[kRTTPinger] == null) {\n            monitor[kRTTPinger] = new RTTPinger(monitor[kCancellationToken], Object.assign({ heartbeatFrequencyMS: monitor.options.heartbeatFrequencyMS }, monitor.connectOptions));\n        }\n        connection.command((0, utils_1.ns)('admin.$cmd'), cmd, options, (err, isMaster) => {\n            var _a;\n            if (err) {\n                failureHandler(err);\n                return;\n            }\n            if ('isWritablePrimary' in isMaster) {\n                // Provide pre-hello-style response document.\n                isMaster.ismaster = isMaster.isWritablePrimary;\n            }\n            const rttPinger = monitor[kRTTPinger];\n            const duration = isAwaitable && rttPinger ? rttPinger.roundTripTime : (0, utils_1.calculateDurationInMs)(start);\n            monitor.emit(server_1.Server.SERVER_HEARTBEAT_SUCCEEDED, new events_1.ServerHeartbeatSucceededEvent(monitor.address, duration, isMaster));\n            // if we are using the streaming protocol then we immediately issue another `started`\n            // event, otherwise the \"check\" is complete and return to the main monitor loop\n            if (isAwaitable && isMaster.topologyVersion) {\n                monitor.emit(server_1.Server.SERVER_HEARTBEAT_STARTED, new events_1.ServerHeartbeatStartedEvent(monitor.address));\n                start = (0, utils_1.now)();\n            }\n            else {\n                (_a = monitor[kRTTPinger]) === null || _a === void 0 ? void 0 : _a.close();\n                monitor[kRTTPinger] = undefined;\n                callback(undefined, isMaster);\n            }\n        });\n        return;\n    }\n    // connecting does an implicit `ismaster`\n    (0, connect_1.connect)(monitor.connectOptions, (err, conn) => {\n        if (err) {\n            monitor[kConnection] = undefined;\n            // we already reset the connection pool on network errors in all cases\n            if (!(err instanceof error_1.MongoNetworkError)) {\n                monitor.emit('resetConnectionPool');\n            }\n            failureHandler(err);\n            return;\n        }\n        if (conn) {\n            if (isInCloseState(monitor)) {\n                conn.destroy({ force: true });\n                return;\n            }\n            monitor[kConnection] = conn;\n            monitor.emit(server_1.Server.SERVER_HEARTBEAT_SUCCEEDED, new events_1.ServerHeartbeatSucceededEvent(monitor.address, (0, utils_1.calculateDurationInMs)(start), conn.ismaster));\n            callback(undefined, conn.ismaster);\n        }\n    });\n}\nfunction monitorServer(monitor) {\n    return (callback) => {\n        stateTransition(monitor, STATE_MONITORING);\n        function done() {\n            if (!isInCloseState(monitor)) {\n                stateTransition(monitor, STATE_IDLE);\n            }\n            callback();\n        }\n        checkServer(monitor, (err, isMaster) => {\n            if (err) {\n                // otherwise an error occurred on initial discovery, also bail\n                if (monitor[kServer].description.type === common_1.ServerType.Unknown) {\n                    monitor.emit('resetServer', err);\n                    return done();\n                }\n            }\n            // if the check indicates streaming is supported, immediately reschedule monitoring\n            if (isMaster && isMaster.topologyVersion) {\n                setTimeout(() => {\n                    var _a;\n                    if (!isInCloseState(monitor)) {\n                        (_a = monitor[kMonitorId]) === null || _a === void 0 ? void 0 : _a.wake();\n                    }\n                }, 0);\n            }\n            done();\n        });\n    };\n}\nfunction makeTopologyVersion(tv) {\n    return {\n        processId: tv.processId,\n        // tests mock counter as just number, but in a real situation counter should always be a Long\n        counter: bson_1.Long.isLong(tv.counter) ? tv.counter : bson_1.Long.fromNumber(tv.counter)\n    };\n}\n/** @internal */\nclass RTTPinger {\n    constructor(cancellationToken, options) {\n        this[kConnection] = undefined;\n        this[kCancellationToken] = cancellationToken;\n        this[kRoundTripTime] = 0;\n        this.closed = false;\n        const heartbeatFrequencyMS = options.heartbeatFrequencyMS;\n        this[kMonitorId] = setTimeout(() => measureRoundTripTime(this, options), heartbeatFrequencyMS);\n    }\n    get roundTripTime() {\n        return this[kRoundTripTime];\n    }\n    close() {\n        var _a;\n        this.closed = true;\n        clearTimeout(this[kMonitorId]);\n        (_a = this[kConnection]) === null || _a === void 0 ? void 0 : _a.destroy({ force: true });\n        this[kConnection] = undefined;\n    }\n}\nexports.RTTPinger = RTTPinger;\nfunction measureRoundTripTime(rttPinger, options) {\n    const start = (0, utils_1.now)();\n    options.cancellationToken = rttPinger[kCancellationToken];\n    const heartbeatFrequencyMS = options.heartbeatFrequencyMS;\n    if (rttPinger.closed) {\n        return;\n    }\n    function measureAndReschedule(conn) {\n        if (rttPinger.closed) {\n            conn === null || conn === void 0 ? void 0 : conn.destroy({ force: true });\n            return;\n        }\n        if (rttPinger[kConnection] == null) {\n            rttPinger[kConnection] = conn;\n        }\n        rttPinger[kRoundTripTime] = (0, utils_1.calculateDurationInMs)(start);\n        rttPinger[kMonitorId] = setTimeout(() => measureRoundTripTime(rttPinger, options), heartbeatFrequencyMS);\n    }\n    const connection = rttPinger[kConnection];\n    if (connection == null) {\n        (0, connect_1.connect)(options, (err, conn) => {\n            if (err) {\n                rttPinger[kConnection] = undefined;\n                rttPinger[kRoundTripTime] = 0;\n                return;\n            }\n            measureAndReschedule(conn);\n        });\n        return;\n    }\n    connection.command((0, utils_1.ns)('admin.$cmd'), { ismaster: 1 }, undefined, err => {\n        if (err) {\n            rttPinger[kConnection] = undefined;\n            rttPinger[kRoundTripTime] = 0;\n            return;\n        }\n        measureAndReschedule();\n    });\n}\n//# sourceMappingURL=monitor.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.ServerHeartbeatFailedEvent = exports.ServerHeartbeatSucceededEvent = exports.ServerHeartbeatStartedEvent = exports.TopologyClosedEvent = exports.TopologyOpeningEvent = exports.TopologyDescriptionChangedEvent = exports.ServerClosedEvent = exports.ServerOpeningEvent = exports.ServerDescriptionChangedEvent = void 0;\n/**\n * Emitted when server description changes, but does NOT include changes to the RTT.\n * @public\n * @category Event\n */\nclass ServerDescriptionChangedEvent {\n    /** @internal */\n    constructor(topologyId, address, previousDescription, newDescription) {\n        this.topologyId = topologyId;\n        this.address = address;\n        this.previousDescription = previousDescription;\n        this.newDescription = newDescription;\n    }\n}\nexports.ServerDescriptionChangedEvent = ServerDescriptionChangedEvent;\n/**\n * Emitted when server is initialized.\n * @public\n * @category Event\n */\nclass ServerOpeningEvent {\n    /** @internal */\n    constructor(topologyId, address) {\n        this.topologyId = topologyId;\n        this.address = address;\n    }\n}\nexports.ServerOpeningEvent = ServerOpeningEvent;\n/**\n * Emitted when server is closed.\n * @public\n * @category Event\n */\nclass ServerClosedEvent {\n    /** @internal */\n    constructor(topologyId, address) {\n        this.topologyId = topologyId;\n        this.address = address;\n    }\n}\nexports.ServerClosedEvent = ServerClosedEvent;\n/**\n * Emitted when topology description changes.\n * @public\n * @category Event\n */\nclass TopologyDescriptionChangedEvent {\n    /** @internal */\n    constructor(topologyId, previousDescription, newDescription) {\n        this.topologyId = topologyId;\n        this.previousDescription = previousDescription;\n        this.newDescription = newDescription;\n    }\n}\nexports.TopologyDescriptionChangedEvent = TopologyDescriptionChangedEvent;\n/**\n * Emitted when topology is initialized.\n * @public\n * @category Event\n */\nclass TopologyOpeningEvent {\n    /** @internal */\n    constructor(topologyId) {\n        this.topologyId = topologyId;\n    }\n}\nexports.TopologyOpeningEvent = TopologyOpeningEvent;\n/**\n * Emitted when topology is closed.\n * @public\n * @category Event\n */\nclass TopologyClosedEvent {\n    /** @internal */\n    constructor(topologyId) {\n        this.topologyId = topologyId;\n    }\n}\nexports.TopologyClosedEvent = TopologyClosedEvent;\n/**\n * Emitted when the server monitors ismaster command is started - immediately before\n * the ismaster command is serialized into raw BSON and written to the socket.\n *\n * @public\n * @category Event\n */\nclass ServerHeartbeatStartedEvent {\n    /** @internal */\n    constructor(connectionId) {\n        this.connectionId = connectionId;\n    }\n}\nexports.ServerHeartbeatStartedEvent = ServerHeartbeatStartedEvent;\n/**\n * Emitted when the server monitors ismaster succeeds.\n * @public\n * @category Event\n */\nclass ServerHeartbeatSucceededEvent {\n    /** @internal */\n    constructor(connectionId, duration, reply) {\n        this.connectionId = connectionId;\n        this.duration = duration;\n        this.reply = reply;\n    }\n}\nexports.ServerHeartbeatSucceededEvent = ServerHeartbeatSucceededEvent;\n/**\n * Emitted when the server monitors ismaster fails, either with an ok: 0 or a socket exception.\n * @public\n * @category Event\n */\nclass ServerHeartbeatFailedEvent {\n    /** @internal */\n    constructor(connectionId, duration, failure) {\n        this.connectionId = connectionId;\n        this.duration = duration;\n        this.failure = failure;\n    }\n}\nexports.ServerHeartbeatFailedEvent = ServerHeartbeatFailedEvent;\n//# sourceMappingURL=events.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.SrvPoller = exports.SrvPollingEvent = void 0;\nconst dns = require(\"dns\");\nconst logger_1 = require(\"../logger\");\nconst utils_1 = require(\"../utils\");\nconst mongo_types_1 = require(\"../mongo_types\");\nconst error_1 = require(\"../error\");\n/**\n * Determines whether a provided address matches the provided parent domain in order\n * to avoid certain attack vectors.\n *\n * @param srvAddress - The address to check against a domain\n * @param parentDomain - The domain to check the provided address against\n * @returns Whether the provided address matches the parent domain\n */\nfunction matchesParentDomain(srvAddress, parentDomain) {\n    const regex = /^.*?\\./;\n    const srv = `.${srvAddress.replace(regex, '')}`;\n    const parent = `.${parentDomain.replace(regex, '')}`;\n    return srv.endsWith(parent);\n}\n/**\n * @internal\n * @category Event\n */\nclass SrvPollingEvent {\n    constructor(srvRecords) {\n        this.srvRecords = srvRecords;\n    }\n    hostnames() {\n        return new Set(this.srvRecords.map(r => utils_1.HostAddress.fromSrvRecord(r).toString()));\n    }\n}\nexports.SrvPollingEvent = SrvPollingEvent;\n/** @internal */\nclass SrvPoller extends mongo_types_1.TypedEventEmitter {\n    constructor(options) {\n        var _a, _b, _c;\n        super();\n        if (!options || !options.srvHost) {\n            throw new error_1.MongoRuntimeError('Options for SrvPoller must exist and include srvHost');\n        }\n        this.srvHost = options.srvHost;\n        this.srvMaxHosts = (_a = options.srvMaxHosts) !== null && _a !== void 0 ? _a : 0;\n        this.srvServiceName = (_b = options.srvServiceName) !== null && _b !== void 0 ? _b : 'mongodb';\n        this.rescanSrvIntervalMS = 60000;\n        this.heartbeatFrequencyMS = (_c = options.heartbeatFrequencyMS) !== null && _c !== void 0 ? _c : 10000;\n        this.logger = new logger_1.Logger('srvPoller', options);\n        this.haMode = false;\n        this.generation = 0;\n        this._timeout = undefined;\n    }\n    get srvAddress() {\n        return `_${this.srvServiceName}._tcp.${this.srvHost}`;\n    }\n    get intervalMS() {\n        return this.haMode ? this.heartbeatFrequencyMS : this.rescanSrvIntervalMS;\n    }\n    start() {\n        if (!this._timeout) {\n            this.schedule();\n        }\n    }\n    stop() {\n        if (this._timeout) {\n            clearTimeout(this._timeout);\n            this.generation += 1;\n            this._timeout = undefined;\n        }\n    }\n    schedule() {\n        if (this._timeout) {\n            clearTimeout(this._timeout);\n        }\n        this._timeout = setTimeout(() => this._poll(), this.intervalMS);\n    }\n    success(srvRecords) {\n        this.haMode = false;\n        this.schedule();\n        this.emit(SrvPoller.SRV_RECORD_DISCOVERY, new SrvPollingEvent(srvRecords));\n    }\n    failure(message, obj) {\n        this.logger.warn(message, obj);\n        this.haMode = true;\n        this.schedule();\n    }\n    parentDomainMismatch(srvRecord) {\n        this.logger.warn(`parent domain mismatch on SRV record (${srvRecord.name}:${srvRecord.port})`, srvRecord);\n    }\n    _poll() {\n        const generation = this.generation;\n        dns.resolveSrv(this.srvAddress, (err, srvRecords) => {\n            if (generation !== this.generation) {\n                return;\n            }\n            if (err) {\n                this.failure('DNS error', err);\n                return;\n            }\n            const finalAddresses = [];\n            for (const record of srvRecords) {\n                if (matchesParentDomain(record.name, this.srvHost)) {\n                    finalAddresses.push(record);\n                }\n                else {\n                    this.parentDomainMismatch(record);\n                }\n            }\n            if (!finalAddresses.length) {\n                this.failure('No valid addresses found at host');\n                return;\n            }\n            this.success(finalAddresses);\n        });\n    }\n}\nexports.SrvPoller = SrvPoller;\n/** @event */\nSrvPoller.SRV_RECORD_DISCOVERY = 'srvRecordDiscovery';\n//# sourceMappingURL=srv_polling.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.GridFSBucket = void 0;\nconst error_1 = require(\"../error\");\nconst download_1 = require(\"./download\");\nconst upload_1 = require(\"./upload\");\nconst utils_1 = require(\"../utils\");\nconst write_concern_1 = require(\"../write_concern\");\nconst mongo_types_1 = require(\"../mongo_types\");\nconst DEFAULT_GRIDFS_BUCKET_OPTIONS = {\n    bucketName: 'fs',\n    chunkSizeBytes: 255 * 1024\n};\n/**\n * Constructor for a streaming GridFS interface\n * @public\n */\nclass GridFSBucket extends mongo_types_1.TypedEventEmitter {\n    constructor(db, options) {\n        super();\n        this.setMaxListeners(0);\n        const privateOptions = {\n            ...DEFAULT_GRIDFS_BUCKET_OPTIONS,\n            ...options,\n            writeConcern: write_concern_1.WriteConcern.fromOptions(options)\n        };\n        this.s = {\n            db,\n            options: privateOptions,\n            _chunksCollection: db.collection(privateOptions.bucketName + '.chunks'),\n            _filesCollection: db.collection(privateOptions.bucketName + '.files'),\n            checkedIndexes: false,\n            calledOpenUploadStream: false\n        };\n    }\n    /**\n     * Returns a writable stream (GridFSBucketWriteStream) for writing\n     * buffers to GridFS. The stream's 'id' property contains the resulting\n     * file's id.\n     *\n     * @param filename - The value of the 'filename' key in the files doc\n     * @param options - Optional settings.\n     */\n    openUploadStream(filename, options) {\n        return new upload_1.GridFSBucketWriteStream(this, filename, options);\n    }\n    /**\n     * Returns a writable stream (GridFSBucketWriteStream) for writing\n     * buffers to GridFS for a custom file id. The stream's 'id' property contains the resulting\n     * file's id.\n     */\n    openUploadStreamWithId(id, filename, options) {\n        return new upload_1.GridFSBucketWriteStream(this, filename, { ...options, id });\n    }\n    /** Returns a readable stream (GridFSBucketReadStream) for streaming file data from GridFS. */\n    openDownloadStream(id, options) {\n        return new download_1.GridFSBucketReadStream(this.s._chunksCollection, this.s._filesCollection, this.s.options.readPreference, { _id: id }, options);\n    }\n    delete(id, callback) {\n        return (0, utils_1.executeLegacyOperation)((0, utils_1.getTopology)(this.s.db), _delete, [this, id, callback], {\n            skipSessions: true\n        });\n    }\n    /** Convenience wrapper around find on the files collection */\n    find(filter, options) {\n        filter !== null && filter !== void 0 ? filter : (filter = {});\n        options = options !== null && options !== void 0 ? options : {};\n        return this.s._filesCollection.find(filter, options);\n    }\n    /**\n     * Returns a readable stream (GridFSBucketReadStream) for streaming the\n     * file with the given name from GridFS. If there are multiple files with\n     * the same name, this will stream the most recent file with the given name\n     * (as determined by the `uploadDate` field). You can set the `revision`\n     * option to change this behavior.\n     */\n    openDownloadStreamByName(filename, options) {\n        let sort = { uploadDate: -1 };\n        let skip = undefined;\n        if (options && options.revision != null) {\n            if (options.revision >= 0) {\n                sort = { uploadDate: 1 };\n                skip = options.revision;\n            }\n            else {\n                skip = -options.revision - 1;\n            }\n        }\n        return new download_1.GridFSBucketReadStream(this.s._chunksCollection, this.s._filesCollection, this.s.options.readPreference, { filename }, { ...options, sort, skip });\n    }\n    rename(id, filename, callback) {\n        return (0, utils_1.executeLegacyOperation)((0, utils_1.getTopology)(this.s.db), _rename, [this, id, filename, callback], {\n            skipSessions: true\n        });\n    }\n    drop(callback) {\n        return (0, utils_1.executeLegacyOperation)((0, utils_1.getTopology)(this.s.db), _drop, [this, callback], {\n            skipSessions: true\n        });\n    }\n    /** Get the Db scoped logger. */\n    getLogger() {\n        return this.s.db.s.logger;\n    }\n}\nexports.GridFSBucket = GridFSBucket;\n/**\n * When the first call to openUploadStream is made, the upload stream will\n * check to see if it needs to create the proper indexes on the chunks and\n * files collections. This event is fired either when 1) it determines that\n * no index creation is necessary, 2) when it successfully creates the\n * necessary indexes.\n * @event\n */\nGridFSBucket.INDEX = 'index';\nfunction _delete(bucket, id, callback) {\n    return bucket.s._filesCollection.deleteOne({ _id: id }, (error, res) => {\n        if (error) {\n            return callback(error);\n        }\n        return bucket.s._chunksCollection.deleteMany({ files_id: id }, error => {\n            if (error) {\n                return callback(error);\n            }\n            // Delete orphaned chunks before returning FileNotFound\n            if (!(res === null || res === void 0 ? void 0 : res.deletedCount)) {\n                // TODO(NODE-3483): Replace with more appropriate error\n                // Consider creating new error MongoGridFSFileNotFoundError\n                return callback(new error_1.MongoRuntimeError(`File not found for id ${id}`));\n            }\n            return callback();\n        });\n    });\n}\nfunction _rename(bucket, id, filename, callback) {\n    const filter = { _id: id };\n    const update = { $set: { filename } };\n    return bucket.s._filesCollection.updateOne(filter, update, (error, res) => {\n        if (error) {\n            return callback(error);\n        }\n        if (!(res === null || res === void 0 ? void 0 : res.matchedCount)) {\n            return callback(new error_1.MongoRuntimeError(`File with id ${id} not found`));\n        }\n        return callback();\n    });\n}\nfunction _drop(bucket, callback) {\n    return bucket.s._filesCollection.drop((error) => {\n        if (error) {\n            return callback(error);\n        }\n        return bucket.s._chunksCollection.drop((error) => {\n            if (error) {\n                return callback(error);\n            }\n            return callback();\n        });\n    });\n}\n//# sourceMappingURL=index.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.GridFSBucketReadStream = void 0;\nconst stream_1 = require(\"stream\");\nconst error_1 = require(\"../error\");\n/**\n * A readable stream that enables you to read buffers from GridFS.\n *\n * Do not instantiate this class directly. Use `openDownloadStream()` instead.\n * @public\n */\nclass GridFSBucketReadStream extends stream_1.Readable {\n    /** @internal\n     * @param chunks - Handle for chunks collection\n     * @param files - Handle for files collection\n     * @param readPreference - The read preference to use\n     * @param filter - The filter to use to find the file document\n     */\n    constructor(chunks, files, readPreference, filter, options) {\n        super();\n        this.s = {\n            bytesToTrim: 0,\n            bytesToSkip: 0,\n            bytesRead: 0,\n            chunks,\n            expected: 0,\n            files,\n            filter,\n            init: false,\n            expectedEnd: 0,\n            options: {\n                start: 0,\n                end: 0,\n                ...options\n            },\n            readPreference\n        };\n    }\n    /**\n     * Reads from the cursor and pushes to the stream.\n     * Private Impl, do not call directly\n     * @internal\n     */\n    _read() {\n        if (this.destroyed)\n            return;\n        waitForFile(this, () => doRead(this));\n    }\n    /**\n     * Sets the 0-based offset in bytes to start streaming from. Throws\n     * an error if this stream has entered flowing mode\n     * (e.g. if you've already called `on('data')`)\n     *\n     * @param start - 0-based offset in bytes to start streaming from\n     */\n    start(start = 0) {\n        throwIfInitialized(this);\n        this.s.options.start = start;\n        return this;\n    }\n    /**\n     * Sets the 0-based offset in bytes to start streaming from. Throws\n     * an error if this stream has entered flowing mode\n     * (e.g. if you've already called `on('data')`)\n     *\n     * @param end - Offset in bytes to stop reading at\n     */\n    end(end = 0) {\n        throwIfInitialized(this);\n        this.s.options.end = end;\n        return this;\n    }\n    /**\n     * Marks this stream as aborted (will never push another `data` event)\n     * and kills the underlying cursor. Will emit the 'end' event, and then\n     * the 'close' event once the cursor is successfully killed.\n     *\n     * @param callback - called when the cursor is successfully closed or an error occurred.\n     */\n    abort(callback) {\n        this.push(null);\n        this.destroyed = true;\n        if (this.s.cursor) {\n            this.s.cursor.close(error => {\n                this.emit(GridFSBucketReadStream.CLOSE);\n                callback && callback(error);\n            });\n        }\n        else {\n            if (!this.s.init) {\n                // If not initialized, fire close event because we will never\n                // get a cursor\n                this.emit(GridFSBucketReadStream.CLOSE);\n            }\n            callback && callback();\n        }\n    }\n}\nexports.GridFSBucketReadStream = GridFSBucketReadStream;\n/**\n * An error occurred\n * @event\n */\nGridFSBucketReadStream.ERROR = 'error';\n/**\n * Fires when the stream loaded the file document corresponding to the provided id.\n * @event\n */\nGridFSBucketReadStream.FILE = 'file';\n/**\n * Emitted when a chunk of data is available to be consumed.\n * @event\n */\nGridFSBucketReadStream.DATA = 'data';\n/**\n * Fired when the stream is exhausted (no more data events).\n * @event\n */\nGridFSBucketReadStream.END = 'end';\n/**\n * Fired when the stream is exhausted and the underlying cursor is killed\n * @event\n */\nGridFSBucketReadStream.CLOSE = 'close';\nfunction throwIfInitialized(stream) {\n    if (stream.s.init) {\n        throw new error_1.MongoGridFSStreamError('Options cannot be changed after the stream is initialized');\n    }\n}\nfunction doRead(stream) {\n    if (stream.destroyed)\n        return;\n    if (!stream.s.cursor)\n        return;\n    if (!stream.s.file)\n        return;\n    stream.s.cursor.next((error, doc) => {\n        if (stream.destroyed) {\n            return;\n        }\n        if (error) {\n            stream.emit(GridFSBucketReadStream.ERROR, error);\n            return;\n        }\n        if (!doc) {\n            stream.push(null);\n            process.nextTick(() => {\n                if (!stream.s.cursor)\n                    return;\n                stream.s.cursor.close(error => {\n                    if (error) {\n                        stream.emit(GridFSBucketReadStream.ERROR, error);\n                        return;\n                    }\n                    stream.emit(GridFSBucketReadStream.CLOSE);\n                });\n            });\n            return;\n        }\n        if (!stream.s.file)\n            return;\n        const bytesRemaining = stream.s.file.length - stream.s.bytesRead;\n        const expectedN = stream.s.expected++;\n        const expectedLength = Math.min(stream.s.file.chunkSize, bytesRemaining);\n        if (doc.n > expectedN) {\n            return stream.emit(GridFSBucketReadStream.ERROR, new error_1.MongoGridFSChunkError(`ChunkIsMissing: Got unexpected n: ${doc.n}, expected: ${expectedN}`));\n        }\n        if (doc.n < expectedN) {\n            return stream.emit(GridFSBucketReadStream.ERROR, new error_1.MongoGridFSChunkError(`ExtraChunk: Got unexpected n: ${doc.n}, expected: ${expectedN}`));\n        }\n        let buf = Buffer.isBuffer(doc.data) ? doc.data : doc.data.buffer;\n        if (buf.byteLength !== expectedLength) {\n            if (bytesRemaining <= 0) {\n                return stream.emit(GridFSBucketReadStream.ERROR, new error_1.MongoGridFSChunkError(`ExtraChunk: Got unexpected n: ${doc.n}`));\n            }\n            return stream.emit(GridFSBucketReadStream.ERROR, new error_1.MongoGridFSChunkError(`ChunkIsWrongSize: Got unexpected length: ${buf.byteLength}, expected: ${expectedLength}`));\n        }\n        stream.s.bytesRead += buf.byteLength;\n        if (buf.byteLength === 0) {\n            return stream.push(null);\n        }\n        let sliceStart = null;\n        let sliceEnd = null;\n        if (stream.s.bytesToSkip != null) {\n            sliceStart = stream.s.bytesToSkip;\n            stream.s.bytesToSkip = 0;\n        }\n        const atEndOfStream = expectedN === stream.s.expectedEnd - 1;\n        const bytesLeftToRead = stream.s.options.end - stream.s.bytesToSkip;\n        if (atEndOfStream && stream.s.bytesToTrim != null) {\n            sliceEnd = stream.s.file.chunkSize - stream.s.bytesToTrim;\n        }\n        else if (stream.s.options.end && bytesLeftToRead < doc.data.byteLength) {\n            sliceEnd = bytesLeftToRead;\n        }\n        if (sliceStart != null || sliceEnd != null) {\n            buf = buf.slice(sliceStart || 0, sliceEnd || buf.byteLength);\n        }\n        stream.push(buf);\n    });\n}\nfunction init(stream) {\n    const findOneOptions = {};\n    if (stream.s.readPreference) {\n        findOneOptions.readPreference = stream.s.readPreference;\n    }\n    if (stream.s.options && stream.s.options.sort) {\n        findOneOptions.sort = stream.s.options.sort;\n    }\n    if (stream.s.options && stream.s.options.skip) {\n        findOneOptions.skip = stream.s.options.skip;\n    }\n    stream.s.files.findOne(stream.s.filter, findOneOptions, (error, doc) => {\n        if (error) {\n            return stream.emit(GridFSBucketReadStream.ERROR, error);\n        }\n        if (!doc) {\n            const identifier = stream.s.filter._id\n                ? stream.s.filter._id.toString()\n                : stream.s.filter.filename;\n            const errmsg = `FileNotFound: file ${identifier} was not found`;\n            // TODO(NODE-3483)\n            const err = new error_1.MongoRuntimeError(errmsg);\n            err.code = 'ENOENT'; // TODO: NODE-3338 set property as part of constructor\n            return stream.emit(GridFSBucketReadStream.ERROR, err);\n        }\n        // If document is empty, kill the stream immediately and don't\n        // execute any reads\n        if (doc.length <= 0) {\n            stream.push(null);\n            return;\n        }\n        if (stream.destroyed) {\n            // If user destroys the stream before we have a cursor, wait\n            // until the query is done to say we're 'closed' because we can't\n            // cancel a query.\n            stream.emit(GridFSBucketReadStream.CLOSE);\n            return;\n        }\n        try {\n            stream.s.bytesToSkip = handleStartOption(stream, doc, stream.s.options);\n        }\n        catch (error) {\n            return stream.emit(GridFSBucketReadStream.ERROR, error);\n        }\n        const filter = { files_id: doc._id };\n        // Currently (MongoDB 3.4.4) skip function does not support the index,\n        // it needs to retrieve all the documents first and then skip them. (CS-25811)\n        // As work around we use $gte on the \"n\" field.\n        if (stream.s.options && stream.s.options.start != null) {\n            const skip = Math.floor(stream.s.options.start / doc.chunkSize);\n            if (skip > 0) {\n                filter['n'] = { $gte: skip };\n            }\n        }\n        stream.s.cursor = stream.s.chunks.find(filter).sort({ n: 1 });\n        if (stream.s.readPreference) {\n            stream.s.cursor.withReadPreference(stream.s.readPreference);\n        }\n        stream.s.expectedEnd = Math.ceil(doc.length / doc.chunkSize);\n        stream.s.file = doc;\n        try {\n            stream.s.bytesToTrim = handleEndOption(stream, doc, stream.s.cursor, stream.s.options);\n        }\n        catch (error) {\n            return stream.emit(GridFSBucketReadStream.ERROR, error);\n        }\n        stream.emit(GridFSBucketReadStream.FILE, doc);\n    });\n}\nfunction waitForFile(stream, callback) {\n    if (stream.s.file) {\n        return callback();\n    }\n    if (!stream.s.init) {\n        init(stream);\n        stream.s.init = true;\n    }\n    stream.once('file', () => {\n        callback();\n    });\n}\nfunction handleStartOption(stream, doc, options) {\n    if (options && options.start != null) {\n        if (options.start > doc.length) {\n            throw new error_1.MongoInvalidArgumentError(`Stream start (${options.start}) must not be more than the length of the file (${doc.length})`);\n        }\n        if (options.start < 0) {\n            throw new error_1.MongoInvalidArgumentError(`Stream start (${options.start}) must not be negative`);\n        }\n        if (options.end != null && options.end < options.start) {\n            throw new error_1.MongoInvalidArgumentError(`Stream start (${options.start}) must not be greater than stream end (${options.end})`);\n        }\n        stream.s.bytesRead = Math.floor(options.start / doc.chunkSize) * doc.chunkSize;\n        stream.s.expected = Math.floor(options.start / doc.chunkSize);\n        return options.start - stream.s.bytesRead;\n    }\n    throw new error_1.MongoInvalidArgumentError('Start option must be defined');\n}\nfunction handleEndOption(stream, doc, cursor, options) {\n    if (options && options.end != null) {\n        if (options.end > doc.length) {\n            throw new error_1.MongoInvalidArgumentError(`Stream end (${options.end}) must not be more than the length of the file (${doc.length})`);\n        }\n        if (options.start == null || options.start < 0) {\n            throw new error_1.MongoInvalidArgumentError(`Stream end (${options.end}) must not be negative`);\n        }\n        const start = options.start != null ? Math.floor(options.start / doc.chunkSize) : 0;\n        cursor.limit(Math.ceil(options.end / doc.chunkSize) - start);\n        stream.s.expectedEnd = Math.ceil(options.end / doc.chunkSize);\n        return Math.ceil(options.end / doc.chunkSize) * doc.chunkSize - options.end;\n    }\n    throw new error_1.MongoInvalidArgumentError('End option must be defined');\n}\n//# sourceMappingURL=download.js.map","\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.GridFSBucketWriteStream = void 0;\nconst stream_1 = require(\"stream\");\nconst bson_1 = require(\"../bson\");\nconst error_1 = require(\"../error\");\nconst utils_1 = require(\"../utils\");\nconst write_concern_1 = require(\"./../write_concern\");\n/**\n * A writable stream that enables you to write buffers to GridFS.\n *\n * Do not instantiate this class directly. Use `openUploadStream()` instead.\n * @public\n */\nclass GridFSBucketWriteStream extends stream_1.Writable {\n    /** @internal\n     * @param bucket - Handle for this stream's corresponding bucket\n     * @param filename - The value of the 'filename' key in the files doc\n     * @param options - Optional settings.\n     */\n    constructor(bucket, filename, options) {\n        super();\n        options = options !== null && options !== void 0 ? options : {};\n        this.bucket = bucket;\n        this.chunks = bucket.s._chunksCollection;\n        this.filename = filename;\n        this.files = bucket.s._filesCollection;\n        this.options = options;\n        this.writeConcern = write_concern_1.WriteConcern.fromOptions(options) || bucket.s.options.writeConcern;\n        // Signals the write is all done\n        this.done = false;\n        this.id = options.id ? options.id : new bson_1.ObjectId();\n        // properly inherit the default chunksize from parent\n        this.chunkSizeBytes = options.chunkSizeBytes || this.bucket.s.options.chunkSizeBytes;\n        this.bufToStore = Buffer.alloc(this.chunkSizeBytes);\n        this.length = 0;\n        this.n = 0;\n        this.pos = 0;\n        this.state = {\n            streamEnd: false,\n            outstandingRequests: 0,\n            errored: false,\n            aborted: false\n        };\n        if (!this.bucket.s.calledOpenUploadStream) {\n            this.bucket.s.calledOpenUploadStream = true;\n            checkIndexes(this, () => {\n                this.bucket.s.checkedIndexes = true;\n                this.bucket.emit('index');\n            });\n        }\n    }\n    write(chunk, encodingOrCallback, callback) {\n        const encoding = typeof encodingOrCallback === 'function' ? undefined : encodingOrCallback;\n        callback = typeof encodingOrCallback === 'function' ? encodingOrCallback : callback;\n        return waitForIndexes(this, () => doWrite(this, chunk, encoding, callback));\n    }\n    abort(callback) {\n        return (0, utils_1.maybePromise)(callback, callback => {\n            if (this.state.streamEnd) {\n                // TODO(NODE-3485): Replace with MongoGridFSStreamClosed\n                return callback(new error_1.MongoAPIError('Cannot abort a stream that has already completed'));\n            }\n            if (this.state.aborted) {\n                // TODO(NODE-3485): Replace with MongoGridFSStreamClosed\n                return callback(new error_1.MongoAPIError('Cannot call abort() on a stream twice'));\n            }\n            this.state.aborted = true;\n            this.chunks.deleteMany({ files_id: this.id }, error => callback(error));\n        });\n    }\n    end(chunkOrCallback, encodingOrCallback, callback) {\n        const chunk = typeof chunkOrCallback === 'function' ? undefined : chunkOrCallback;\n        const encoding = typeof encodingOrCallback === 'function' ? undefined : encodingOrCallback;\n        callback =\n            typeof chunkOrCallback === 'function'\n                ? chunkOrCallback\n                : typeof encodingOrCallback === 'function'\n                    ? encodingOrCallback\n                    : callback;\n        if (checkAborted(this, callback))\n            return;\n        this.state.streamEnd = true;\n        if (callback) {\n            this.once(GridFSBucketWriteStream.FINISH, (result) => {\n                if (callback)\n                    callback(undefined, result);\n            });\n        }\n        if (!chunk) {\n            waitForIndexes(this, () => !!writeRemnant(this));\n            return;\n        }\n        this.write(chunk, encoding, () => {\n            writeRemnant(this);\n        });\n    }\n}\nexports.GridFSBucketWriteStream = GridFSBucketWriteStream;\n/** @event */\nGridFSBucketWriteStream.CLOSE = 'close';\n/** @event */\nGridFSBucketWriteStream.ERROR = 'error';\n/**\n * `end()` was called and the write stream successfully wrote the file metadata and all the chunks to MongoDB.\n * @event\n */\nGridFSBucketWriteStream.FINISH = 'finish';\nfunction __handleError(stream, error, callback) {\n    if (stream.state.errored) {\n        return;\n    }\n    stream.state.errored = true;\n    if (callback) {\n        return callback(error);\n    }\n    stream.emit(GridFSBucketWriteStream.ERROR, error);\n}\nfunction createChunkDoc(filesId, n, data) {\n    return {\n        _id: new bson_1.ObjectId(),\n        files_id: filesId,\n        n,\n        data\n    };\n}\nfunction checkChunksIndex(stream, callback) {\n    stream.chunks.listIndexes().toArray((error, indexes) => {\n        let index;\n        if (error) {\n            // Collection doesn't exist so create index\n            if (error instanceof error_1.MongoError && error.code === error_1.MONGODB_ERROR_CODES.NamespaceNotFound) {\n                index = { files_id: 1, n: 1 };\n                stream.chunks.createIndex(index, { background: false, unique: true }, error => {\n                    if (error) {\n                        return callback(error);\n                    }\n                    callback();\n                });\n                return;\n            }\n            return callback(error);\n        }\n        let hasChunksIndex = false;\n        if (indexes) {\n            indexes.forEach((index) => {\n                if (index.key) {\n                    const keys = Object.keys(index.key);\n                    if (keys.length === 2 && index.key.files_id === 1 && index.key.n === 1) {\n                        hasChunksIndex = true;\n                    }\n                }\n            });\n        }\n        if (hasChunksIndex) {\n            callback();\n        }\n        else {\n            index = { files_id: 1, n: 1 };\n            const writeConcernOptions = getWriteOptions(stream);\n            stream.chunks.createIndex(index, {\n                ...writeConcernOptions,\n                background: true,\n                unique: true\n            }, callback);\n        }\n    });\n}\nfunction checkDone(stream, callback) {\n    if (stream.done)\n        return true;\n    if (stream.state.streamEnd && stream.state.outstandingRequests === 0 && !stream.state.errored) {\n        // Set done so we do not trigger duplicate createFilesDoc\n        stream.done = true;\n        // Create a new files doc\n        const filesDoc = createFilesDoc(stream.id, stream.length, stream.chunkSizeBytes, stream.filename, stream.options.contentType, stream.options.aliases, stream.options.metadata);\n        if (checkAborted(stream, callback)) {\n            return false;\n        }\n        stream.files.insertOne(filesDoc, getWriteOptions(stream), (error) => {\n            if (error) {\n                return __handleError(stream, error, callback);\n            }\n            stream.emit(GridFSBucketWriteStream.FINISH, filesDoc);\n            stream.emit(GridFSBucketWriteStream.CLOSE);\n        });\n        return true;\n    }\n    return false;\n}\nfunction checkIndexes(stream, callback) {\n    stream.files.findOne({}, { projection: { _id: 1 } }, (error, doc) => {\n        if (error) {\n            return callback(error);\n        }\n        if (doc) {\n            return callback();\n        }\n        stream.files.listIndexes().toArray((error, indexes) => {\n            let index;\n            if (error) {\n                // Collection doesn't exist so create index\n                if (error instanceof error_1.MongoError && error.code === error_1.MONGODB_ERROR_CODES.NamespaceNotFound) {\n                    index = { filename: 1, uploadDate: 1 };\n                    stream.files.createIndex(index, { background: false }, (error) => {\n                        if (error) {\n                            return callback(error);\n                        }\n                        checkChunksIndex(stream, callback);\n                    });\n                    return;\n                }\n                return callback(error);\n            }\n            let hasFileIndex = false;\n            if (indexes) {\n                indexes.forEach((index) => {\n                    const keys = Object.keys(index.key);\n                    if (keys.length === 2 && index.key.filename === 1 && index.key.uploadDate === 1) {\n                        hasFileIndex = true;\n                    }\n                });\n            }\n            if (hasFileIndex) {\n                checkChunksIndex(stream, callback);\n            }\n            else {\n                index = { filename: 1, uploadDate: 1 };\n                const writeConcernOptions = getWriteOptions(stream);\n                stream.files.createIndex(index, {\n                    ...writeConcernOptions,\n                    background: false\n                }, (error) => {\n                    if (error) {\n                        return callback(error);\n                    }\n                    checkChunksIndex(stream, callback);\n                });\n            }\n        });\n    });\n}\nfunction createFilesDoc(_id, length, chunkSize, filename, contentType, aliases, metadata) {\n    const ret = {\n        _id,\n        length,\n        chunkSize,\n        uploadDate: new Date(),\n        filename\n    };\n    if (contentType) {\n        ret.contentType = contentType;\n    }\n    if (aliases) {\n        ret.aliases = aliases;\n    }\n    if (metadata) {\n        ret.metadata = metadata;\n    }\n    return ret;\n}\nfunction doWrite(stream, chunk, encoding, callback) {\n    if (checkAborted(stream, callback)) {\n        return false;\n    }\n    const inputBuf = Buffer.isBuffer(chunk) ? chunk : Buffer.from(chunk, encoding);\n    stream.length += inputBuf.length;\n    // Input is small enough to fit in our buffer\n    if (stream.pos + inputBuf.length < stream.chunkSizeBytes) {\n        inputBuf.copy(stream.bufToStore, stream.pos);\n        stream.pos += inputBuf.length;\n        callback && callback();\n        // Note that we reverse the typical semantics of write's return value\n        // to be compatible with node's `.pipe()` function.\n        // True means client can keep writing.\n        return true;\n    }\n    // Otherwise, buffer is too big for current chunk, so we need to flush\n    // to MongoDB.\n    let inputBufRemaining = inputBuf.length;\n    let spaceRemaining = stream.chunkSizeBytes - stream.pos;\n    let numToCopy = Math.min(spaceRemaining, inputBuf.length);\n    let outstandingRequests = 0;\n    while (inputBufRemaining > 0) {\n        const inputBufPos = inputBuf.length - inputBufRemaining;\n        inputBuf.copy(stream.bufToStore, stream.pos, inputBufPos, inputBufPos + numToCopy);\n        stream.pos += numToCopy;\n        spaceRemaining -= numToCopy;\n        let doc;\n        if (spaceRemaining === 0) {\n            doc = createChunkDoc(stream.id, stream.n, Buffer.from(stream.bufToStore));\n            ++stream.state.outstandingRequests;\n            ++outstandingRequests;\n            if (checkAborted(stream, callback)) {\n                return false;\n            }\n            stream.chunks.insertOne(doc, getWriteOptions(stream), (error) => {\n                if (error) {\n                    return __handleError(stream, error);\n                }\n                --stream.state.outstandingRequests;\n                --outstandingRequests;\n                if (!outstandingRequests) {\n                    stream.emit('drain', doc);\n                    callback && callback();\n                    checkDone(stream);\n                }\n            });\n            spaceRemaining = stream.chunkSizeBytes;\n            stream.pos = 0;\n            ++stream.n;\n        }\n        inputBufRemaining -= numToCopy;\n        numToCopy = Math.min(spaceRemaining, inputBufRemaining);\n    }\n    // Note that we reverse the typical semantics of write's return value\n    // to be compatible with node's `.pipe()` function.\n    // False means the client should wait for the 'drain' event.\n    return false;\n}\nfunction getWriteOptions(stream) {\n    const obj = {};\n    if (stream.writeConcern) {\n        obj.writeConcern = {\n            w: stream.writeConcern.w,\n            wtimeout: stream.writeConcern.wtimeout,\n            j: stream.writeConcern.j\n        };\n    }\n    return obj;\n}\nfunction waitForIndexes(stream, callback) {\n    if (stream.bucket.s.checkedIndexes) {\n        return callback(false);\n    }\n    stream.bucket.once('index', () => {\n        callback(true);\n    });\n    return true;\n}\nfunction writeRemnant(stream, callback) {\n    // Buffer is empty, so don't bother to insert\n    if (stream.pos === 0) {\n        return checkDone(stream, callback);\n    }\n    ++stream.state.outstandingRequests;\n    // Create a new buffer to make sure the buffer isn't bigger than it needs\n    // to be.\n    const remnant = Buffer.alloc(stream.pos);\n    stream.bufToStore.copy(remnant, 0, 0, stream.pos);\n    const doc = createChunkDoc(stream.id, stream.n, remnant);\n    // If the stream was aborted, do not write remnant\n    if (checkAborted(stream, callback)) {\n        return false;\n    }\n    stream.chunks.insertOne(doc, getWriteOptions(stream), (error) => {\n        if (error) {\n            return __handleError(stream, error);\n        }\n        --stream.state.outstandingRequests;\n        checkDone(stream);\n    });\n    return true;\n}\nfunction checkAborted(stream, callback) {\n    if (stream.state.aborted) {\n        if (typeof callback === 'function') {\n            // TODO(NODE-3485): Replace with MongoGridFSStreamClosedError\n            callback(new error_1.MongoAPIError('Stream has been aborted'));\n        }\n        return true;\n    }\n    return false;\n}\n//# sourceMappingURL=upload.js.map"]}